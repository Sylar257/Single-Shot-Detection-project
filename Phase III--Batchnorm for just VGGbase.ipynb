{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II\n",
    "In the last [notebook](), we built a  vanilla SSD that achieves pretty good resuilt(mAP = 0.746).<br>\n",
    "In this notebook, we will implement some more modern technique and hopefully push the performance a little further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "In the previous version, we have a base net which is a pre-trained VGG-16. On top of our base, we have two extensions (AuxiliaryConvolutions & PredictionConvolutions) that need to be initialized. We adopted the traditional initialization method from the [original paper](https://arxiv.org/abs/1512.02325), xavier uniform. <br>\n",
    "However, it was pointed out in this [paper](https://arxiv.org/pdf/1502.01852.pdf) that initialization is crutial to the training process of a neural network and Xavier is not the best candidate when we adopt 'ReLU' activation after our Conv layers.<br>\n",
    "Hence, we make modification to the our AuxiliaryConvolutions & PredictionConvolutions to `Kaiming_uniform_` initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference form the original paper: <br>\n",
    "    Rectifier networks are easier to train compared with traditional *sigmoid-like* activation networks. But a bad initialization can still hamper the learning of a highly non-linear system. Let’s see how robust initialization method that removes an obstacle of training extremely deep rectifier networks.\n",
    "\n",
    "Recent deep CNNs are mostly initialized by random weights drawn from ***Gaussian distributions***. With fixed standard deviations (e.g. 0.01), very deep models have difficulties to converge and leads to poorer local optimum. *Glorot and Bengio* proposed to adopt a properly scaled uniform distribution for initialization. This is called “*Xavier*” initialization. Its derivation is based on the assumption that the activations are linear. This assumptions is invalid for **ReLU**.\n",
    "\n",
    "Therefore, He et. al. derived a theoretically more sound initialization by taking **ReLU** into account. This method greatly improved our ability to train our models towards convergence.\n",
    "\n",
    "The central idea is to investigate the variance of the responses in each layer, for a **Conv layer**, a response is:\n",
    "\n",
    "​\t\t\t\t\t\t\t\t\t\t\t\t$$y_l = W_lX_l+b_l$$\t\t\n",
    "\n",
    "Here, x is a $k^2c$-by-1 vector that represents co-located $k\\times k$ pixels in $c$ input channels.$k$ is the spatial filter size of the layer. With $n=k^2c$ denoting the number of connections of a response, $W$ is a $d$-by-$n$ matrix, where $d$ is the number of filters and each row of $W$ represents the weights of a filter.**$b$** is vector of biases, and $y$ is the response at a pixel of the output map. We use $l$ to index a layer and have $x_l = f(y_{l-1})$ where $f$ is the activation. Finally, $c_l=d_{l-1}$.\n",
    "\n",
    "**A note there**: a **Conv** layer can also viewed just as a **Dense** layer activation is this fashion.\n",
    "\n",
    "We let the initialized elements in $W_l$ be mutually independent and share the same distribution. Assuming that the elements in $x_l$ are also mutually independent and share the same distribution, and $x_l$ and $W_l$ are independent of each other. Then we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\tVar[y_l] = n_lVar[w_lx_l]\n",
    "\\end{align*}\n",
    "$$\n",
    "where now $y_l$,$x_l$, and $w_l$ represent the random variables of each element in $y_l$, $W_l$, and $x_l$ respectively. We let $w_l$ have zero mean. Then the variance of the product of independent variables gives us: \n",
    "$$\n",
    "￼\n",
    "\\begin{align*}\n",
    "    Var[y_l] = n_lVar[w_l]E[{x_l}^2]\n",
    "\\end{align*}\n",
    "$$\n",
    "Here $E[{x_l}^2]$ is the expectation of the square of $x_l$. \n",
    "\n",
    "With $L$ layers put together, we have:\n",
    "$$\n",
    "￼\\begin{align*}\n",
    "    Var[y_L] = Var[y_l](\\prod_{l=2}^L\\frac{1}{2}n_lVar[w_l])\n",
    "\\end{align*}\n",
    "$$\n",
    "This leads to a zero-mean **Gaussian distribution** whose standard deviation is $\\sqrt{2/n_l}$. This is *Kaiming He*‘s way of initialization. They also initialize $b=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import transform\n",
    "# specify GPU for cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    We implement VGG-16 here for low-level feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # Stabdard convolutional layers in VGG16\n",
    "        # We have an input size of 300 by 300\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # stride = 1, output = (300+2-3)/1+1 = 300\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # output = 300 as before\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (300-2)/2+1 = 150\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64,  128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (150-2)/2 +1  = 75\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # output = ceil((75-2)/2)-1 = 38   if floor we would be getting 37 here which is an odd number\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)           # output = (38-2)/2 +1  = 19\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # We retain the size at this step with padding and stride of 1\n",
    "        # output = (19+2-3)/1+1 = 19\n",
    "\n",
    "        # Here we replace the FC6 and FC7 with the technique introduce by sgrvinod(same with the original paper)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # output = (19+12-3-2*(6-1))/1+1 = 19\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)                       # output = (19-1)/1+1 = 19\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward run with an image input of size 300 by 300\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))   # (N,64,300,300)\n",
    "        out = F.relu(self.conv1_2(out))     # (N,64,300,300)\n",
    "        out = self.pool1(out)               # (N,64,150,150)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N,128,150,150)\n",
    "        out = F.relu(self.conv2_2(out))  # (N,128,150,150)\n",
    "        out = self.pool2(out)            # (N,128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_2(out))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_3(out))  # (N,256, 75, 75)\n",
    "        out = self.pool3(out)            # (N,256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
    "        # here we extract the feature from conv4_3\n",
    "        conv4_3_feats = out              # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)            # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)            # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))    # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        Use pre-trained wieght from Torch Vsion. \n",
    "        Convert fc6 and fc7 weights into conv6 and conv7\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
    "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
    "        # ...operating on the 2D image of size (C, H, W) without padding\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model with pre-trained weights\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-96b4834b19d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVGGBase_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mWe\u001b[0m \u001b[0mimplement\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0mhere\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mextraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VGGBase_BN(nn.Module):\n",
    "    \"\"\"\n",
    "    We implement VGG-16 here for low-level feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase_BN, self).__init__()\n",
    "\n",
    "        # Stabdard convolutional layers in VGG16\n",
    "        # We have an input size of 300 by 300\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # stride = 1, output = (300+2-3)/1+1 = 300\n",
    "        self.bn_1_1  = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # output = 300 as before\n",
    "        self.bn_1_2  = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (300-2)/2+1 = 150\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64,  128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.bn_2_1  = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.bn_2_2  = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (150-2)/2 +1  = 75\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_1  = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_2  = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_3  = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # output = ceil((75-2)/2)-1 = 38   if floor we would be getting 37 here which is an odd number\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_1  = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_2  = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_3  = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)           # output = (38-2)/2 +1  = 19\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_1  = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_2  = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_3  = nn.BatchNorm2d(512)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # We retain the size at this step with padding and stride of 1\n",
    "        # output = (19+2-3)/1+1 = 19\n",
    "\n",
    "        # Here we replace the FC6 and FC7 with the technique introduce by sgrvinod(same with the original paper)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # output = (19+12-3-2*(6-1))/1+1 = 19\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)                       # output = (19-1)/1+1 = 19\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward run with an image input of size 300 by 300\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn_1_1(self.conv1_1(image)))   # (N,64,300,300)\n",
    "        out = F.relu(self.bn_1_2(self.conv1_2(out)))     # (N,64,300,300)\n",
    "        out = self.pool1(out)               # (N,64,150,150)\n",
    "\n",
    "        out = F.relu(self.bn_2_1(self.conv2_1(out)))  # (N,128,150,150)\n",
    "        out = F.relu(self.bn_2_2(self.conv2_2(out)))  # (N,128,150,150)\n",
    "        out = self.pool2(out)            # (N,128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.bn_3_1(self.conv3_1(out)))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.bn_3_2(self.conv3_2(out)))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.bn_3_3(self.conv3_3(out)))  # (N,256, 75, 75)\n",
    "        out = self.pool3(out)            # (N,256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.bn_4_1(self.conv4_1(out)))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.bn_4_2(self.conv4_2(out)))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.bn_4_3(self.conv4_3(out)))  # (N, 512, 38, 38)\n",
    "        # here we extract the feature from conv4_3\n",
    "        conv4_3_feats = out              # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)            # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.bn_5_1(self.conv5_1(out)))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.bn_5_2(self.conv5_2(out)))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.bn_5_3(self.conv5_3(out)))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)            # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))    # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        Use pre-trained wieght from Torch Vsion. \n",
    "        Convert fc6 and fc7 weights into conv6 and conv7\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16_bn(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
    "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
    "        # ...operating on the 2D image of size (C, H, W) without padding\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model with pre-trained weights\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializa AuxiliaryConvolutions with Kaiming_uniform_ and set non-linearity to 'relu'\n",
    "\n",
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    These layers are put on top of base model to produce more feature maps for object detections.(smaller maps)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)         # output=(19-1)/1+1 = 19\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)# output=(19+2-3)/2+1 = 10\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)          # output=(10-1)/1+1 = 10\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)# output=(10+2-3)/2+1 = 5 because by defaul we use \"floor\"\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(5-1)/1+1 = 5\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(5-3)/1+1 = 3\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(3-1)/1+1 = 3\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(3-3)/1+1 = 1\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(c.bias, 0.) \n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        conv7_feats: (N, 1024, 19, 19)\n",
    "        return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, we replace xavier_uniform with kaiming_uniform_\n",
    "\n",
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict class scores and bounding boxes using lower and higher level feature maps\n",
    "\n",
    "    The bounding boxes (offsets (g_{c_x}, g_{c_y}, g_w, g_h) of the 8732 default priors)\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for encoding definition\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 8732 hounding boxes\n",
    "    A high score for 'background' = no object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of proior_boxes we are considering per position in each feature map\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                    'conv7': 6,\n",
    "                    'conv8_2': 6,\n",
    "                    'conv9_2': 6,\n",
    "                    'conv10_2': 4,\n",
    "                    'conv11_2': 4}\n",
    "        # 4 prior-boxes prediction convoluitions (predict offsets w.r.t prior-boxes)\n",
    "\n",
    "        # This is the part to compute LOCALIZATION prediction\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3']*4, kernel_size=3, padding=1) # output = (38-3+2)/1+1 = 38, same padding\n",
    "        self.loc_conv7   = nn.Conv2d(1024, n_boxes['conv7']*4, kernel_size=3, padding=1)  # output = (19-3+2)/1+1 = 19\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2']*4, kernel_size=3, padding=1) # output = (10-3+2)/1+1 = 10\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2']*4, kernel_size=3, padding=1) # output = (5-3+2)/1 +1 = 5\n",
    "        self.loc_conv10_2= nn.Conv2d(256, n_boxes['conv10_2']*4,kernel_size=3, padding=1) # output = (3-3+2)/1 +1 = 3\n",
    "        self.loc_conv11_2= nn.Conv2d(256, n_boxes['conv11_2']*4,kernel_size=3, padding=1) # output = (1-3+2)/1 +1 = 1\n",
    "\n",
    "        # This is the part to comput CLASS prediction\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7   = nn.Conv2d(1024,n_boxes['conv7']   * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,n_boxes['conv10_2'] * n_classes,kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,n_boxes['conv11_2'] * n_classes,kernel_size=3, padding=1)\n",
    "\n",
    "        self.init_conv2d()\n",
    "    def init_conv2d(self):\n",
    "        # Use Kaiming_uniform_ here instead of xavier_uniform_\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds w.r.t prior boxes\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)            # (N, 16, 38, 38)  16 is from 4 priors 4*4=16\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)  to match prior-box order (after .view())\n",
    "        # .contiguous() ensures it is stores in a contiguous chunk of memory, needed for .view() below\n",
    "\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)          # This give us (N, 5776, 4) the (g_{c_x}, g_{c_y}, g_w, g_h) for all 5776 priors\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                    self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                                self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # A total of 8732 boxes\n",
    "        # Concatenate in this specific order    \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "\n",
    "        \"\"\"\n",
    "        This class works as a wrapper that encapsulates the base VGG network, auxiliary, and prediciton convolutions.\n",
    "        \"\"\"\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factor = nn.Parameter(torch.FloatTensor(1, 512, 1, 1)) # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factor, 20)\n",
    "\n",
    "        # The above two lines demonstrate a simple example of how do add a leanable-parameter into our computation\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()  # with shape of (8732, 4)\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        image: (N, 3, 300, 300)\n",
    "        return:  8732 locations and class scores (i.e.  w.r.t each prior box) for the given image(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run VGG base network convolutions (lower level feature map generators, up to conv7)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)   # (N, 512, 38, 38),  (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm using our learnable parameter\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm                         # (N, 512, 38, 38) this step was done by broadcasting\n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factor            # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliaury convolution (higher level feature map extraction)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats) \n",
    "        # (N, 512, 10, 10), (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offset w.r.t. priors and classes in each resulting location)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of our forward pass) to detect objects.\n",
    "\n",
    "        For each class. perform Non-Maximum Suppression (NMS) on boxes that are above a minimum score\n",
    "\n",
    "        predicted_locs: predicted locations w.r.t the 8732 prior boxes, a tensor of (N, 8732, 4)\n",
    "        predicted_scores: predicted class score for each of prediced locations, a tensor of (N, 8732, n_classes)\n",
    "        min_score: the minimun score for a box to be consifered a match for a CERTAIN CLASS\n",
    "        max_overlap: the maximum overlap that we allow. For any pair of boxes with higher overlap, the lower class score one will be suppressed\n",
    "        top_k: if there are a lot of resulting detection across all classes, keep only the top_k \n",
    "        \n",
    "        return: detections (boxes, labels, and scores), lists of length batch_size N\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0) # N\n",
    "        n_priors = self.priors_cxcy.size(0) # 8732\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)\n",
    "\n",
    "        # list to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_scores = list()\n",
    "        all_images_labels = list()\n",
    "\n",
    "        assert n_priors == predicted_scores.size(1) == predicted_locs.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Convert diviation from prior boxes to (c_x, c_y, w, h)\n",
    "            # Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max)\n",
    "            \n",
    "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_boxes = list()\n",
    "            image_scores = list()\n",
    "            image_labels = list()\n",
    "\n",
    "            max_score,  best_label = predicted_scores[i].max(dim=1) # (8732), (8732)\n",
    "\n",
    "            # operations for each class. Class 0 is not included here because it denotes background(negative)\n",
    "            for c in range(1, self.n_classes):\n",
    "                # Keep only predicted boxes and scores where scores for this class are above minimum_score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_scores > min_score # torch.uint8 (byte) tensor, for infexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                # here, we will retain the score & locs of the boxes with score higher than the threshold\n",
    "                class_scores = class_scores[score_above_min_score] # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualfied, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs) # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "                \n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multibox loss function for SSD300 architecture, which is a combination of:\n",
    "\n",
    "    1) a localization loss for the predicted locations of the boxes, and\n",
    "    2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        \"\"\"\n",
    "        priors_cxcy: priors' (c_x, c_y, w, h)\n",
    "        threshold: overlapping less than 'threshold' with priors are set to class-background\n",
    "        neg_pos_ratio: a parameter used when calculating hard negative mining. Detail in forward() section\n",
    "        alpha: the ratio between localization loss and confidence loss\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # the two loss functions for localization and classification\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "        \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        predicted_locs:   predicted locations/box w.r.t 8732 priors, (N, 8732, 4)\n",
    "        predicted_scores: preidted class scores for each of the encoded locations, (N, 8732, n_classes)\n",
    "        boxes:            ground truth boxes,  a list of N tensors\n",
    "        label:            ground truth labels, a list of N tensors\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors   = self.priors_cxcy.size(0)\n",
    "        n_classes  = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)   # (N, 8732)\n",
    "\n",
    "        # for each image in the minibatch\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0) # the number of objects exist in the given image\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # for each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0) # (8732),  (8732)\n",
    "\n",
    "            # we dont want a situation where an object is not represented in our positive (non-background) priors for reasons like:\n",
    "            # 1. An objext might not be the best object for all priors, and is theresore not in the object_for_each_prior\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5 by defaul)\n",
    "\n",
    "            # to remedy this\n",
    "            # first, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)   # (n_object)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (this fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used as DataLoader later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        \"\"\"\n",
    "        data_folder: folder where data files are stored\n",
    "        split: this must be either 'TRAIN' or 'TEST'\n",
    "        keep_difficult: keep or discard objects that are considered as difficult(a property come with the dataset)\n",
    "        \"\"\"\n",
    "        self.split = split.upper()\n",
    "\n",
    "        assert self.split in {'TRAIN','TEST'}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        with open(os.path.join(data_folder,self.split+'_images.json'),   'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split+'_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Read Image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
    "        objects = self.objects[i]\n",
    "        boxes  = torch.FloatTensor(objects['boxes']) # (n_objects, 4)\n",
    "        labels = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
    "\n",
    "        # Discard difficult objects, if specified\n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1-difficulties]\n",
    "            labels = labels[1-difficulties]\n",
    "            difficulties = difficulties[1-difficulties]\n",
    "\n",
    "        # Apply transformations\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader)\n",
    "\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "\n",
    "        @Params\n",
    "        batch: an iterable of N sets from __getitem__()\n",
    "        return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes  = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate annealing\n",
    "In the first phase, we used a constant learning rate in our training loop. \n",
    "Decay the learning rate by a ratio of 0.9, when our model cease to make an improvemnet. \n",
    "This was recommentded in the [original paper](https://arxiv.org/abs/1512.02325). <br>\n",
    "What we are going to do differently this time is to implement a learning rate scheduler that anneal our learning rate in a way that it has a period of \"warming up & catch up speed\", followed by a period of \"high learning rate\" then gradually slow down to find good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "@annealer\n",
    "def sched_no(start, end, pos): return start\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
    "\n",
    "#This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will build different annealing options\n",
    "Cosine scheduling will be implemented later same as it's done in FastAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd1zV1f/A8dfhspENLhAx9144c+DempmKe+Aqs/xlWWnDplaOtDQVV1rq18xypCKas3Ln3igqLmTI3pzfHxcNFQThDric5+NxH3A/67y52ZsP53Pe5wgpJYqiKIrpMjN2AIqiKIp+qUSvKIpi4lSiVxRFMXEq0SuKopg4legVRVFMnLmxA8iOm5ub9Pb2NnYYiqIoRcaxY8fCpZTu2e0rlIne29ubo0ePGjsMRVGUIkMIcT2nfarrRlEUxcSpRK8oimLiVKJXFEUxcSrRK4qimDiV6BVFUUxcroleCFFOCLFbCHFeCHFWCPFmNscIIcQ8IcQVIcQpIUSDLPuGCSEuZ76G6foHUBRFUZ4tL8Mr04BJUsrjQgh74JgQIkhKeS7LMV2AypmvJsAPQBMhhAvwMeADyMxzN0kpo3T6UyiKoig5yjXRSynvAHcyv48VQpwHPICsib4XsFJq5zw+KIRwEkKUAXyBICllJIAQIgjoDKzR6U+RadAvXxMWnYoZVphJK8ywxVzao5ElMMceUTjLBhRFUQCoUdaBj3vU1Pl1nyvzCSG8gfrAoSd2eQA3s7wPzdyW0/bsrj0GGAPg5eX1PGE9ciZhLRkWqdnvlAJz7DGXzlhIVyxlSSxlSawySmMly6LBNl9tKoqiFHZ5TvRCiBLAr8BEKWXMk7uzOUU+Y/vTG6VcDCwG8PHxyddqKIc9XyKhfHMSSlcnPjWe2JRYopKiiEyKJCIxgrsJd7kbf5fbcbcJjT1Fmkx7dG4p21JUdalKTdea2pdbTdxs3PIThqIoSqGSp0QvhLBAm+R/llJuyOaQUKBclveewO3M7b5PbN+Tn0BzlRiF1al1WO2fjXPj0dDuI3CukuPhaRlp3Im7w7WYa1yOusyVB1e4EHmBA7cOkCEzACjvUJ76JevTsFRDmpZpSmm70noJXVEURZ9EbksJCiEE8CMQKaWcmMMx3YDXga5oH8bOk1I2znwYewx4OArnONDwYZ99Tnx8fGS+5rpJjoU/P4dDi8DBA7rPgSodn+sSCakJXIi8wMn7Jzkedpx/w/4lOjkaAG8Hb5qWaUorz1Y0Kt0Ia3Pr549RURRFD4QQx6SUPtnuy0OibwHsB04DGZmbpwBeAFLKhZm/DL5H+6A1ARghpTyaef7IzOMBvpBSLs8t4Hwn+oduHoZNE+D+BajdFzrPALv8dcNkyAwuR13m0J1DHLxzkKP3jpKYloi1xpqmZZrSvnx7fMv54mjlmP94FUVRCqhAid4YCpzoAdKS4cAc2DcTrOy1yb5OPxDZPTbIu+T0ZI7cPcLem3vZE7qHu/F3MRfmNCnbhK4VutLOqx12FnYFi11RFOU5Fc9E/1DYBe3dfehhqNRe253jlL9RPU+SUnIm/AxBN4LYEbKDW3G3sNZY41vOl16VetGsTDM0ZhqdtKUoivIsxTvRA2Skw5GlsOsTkBLafQiNx4AOk7CUkpP3T7Ll6hYCQwJ5kPyA0nal6VWxF30q96FMiTI6a0tRFOVJKtE/9OAmbPk/uBIEHj7Q63soWV3nzaSkp7D75m5+u/wbf9/+GyEErT1b41fVj6Zlm2Im1BRDiqLoVrFI9FJKks6cRWNfAstnLUMoJZxeD9vfhaQYaPkWtJwE5lYFCzoHt+Jusf7SejZc3kBkUiQVHCswpMYQerzQQ43aURRFZ4pFos9ITORSs+Y4vdyb0h99lPsJ8eGw/X04vQ7cqkLP78CrST4jzl1KegqBIYGsOreK85HncbZyxq+aHwOrDcTJ2klv7SqKUjwUi0QPEDphAoknT1Fpz26EWR67Ry4HweaJEHMLHhZaWdk/d9t5JaXk2L1j/HjuR/bc3IONuQ19KvdheM3hlLIrpbd2FUUxbc9K9CbVWWzfvj1pYWEknT6d95Mqd4DxB7UPZw8HwPymcGmH3mIUQuBT2ofv2n7Hhp4baO/VnjUX1tBlQxe+PPQl9+Lv6a1tRVGKJ5NK9CV8fcHcnNidO5/vRCt76Po1+O8AqxKwui+s99d27+hRZefKfNnyS7b03kLPij355eIvdNnQhemHphOeqN+2FUUpPkyq6wbgxsiRpN6+wwvbtiLyUxyVlgz7Z8P+WTottMqL0NhQAk4HsPHKRiw1lgyuPpjhtYbjYOmg97YVRSnaik3XDUCJ9u1JCQkh5erV/F3A3AravA/j9oNrRfhtDPz8Cjy4odtAs+Fp78knzT/h916/09qzNQGnA+i6oSs/n/+Z1PQcpl9WFEXJhcklevt27QCIDXrO7psnlawOIwOhy9dw46C27/7gQm3xlZ55O3rzTetvWNd9HdVcqjHj8Axe2vgSu67vojD+BaYoSuFmconeolQprOvUef5++uyYaaDJWHjtIJRvrh17v6wThJ0v+LXzoLprdQI6BLCg3QIszCyYuGcio3eM5lLUJYO0ryiKaTC5RA/a0TdJZ86QeueObi7oVA4G/QIvB0DkVVjYEnZP1/bn65kQgpaeLVnfcz1TmkzhfOR5+m7uyxcHv3g0fbKiKMqzmGyiB4jduUt3FxVC+1B2/GGo2Rv2zoBFrbRTIhuAuZk5A6oN4I/ef9CvSj/WXVpHz997sil4k+rOURTlmUwy0Vu9UAHLShWJDQzU/cXt3KBPAAz8BZLjYGlH2PqOdtETA3CydmJq06ms7bYWT3tPph6YyvDtw7n6IJ8PnxVFMXkmmegBHDp1JuHYMVLDwvTTQJWOBi20elJ11+qs6rKKT5p/QnB0MH0292H+ifkkp+u/O0lRlKIl10QvhFgmhAgTQpzJYf87QogTma8zQoj0zCUEEUKECCFOZ+7Tw3SUOXPo3AmkJDYoSH+NZFdo9esovRdaPWQmzHi58sts7LWRTt6dWHhyIa9seoXj944bpH1FUYqGvNzRr0C7RGC2pJTfSCnrSSnrAe8De59YE7ZN5v5sB/Lri1Xlytrum23b9d9YucYwdh/4vg9nf4fvG8HJ/2lnyjQAVxtXZrScwaL2i0jNSGX49uFMPzSdhNQEg7SvKErhlmuil1LuA565mHcWA4A1BYpIh/TefZOVuRX4vvdEoVVfgxRaPdTcozkbem7Ar5ofqy+s5uVNL3Pk7hGDta8oSuGksz56IYQt2jv/X7NslsAOIcQxIcSYXM4fI4Q4KoQ4ev/+fZ3EZJDumydlLbS6/rdBC60AbC1smdJkCis6r8BMmDEycCRfHf6KpLQkg7SvKErho8uHsT2Av57otnlRStkA6AKMF0K0yulkKeViKaWPlNLH3d1dJwEZtPsmq4eFVuMPQvlmBi+0AmhYqiHre6zHr6ofP53/ib6b+3ImPNvHLIqimDhdJno/nui2kVLezvwaBvwGNNZhe3li0O6bJzl5waD12kKriGCDFlqB9u5+atOpBHQMICk9iSFbh7Do5CLSMtIM0r6iKIWDThK9EMIRaA1szLLNTghh//B7oCNg8FtKo3TfZPWw0Or1I0YptAJoWqYpv/b8lQ7eHfj+xPeM2D6C0NhQg7WvKIpx5WV45RrgH6CqECJUCOEvhBgnhBiX5bDewA4pZXyWbaWAA0KIk8Bh4A8ppYH7UP7rvonZus3QTT/uYaHVoPVZCq0ma783AAdLB75u9TUzWs7gyoMr9N3cl23XjPyZKIpiECY3H312wn/4gftz51Hpz11YlC2rs+vmW3Is7PpUW2jl6And52hXujKQW3G3mLxvMqfun6JXxV5MaTIFWwtbg7WvKIruFav56LPj0K0bADFbtxo5kkxW9tD1G+3oHAtb7Xz3v442WKGVRwkPVnRewejao9kUvAm/P/zUjJiKYsKKRaK39PLCum4dorf8YexQHufVRDvuvvV7cPY3mN8YTq0zSKGVhZkFbzR4g4COAcQkxzDwj4FsuLxBTZCmKCaoWCR6AMdu3Um+cIHky5eNHcrjHq5oNXYfOFeADaMNWmjVpEwT1vdcT72S9fj474+ZemCqqqhVFBNTbBK9Q5fOYGZG9B+F7K7+oVI1tHPmdJ5h8EIrNxs3FrVfxGv1XmPL1S0M2jqIa9HX9N6uoiiGUWwSvbm7O3ZNmxKz5Y/C2z1hpoGmr8Jr/4BXU4MWWmnMNLxa91UWdlhIRGIEflv82BFiuNk4FUXRn2KT6AEcevQgNTSUpJMnjR3KszmXh8G/Qu/FBi+0al62Oet6rKOScyUm7Z3E7GOzVYGVohRxxSrR23doj7C0JHrzFmOHkjshoG7/zEKrlwxaaFXarjTLOy2nf9X+LD+znHFB44hMyuu8doqiFDbFKtFrSpSgRJs2xGzbhkxNNXY4eWPnBn2WPLGi1WS9r2hlqbHkg6Yf8GnzT/k37F/8tvhxPsJwc/UoiqI7xSrRAzi+1Iv0yEji9u83dijP59GKVqPh8GJY0Awu639ah96Ve7Oyy0oyZAZDtw1l69VCUougKEqeFbtEX6JFCzQuLkT/9ruxQ3l+Riq0qulWk7Xd11LDtQbv7n+XOcfmkG6gaZcVRSm4YpfohYUFjj26E7tnD2lRUcYOJ3+MUGjlZuPGko5L6FelH8vOLOPN3W8Sl2KYeXoURSmYYpfoARxfeglSUwvPlAj58bDQatx+gxVaWWgs+LDZh0xtMpUDtw4wZNsQbsbe1Ft7iqLoRrFM9NbVq2NVtSrRv2/M/eDCrmT1zEKrr/4rtDq0SK+FVn7V/FjYYSFhCWEM/GMgx+4d01tbiqIUXLFM9KC9q086fZrk4GBjh1JwZhpoOu6/Qqttk2FZZwi7oLcmm5Zpyppua3CycmLUjlFsCt6kt7YURSmY4pvoe3QHjcY07uofeqzQ6gosbAF7ZkBail6a83Lw4qeuP9GwZEOmHpjK3ONzyZAZemlLUZT8K7aJ3tzNjRItWhC9cSMyzYQqPx8WWo0/DDV6wZ7pmYVWR/TSnKOVIz90+IFXqrzCktNLeHffuySnG2apREVR8iYvK0wtE0KECSGyXQZQCOErhIgWQpzIfH2UZV9nIcRFIcQVIcR7ugxcFxxf6UNaWBhxBw4YOxTdK+EOryyFgeu0xVVLO8C2d/WyopWFmQUfNf2Itxq+xfaQ7YwKHKUqaRWlEMnLHf0KoHMux+yXUtbLfH0KIITQAPOBLkANYIAQokZBgtU1e19fNK6uPFi/3tih6E+VTv8VWh1aBAuawuWdOm9GCMGIWiOY1XoW5yPPM3jrYK7HXNd5O4qiPL9cE72Uch+Qn9uzxsAVKeVVKWUKsBbolY/r6I2wsMCp90vE7d5D2v37xg5Hf54qtOqTWWgVofOmOnp3ZGmnpcSlxDF462BOhJ3QeRuKojwfXfXRNxNCnBRCbBNC1Mzc5gFkHWQdmrktW0KIMUKIo0KIo/cNmHQd+/SB9HQe/F4EK2Wf16NCq3czC60awalfdF5oVde9Lj91/QkHSwf8A/0Juq7/qRoURcmZLhL9caC8lLIu8B3wMGOKbI7NMaNIKRdLKX2klD7u7u46CCtvrCpUwNbHhwfr1xfeeep1ydwK2kzJsqLVKL0UWnk5eLGq6yqquVZj0p5J/Hz+Z51eX1GUvCtwopdSxkgp4zK/3wpYCCHc0N7Bl8tyqCdwu6Dt6YNT31dIvX6DhCP6GZlSKGW3opWOC61crF1Y2nEpvuV8mXF4BnOOzVHDLxXFCAqc6IUQpYUQIvP7xpnXjACOAJWFEBWEEJaAH1Aoq2rsO3bEzN7etB/KZufJFa30UGhlbW7NHN85j+bImXpgKqnpRWSKaEUxEXkZXrkG+AeoKoQIFUL4CyHGCSHGZR7yCnBGCHESmAf4Sa004HUgEDgPrJNSntXPj1EwZjY22onOtgcW3YnOCkLPhVYaMw0fNP2ACfUnsOXqFib8OUEtQK4oBiQKY7+0j4+PPHr0qEHbTLp4iWu9elFy8mRcR44waNuFSnw4bH8PTv8C7tWh53dQrpHOLr/h8gY++ecTarrWZH67+ThbO+vs2opSnAkhjkkpfbLbV2wrY59kXbUKNg0bErV2LTKjGPcjP1rRSj+FVi9Xfplvfb/lUtQlhm4byp24Ozq5rqIoOVOJPgtnPz9Sb9wg/u9/jB2K8T0stGo0SueFVm282rC4w2IiEiMYvG0wwQ9MYGI5RSnEVKLPwr5TRzQuLkStWWPsUAoHK3voNhNGbgcLG50WWjUo1YDlnZeTITMYtn0Yp+6f0kHAiqJkRyX6LMwsLXHq04e43btJvaO6FB7xagrjDui80KqqS1VWdl6JvYU9o3aM4u/bf+soYEVRslKJ/glO/fuDlEStW2fsUAqX7AqtVveDBwVbYaqcQzlWdlmJp70nr+96nZ3XdT8Pj6IUdyrRP8HS04MSrVrx4Jf1yBT9zONepGUttAo5oO27P7QYCvAA293WneWdllPDtQaT9k7it8u/6TBgRVFUos+G8+BBpIeHE7N9u7FDKZweFVodhHKNYds7sKxTgQqtHK0cWdxhMU3LNOWjvz9i1blVOgxYUYo3leizYffii1hWqEDkylXFY/6b/HIuD4M3QO9FEHEZFrWEPV/lu9DK1sKW79p+R3uv9nx95GsWnlyoPn9F0QGV6LMhzMxwHjKYpDNnSPxXTbP7TEJAXT8YfwSq94Q9XxZoRStLjSXftP6GnhV7Mv/EfGYfm62SvaIUkEr0OXDq1QszBwciV640dihFgw5XtDI3M+ezFz/Dr6ofK86u4LODn6nJ0BSlAFSiz4GZnR1Or7xCbFCQGmr5PHS0opWZMGNKkyn41/Lnl0u/8MGBD0jLMKG1fRXFgFSifwaXQQO1Qy1XrzZ2KEVLditabRjz3IVWQggmNpzIhPoT2Hx1M5P3TVYzXypKPqhE/wwWHh7Yt29P1LpfyEhQsy0+t6wrWp3ZkO9CqzF1xjC50WSCrgfx5u43SU5P1lPAimKaVKLPhcvwYWRER/NggxrbnS86KrQaUmMIHzX7iAO3DvD6rtfVNMeK8hxUos+FbYMG2NSrR+Ty5cg01UecbzootOpbpS+ft/icw3cP8+rOV4lPjddjwIpiOlSizwPXUf6k3rpF7I4dxg6laHus0KpJvgqtelbsyVctv+Lk/ZOMCRpDTEqMHgNWFNOQlxWmlgkhwoQQZ3LYP0gIcSrz9bcQom6WfSFCiNNCiBNCCMOuJKJDJdq2xdLbm4ily9SYbl14ckWr5yy06lyhM7N8Z3Eu4hyjd4wmOjlazwErStGWlzv6FUDnZ+y/BrSWUtYBPgMWP7G/jZSyXk4rnxQFwswMlxEjSDp7loRDh40djmkQAur2h/GH81Vo1c6rHXPbzOVK1BX8A/2JTIrUc8CKUnTlmuillPuAHP8vklL+LaV8uNDqQcBTR7EVKo4v9ULj6krE0qXGDsW0ZFto9V6eCq1aebbiu7bfERITgn+gP+GJ4QYIWFGKHl330fsD27K8l8AOIcQxIcSYZ50ohBgjhDgqhDh6//59HYdVcGZWVrgMGUz8/v0kXcj/5F1KDh4rtFoIC5rBldwLrZp7NGd+u/ncirvFyMCR3E8ofP92FMXYdJbohRBt0Cb6d7NsflFK2QDoAowXQrTK6Xwp5WIppY+U0sfd3V1XYemU84ABmNnZEb5okbFDMU2PCq22g4U1/NQHNoyFhGd3yzQp04QF7RZwN/4uIwNHci/+noECVpSiQSeJXghRB1gC9JJSPip/lFLezvwaBvwGNNZFe8aicXTEedAgYrcHknz1mrHDMV0PV7RqNRnOrIfvG8Hp9c8stPIp7cOiDosISwhjROAI7sbfNWDAilK4FTjRCyG8gA3AECnlpSzb7YQQ9g+/BzoC2Y7cKUpchg9DWFkRsfjJZ86KTplbQdupmYVW5eFXf1jdH6JDczylfsn6LO64mKikKEZsH8GdODVHkaJA3oZXrgH+AaoKIUKFEP5CiHFCiHGZh3wEuAILnhhGWQo4IIQ4CRwG/pBSFvmVPMxdXHDu34/ozZtJCc056Sg6Uqom+AdBp+kQsh/mN4HDATkWWtV1r8uiDot4kPyAEYEjuB1328ABK0rhIwrjuHAfHx959GjhHXafeu8ewe074NjnZcpMm2bscIqPqBDYPBGu7tYWXPX8DtyrZnvomfAzjAkag72FPcs6L8OjhIdhY1UUAxNCHMtpGLuqjM0Hi1KlcHz5ZaJ/3UDqPfXgz2CcvWHIb/DSQgi/BAtb5FhoVcutFgEdA4hLjWPk9pHcirtl+HgVpZBQiT6fXEePRkpJhBqBY1hCQL0BmSta9fiv0Cr06b8Aa7rWfJTsR2wfQWis6mpTiieV6PPJ0tMDp5dfJuqX9aTeVv3ABlfCHV5ZBgP+B8kxsKR9toVWNVxrENAxgPjUeEYGjuRm7PPNmqkopkAl+gJwGzcWAYT/sNDYoRRfVTtrJ0lr5A+Hfsi20KqGaw2WdFxCQloC/oH+KtkrxY5K9AVgUbYsTn378uC330i5qZKH0Vg7QLdZMCLnQqvqrtUJ6BCgkr1SLKlEX0CuY8cizMwIX/CDsUNRyjeDsfuh1TvZFlo9mexVn71SXKhEX0AWpUriPMCP6I0bSb6mqmWNzsIa2n6QY6HVw2T/sM9eJXulOFCJXgdcR49GWFtzf+48Y4eiPPSMQqvqrtVZ0nEJ8anx+Af6q6GXislTiV4HzN3ccB0+jNjt20k8fdrY4SgPmWmg2Wvw2j/g2Qi2vg3LO8P9i9o7ezXOXikmVKLXEZeRI9E4OxM2a7Zahaqwya7Qau/X1HCsREDHAGJTY/EP9Fdz4ygmSyV6HdGUKIHbq+NIOHiQ+L/+NnY4ypMeFVodhmrdYfcXsLg1NRITCOgQQExKjJr1UjFZKtHrkJOfHxYeHoTNmoXMYdItxchKlIS+y2HAWkh8AEvaU/PIKha3/paY5BhGbFfJXjE9KtHrkJmlJe5vvkHy+fPE/PGHscNRnqVqFxh/CHxGwqEfqPU/fxZV9+dB8gO1eIliclSi1zGH7t2xrlGDsFmzyUhMNHY4yrNYO0D32dpCK3Mram98i0UWFYhMjMB/h79K9orJUIlex4SZGaXef4+0u3eJWL7c2OEoeVG+WeaKVu9Q53wgC8MiCY+7w6gd/moNWsUk5CnRCyGWCSHChBDZrhAltOYJIa4IIU4JIRpk2TdMCHE58zVMV4EXZraNGmHfoQMRAUtIvRdm7HCUvHhYaDVmL/XsPFkYeoOwmJuM3DZUJXulyMvrHf0KoPMz9ncBKme+xgA/AAghXICPgSZo14v9WAjhnN9gi5KS77wNaWncnzvX2KEoz6N0LRi1k3q+0/ghLJJ7MTfx39iH8Hj1C1spuvKU6KWU+4DIZxzSC1gptQ4CTkKIMkAnIEhKGSmljAKCePYvDJNh6eWF85AhRP/2G4lnzxo7HOV5mGmg2XgajNrPAjMP7iZF4P9LJ8JDDxk7MkXJF1310XsAWacDDM3cltP2YsHt1XFonJ259/kXarhlUeTsjc/Q7cyvPIQ7MpXR24YR8ecn2a5opSiFma4Svchmm3zG9qcvIMQYIcRRIcTR+/dNo09UY29PyUmTSPz3X6I3bjJ2OEp+CEGjFu/xfetZhFpaMSp4NZGLs1/RSlEKK10l+lCgXJb3nsDtZ2x/ipRysZTSR0rp4+7urqOwjM+x90vY1K1L2MyZpMfEGDscJZ8av9CJ7zou4qaVLaOs4oha1hG2vw8p8cYOTVFypatEvwkYmjn6pikQLaW8AwQCHYUQzpkPYTtmbis2hJkZpT78kPTISO5//72xw1EKoGmZpnzXfgE3rKwY/UJVHhxeCAuawpVdxg5NUZ4pr8Mr1wD/AFWFEKFCCH8hxDghxLjMQ7YCV4ErQADwGoCUMhL4DDiS+fo0c1uxYlOrJk79+xH182qSLl40djhKATQr24x5bb7jmkxhTM1mRGss4aeX4bdxj61opSiFiSiMMy36+PjIo0dNqw80/cEDgjt3wbJCBcr//BPCTNWqFWUHbh3gjT/foJJjRQJsquH4zwKwdoIuX0GtPtpJ1BTFgIQQx6SUPtntU9nGQDROTpR8710S//2XB+vWGTscpYBaeLRgbpu5XIkOZmzyZWJGbAUnL+2KVmv8Hq1opSiFgUr0BuTYqxe2zZoSNnOWqpg1AS09W/Jtm2+5GHWRsSe/JWboBuj0JVzb99iKVopibCrRG5AQgjIff4xMSeHel18aOxxFB1p5tmKO7xwuRF1g3K7xxDYc+sSKVl3g/iVjh6kUcyrRG5iltzdur71KbGAgsX/uNnY4ig74lvNlduvZnI88z7igccTauWauaPUD3L8AC1+EvV+rQivFaFSiNwLXkSOxqlyZu598osbWm4g2Xm2Y1XoW5yLOMW7nOOJS46HeQHj9SJYVrXwh9JixQ1WKIZXojUBYWlLmyy9JCw/n3ldfGTscRUfaerVlpu9MzoWfY+zOscSlxP23opXfGkiMgiXtVKGVYnAq0RuJTe1auPr7E/3rBuL27zd2OIqOtPNqx8zWTyR7gGpd/1vR6uACVWilGJRK9Ebk9vp4LCtV5M6HH5EeG2vscBQdaVc+h2SfdUUrjZUqtFIMRiV6IzKztKTsl1+SFhbGvRkzjB2OokM5Jnv4b0Wrlm/D6V/g+0Zwej0UwuJFxTSoRG9kNnXq4DpqFNG/biB2505jh6Po0JPJPjYly19tFtbQ7kMYs1cVWil6pxJ9IeD++nisa9TgzocfkWYiUzQrWlmT/bigcY8ne3i0otV/hVZNVaGVonMq0RcCwtKSst98TUZCArenTqUwzj+k5F+78u20o3EizjE2aCwxKU8Mqc1c0UpbaOWjCq0UnVOJvpCwqliRku+8Q/y+/UStWWPscBQda+fVjlm+szgfeZ4xO8YQnRz99EHO3v8VWoVfzCy0+kYVWikFphJ9IeI8aCB2LVsS9tXXJF1Ud3Ompq1XW+b4zuFi1EXGBOWQ7IXQFlqNP5xZaPW5KrRSCkwl+kJECEHZGdMxc7Dn1nleTHMAACAASURBVP/9HxkJCcYOSdEx33K+zG0zl8tRlxm9Y3T2yR7+K7QasFZbaLW0PWyfogqtlHxRib6QMXd1xePrr0m5do27n39h7HAUPWjl2Yq5beYS/CAY/0B/opKicj64ahdtoVXDEXBwviq0UvIlrytMdRZCXBRCXBFCvJfN/jlCiBOZr0tCiAdZ9qVn2adWyM4Du2bNcB03lugNG4jepD4yU9TSsyXftf2OkJgQ/Hf4E5EYkfPBjwqttmUptHpVFVopeZbrClNCCA1wCeiAdrHvI8AAKeW5HI6fANSXUo7MfB8npSzxPEGZ4gpTz0umpXF9+HCSzp2nwrr/YVWpkrFDUvTg4J2DTNg1AY8SHizptAQ3G7dnn5CaBPu+gb++1a5o1fVrqPmyWtFKKfAKU42BK1LKq1LKFGAt0OsZxw8A1LCRAhLm5njMmo2ZrS2hE94gPS4u95OUIqdpmaYsaL+A2/G3GbF9BGEJuSxI82Sh1fqRsGYARN8yTMBKkZSXRO8B3MzyPjRz21OEEOWBCsCfWTZbCyGOCiEOCiFeyqkRIcSYzOOO3ldFQwBYlCqJx+xZpNy4wZ0pany9qWpUuhEL2y8kLCGMEdtHcDf+bu4nPSy06vgFXN2jXdHqyBJVaKVkKy+JPru/CXPKOH7AeillepZtXpl/TgwEvhVCVMzuRCnlYimlj5TSx93dPQ9hFQ92jRtT8q23iN2xg8hly40djqInDUo1YHHHxUQlRTF8+3BCY/MwFYKZBpq/nllo1RD+mAQruqpCK+UpeUn0oUC5LO89gds5HOvHE902UsrbmV+vAnuA+s8dZTHnMnIE9h07EjZrFnEH/jJ2OIqe1HWvS0CnAGJTYhm+fTjXY67n7USXCjDkd+i1AMLOawut9n0D6an6DVgpMvKS6I8AlYUQFYQQlmiT+VNDQYQQVQFn4J8s25yFEFaZ37sBLwLZPsRVciaEoOz0L7GqVIlbb71F8rVrxg5J0ZOarjVZ1mkZqRmpDN8+nOAHwXk7UQioPyhzRatu8OfnsKg13FKFVkoeEr2UMg14HQgEzgPrpJRnhRCfCiF6Zjl0ALBWPt6RXB04KoQ4CewGZuQ0Wkd5NjM7OzwXLEBoNIS+Nl7NX2/CqrpUZVmnZQCM2D6Ci5EX835yiZLQd0XmilaRsEQVWil5GF5pDGp4Zc7iDx/mxkh/7Jo1o9wPCxDm5sYOSdGT6zHX8Q/0JzEtkYXtF1LbvfbzXSApGnZOg6PLwKk89PgWKrbVS6yK8RV0eKVSiNg1bkzpDz8kfv9+7n35pRqJY8LKO5Tnxy4/4mDpwOig0Ry9+5w3P9aO0H0ODN8KGgtY1VsVWhVTKtEXQc79++HiP5Ko1WuIXPGjscNR9MijhAcrOq+gpG1JXt35Kn/f+vv5L+L9Ioz7C1pOgtPrYH5jOPOrWtGqGFGJvogqOWkS9p06Efb118QEBRk7HEWPStmVYnmn5ZR3KM/rf77Oruv5mOvGwhrafQRj9oCjpyq0KmZUoi+ihJkZZb+agU2dOtx++x0SjqnRFabM1caVpZ2WUt21OpP2TmJz8Ob8Xah0bfDfCR0/V4VWxYhK9EWYmbU1nj8swKJMGW6++pqaw97EOVo5EtAhAJ9SPkw5MIX/Xfhf/i6kMYfmE1ShVTGiEn0RZ+7igtfSJZhZW3Nz9GhSQtWf4qbM1sKW+e3n4+vpy+eHPmfJ6SX5fyD/qNBqviq0MnEq0ZsACw8Pyi0JICMpiZv+/qSFhxs7JEWPrDRWzG4zm64VujL3+FzmHJuT/2QvBNQfrF3RqmpXVWhlolSiNxHWVapQbuFCUsPCuDHSn7SoZyxmoRR5FmYWTG85nf5V+7P87HI++ecT0jPScz8xJ/aloN+P4LdaFVqZIJXoTYhtg/qUWzCflJAQbvqPIj0mxtghKXpkJsyY2mQqo2uP5tfLv/LOvndISS/gQuLVumWuaDU8c0WrZhD8Z66nKYWbSvQmxq5ZMzy/m0fS5cvcHD1GzWNv4oQQvNHgDd72eZug60G8tus14lMLeBeuCq1MTpGZAiE1NZXQ0FCSkpKMFFXhZG1tjaenJxYWFo9tj9mxg1v/9xY2tWtTLmAxGnt7I0WoGMqm4E189NdHVHepzvz283Gxdin4RVOTYN/X8NdcsHGGLl9Dzd5qRatC6FlTIBSZRH/t2jXs7e1xdXVFqH9kAEgpiYiIIDY2lgoVKjy1P2bHDm69NQnrmjXwCghA4+BghCgVQ9pzcw9v732bMnZlWNRhEWVLlNXNhe+ehk0T4Pa/UKULdJsFjtmuP6QYiUnMdZOUlKSS/BOEELi6uub4V45Dx454zv2WpHPnuTHSn/QHD7I9TjEdvuV8WdxhMRFJEQzZOoRLUToaG68KrYq0IpPoAZXks5HbZ2Lfrh2e8+aSfPEi14cMJTUslzVJlSKvQakG/NhZOwfS8G3Dn38ytJxkLbTyaKAKrYqQIpXolfyxb9OGcosXkXLrFtcHD1FFVcVAZefKrOq6CjdbN8YGjWVHyA7dXdylAgzdqAqtihCV6J+DEIJJkyY9ej9z5kymTZv26P3ixYupVq0a1apVo3Hjxhw4cMAIUWbPrlkzyi9fRnp0NNcHDiT58mVjh6ToWdkSZVnVZRU1XGvw9t63+fn8z7q7uCq0KlLylOiFEJ2FEBeFEFeEEO9ls3+4EOK+EOJE5mtUln3DhBCXM1/DdBm8oVlZWbFhwwbCs6k83bJlC4sWLeLAgQNcuHCBhQsXMnDgQO7evWuESLNnU7cu5VeuBCkJGTiI+MOHjR2SomeOVo4EdAygTbk2zDg8g9lHZ5Mhddivnl2hVeBUVWhVyOS6PJEQQgPMBzqgXSj8iBBiUzZLAv5PSvn6E+e6AB8DPoAEjmWeW6CyzU82n+Xcbd0WA9Uo68DHPWo+8xhzc3PGjBnDnDlz+OKLLx7b99VXX/HNN9/g5uYGQIMGDRg2bBjz58/ns88+02msBWFdtQrea9dwY8xYbvqPouxXM3Do2tXYYSl6ZG1uzWzf2Uw/PJ3lZ5dzJ/4On7f4HCuNle4aqdYNvFtA0Mfwz/dwfrNa0aoQycsdfWPgipTyqpQyBVgL9Mrj9TsBQVLKyMzkHgR0zl+ohcP48eP5+eefiY6Ofmz72bNnadiw4WPbfHx8OHv2rCHDyxMLDw+8f/4J67p1uPXWJMIDAtRKVSZOY6ZhapOpvNXwLbaHbGfMjjFEJ0fnfuLzsHbUJveshVa/v6YKrQqBvCw46gHczPI+FGiSzXF9hBCtgEvA/0kpb+ZwbraDb4UQY4AxAF5eXs8MKLc7b31ycHBg6NChzJs3Dxsbm2ceK6UstCOFNE5OeC1dyp333+f+rNmkXL1GmU+mISwtjR2aoidCCEbUGkEZuzJMOTCFwVsHs6DdAso5lNNtQw9XtNr7lbbQ6vIOVWhlZHm5o8/uv8yTt3+bAW8pZR1gJ/Bwfbu8nKvdKOViKaWPlNLH3d09D2EZz8SJE1m6dCnx8f/1Q9aoUYNjTyz+cfz4cWrUqGHo8PLMzMqKsrNm4TZ+PNG//cb1kSPVZGjFQOcKnQnoGEBUchSDtg7iRNgJ3TdiYQ3tP4axe8HBA9aPUCtaGVFeEn0okPVXvidwO+sBUsoIKWVy5tsAoGFezy2KXFxc6NevH0uXLn20bfLkybz77rtEREQAcOLECVasWMFrr71mrDDzRAiB+4TXKTtzJkmnThPS5xWSzj35+EUxNQ1LNeSnLj9hb2mPf6A/20O266eh0rVh1K4nCq2WqkIrA8tLoj8CVBZCVBBCWAJ+wKasBwghymR52xM4n/l9INBRCOEshHAGOmZuK/ImTZr02Oibnj17MnLkSJo3b061atUYPXo0P/30E2XKlHnGVQoPx+7dKP/zT8iMDEIGDCR6cz6XqlOKDG9Hb37q+hO13Grxzt53WHhyoX6e1TwqtPo7s9DqLVjRDcLVEF+DkVLm+gK6ou17DwamZm77FOiZ+f104CxwEtgNVMty7kjgSuZrRF7aa9iwoXzSuXPnntqmaOnys0kND5chgwbLc1WryTuffS4zkpN1dm2lcEpOS5bv73tf1lpRS767712ZlJakv8YyMqQ8vkrK6eWk/NRdyr1fS5mWor/2ihHgqMwhpxaZSc3Onz9P9erVjRRR4abrz0amphI2cyaRP67EunZtPObMwdJTTWBlyqSULDm9hHn/zqOOex3mtpmLm42b/hqMvQfbJsO536FULeg5Dzwa5n6ekiOTmNRMMRxhYUGp99/HY95cUkJCuPbyy8Tu2mXssBQ9EkIwus5oZvvO5nLUZfy2+HE+4nzuJ+ZX1kKrhAhVaKVnKtErOXLo2JEKG37Fslw5Qse/zp1p08hITDR2WIoedSjfgZVdViKEYOi2oQSG6PmR2sMVrRoM0xZaLWgGwbv122YxpBK98kyW5cpRfs1qXEaO5MHa/3Htlb4kXbhg7LAUParmUo013dZQzaUab+99m3nH5xVsPdrcPFVo9ZIqtNIxleiVXJlZWlJq8juUW7qE9JhorvXtR/jCRci0NGOHpuiJm40bSzstpU/lPgScDuCN3W8QmxKr30YfFlq1eAtOroX5jeHMBiiEzxGLGpXolTwr8eKLvLBpE/bt23H/228JGTiI5KtXjR2WoieWGks+bvYxHzT5gL9v/c2APwZwJeqKfhvNrtBq7UBVaFVAKtE/hxIlSjy1bdq0acycOROA4cOH4+HhQXKytnYsPDwcb29vAEJCQrCxsaFevXqPXitXrnx0nX///RchBIGBj/eJajQa6tWrR61atejRowcPjLxKlLmzM55z5uAxexap169z7aXe2rv7VDUXuSkSQtC/Wn+WdFpCXEocA7cO1H+/PTxeaBW8GxY0VYVWBaASvY5pNBqWLVuW7b6KFSty4sSJR6+hQ4c+2rdmzRpatGjBmjVrHjvHxsaGEydOcObMGVxcXJg/f75e488rh65deWHLZkq0acP9b7/lWt9+JJ4+Y+ywFD1pWKoh63qso4pzFd7e+zazjs4iLUPPXXdZC63K1leFVgWQl0nNCp9t72kXK9al0rWhy4wCX2bixInMmTOH0aNH5/kcKSXr168nKCiIli1bkpSUhLW19VPHNWvWjFOnThU4Rl0xd3fHc+63xAQFce/Tzwjp3x/nAQNwn/gmGnt7Y4en6FhJ25Is77Scr458xYqzKzh1/xQzW8/E3VbPc1O5vKBd0erEagicAj+8CK0nw4tvah/eKrlSd/Q65uXlRYsWLVi1atVT+4KDgx/rutm/fz8Af/31FxUqVKBixYr4+vqydevWp85NT09n165d9OzZU+8/w/Ny6NCBF/7YgvOAAUStXk1wl65Eb96spj42QRYaCz5o+gHTW07nfOR5+m7uy5G7R/TfsBBQf1DmilZd4M/PYLEv3Dqu/7ZNQNG8o9fBnbc+TZkyhZ49e9KtW7fHtj/sunnSmjVr8PPzA8DPz49Vq1bx8ssvA5CYmEi9evUICQmhYcOGdOjQQf8/QD5oHBwo/eEHOPbuzd1p07j9zmSi1qyl1NQp2NQ03rTSin50f6E71Zyr8X97/o9RO0bxat1XGV17NBozjX4bflhodeEP7eLkS9pB09egzRSwtNNv20WYuqPXg0qVKlGvXj3WrVuX67Hp6en8+uuvfPrpp3h7ezNhwgS2bdtGbKx2KNvDPvrr16+TkpJSaProc2JTqybe/1tL6c8+JSUkhJBX+nL7gw9IDQszdmiKjlVyrsTa7mvp7N2Z+SfmM3bnWMITn15mUy9UodVzUYleT6ZOnfpoNM6z7Ny5k7p163Lz5k1CQkK4fv06ffr04ffff3/sOEdHR+bNm8fMmTNJLeQjXIRGg3PfvlQM3I7LsGFEb9xEcOcu3P/uezLiVYm7KbGzsGNGyxlMazaNE2EneGXTK/x962/DNJ5todV4VWiVDZXon0NCQgKenp6PXrNnz87x2Jo1a9KgQYPHtj3ZRz9v3jzWrFlD7969HzuuT58+rF69+qlr1q9fn7p167J27Vrd/EB6prG3p9R771Lxjy2UaNWK8PnzudKpM5E//UxGSoqxw1N0RAhBnyp9WN1tNU5WTozdOZbZR2eTmm6gG5LHCq3WqEKrbKjZK01AUflsEk+cIGzWbBKOHMG8bBncx4/HsVcvhHnRfFSkPC0xLZGZR2ay7tI6arrWZEbLGXg7ehsugLunYePrcOcEVO0K3WaBQ1nDtW9EavZKpVCwqVcPr5U/Um7pEsxdXLkz9QOCu3Tlwa+/qoIrE2FjbsOHzT5kju8cQuNC6belH79c+sVwI7CeLLRSK1oBKtErBiaEoMSLL+L9yzo8F8xH4+CgTfiduxC1Zg0ZSUnGDlHRgfbl2/Nrj1+p516PT//5lDf+fMNwD2qzK7T6sTuE63n6hkIsT4leCNFZCHFRCHFFCPFeNvvfEkKcE0KcEkLsEkKUz7IvXQhxIvO16clzleJJCIF927Z4r/8Fz4U/oHFz5e4nn3KlfQfCFweQHh1t7BCVAiplV4qFHRbybqN3+fv23/Te2Nsw0yc89LDQqtd8uHcGfmgO+2eBoZ4dFCK59tELITRolxHsgHax7yPAACnluSzHtAEOSSkThBCvAr5Syv6Z++KklE9PEvMMqo/++ZjCZyOlJOHwESICAog/cABha4vTyy/jMnQIll5exg5PKaCrD64y9cBUzkScobN3Z6Y0mYKztbPhAoi9B9vegXMboVTtzBWtGuR+XhFS0D76xsAVKeVVKWUKsBbolfUAKeVuKWVC5tuDgGdBAlaKHyEEdk0a47UkgAq//4ZDx45E/e9/BHfqzM1XXyPur7+QxbyftSh7wekFVnVdxYT6E9h5YycvbXyJwJBAw/Xd25eCfiuh/88Qf19baLXjA0hJyP1cE5CXRO8B3MzyPjRzW078gW1Z3lsLIY4KIQ4KIV7K6SQhxJjM447ev38/D2Eppsq6WjXKzphOpV07cR03lsRTp7jpP4qr3boT+eOPpBt5Bk8lf8zNzBlTZwxru62ltF1p3t77Nm/teYv7CQb8/71698xCq6Hw93fwQzO4usdw7RtJXhK9yGZbtr+GhRCDAR/gmyybvTL/nBgIfCuEqJjduVLKxVJKHymlj7u7nidJyqe7d+/i5+dHxYoVqVGjBl27duXSpUucPXuWtm3bUqVKFSpXrsxnn3326E7l3r17dO/enbp16z46R8kbi5IlKfnmm1Ta/Sdlv/kajYMD96bP4HKr1tyaPJn4w4fVfDpFUFWXqvzc9WcmNpjIvtB99Pq9F+suriNDGugvNhsn6DEXhv8BQgMre2kLrRKjDNO+MUgpn/kCmgGBWd6/D7yfzXHtgfNAyWdcawXwSm5tNmzYUD7p3LlzT20zpIyMDNm0aVP5ww8/PNr277//yn379skXXnhBBgYGSimljI+Pl507d5bff/+9lFLKMWPGyG+//fbROSdPntR5bMb+bAwp8cIFeeeTT+WFhj7yXNVq8nL7DjJs/nyZEhpq7NCUfLj24JocuX2krLWilhyydYi8FHnJsAGkJEgZ9LGU05yl/LqSlGc2SJmRYdgYdAQ4KnPIqXl5GGuO9mFsO+AW2oexA6WUZ7McUx9YD3SWUl7Ost0ZSJBSJgsh3IB/gF4yy4Pc7OT2MParw19xIVK365ZWc6nGu43fzXH/n3/+ybRp09i3b99j25cuXcrevXsfW0QkODgYX19fbt68Sc+ePRk2bBh9+vTRabxZmcLD2OeVkZhIbFAQDzb8RsLBgwDY+vjg0LMHDp06oXF0NHKESl5JKdkUvImZR2cSmxLL4OqDebXeq9hZGHCSsjsnYdME7deq3aDbzCJXaFWgh7FSyjTgdSAQ7R37OinlWSHEp0KIh3PmfgOUAH55YhhldeCoEOIksBuYkVuSL6zOnDlDw4YNn9p+9uzZp7ZXrFiRuLg4YmJiGD9+PP7+/rRp04YvvviC27dvGypkk2ZmY4Njz56UX7Gcijt34j7xTdIiIrj70cdcatGSm+NeJXrzZtLj1Nw6hZ0Qgl6VerH5pc28VOklfjz3Iz1/68nWq1sN1zVXpi6M+hM6fAbBu7SFVkeXmUyhlZoCIY/mzZvHtWvXmDNnzmPb/+///o8KFSrwxhtvPLbd2dmZGzduYG9vT2RkJNu3b2fbtm3s2LGDM2fOoMvnEMb+bAoLKSVJZ84Ss3UrMdu3k3bnDsLSErsXX8S+Qwfs27ZB4+Rk7DCVXJy6f4rPD37O+cjz1C9Zn/cav0cN1xqGCyDyKmx+E67tg/IvQo954FbJcO3nk5oCQQdq1qzJsWPHst3+5C+lq1evUqJECewzV1lycXFh4MCBrFq1ikaNGj3V/aPohhACm9q1KPXuZCrt2kn51T/jPMCPpIsXuDNlCpdebMH1ocOI/PFHUm7ezP2CilHUca/Dmm5rmNZsGtdjruO3xY8P//qQe/H3DBOAywswdBP0/N5kCq1Uos+jtm3bkpycTEBAwKNtR44coXLlyhw4cICdO3cC2oVC3njjDSZPngxo+/YTErRjdWNjYwkODsZLFQDpnTAzw7ZBA0q9/z6Vdu3C+5d1uI4aRXpUJPemzyC4Q0eCu3bj3oyviP/nHzWbZiGjMdPQp0oftvTewrCaw/jj6h/0+L0H80/MJyHVAGPfhYAGQ7QrWlXpBLs+hcVtiuyKVqrr5jncvn2biRMncuzYMaytrfH29ubbb78lKSmJCRMmcOfOHdLT0xkyZAgfffQRQgi++eYbli9fjrm5ORkZGYwYMYJJkybpNK7C8NkUJSk3bhC3Zw9xe/eRcPgwMjUVYWODbeNGlHjxReyaNcOyUiWEyG5ksWIMN2NvMu/4PLaHbMfF2oXRtUfTr2o/LDWWhgng/Gb4422ID4Nm48F3CljaGqbtPHpW141K9CZAfTb5lxEfT/yhw8T/9RfxBw6Qcv06ABp3N+yaNMW2cSPsGjfGonx5lfgLgVP3TzH3+FwO3z1MGbsyjKs7jh4Ve2BhZoBFwhMfQNBHcPxHcPbWjsV/wVf/7eaRSvQmTn02upN66xbxBw8S/89B4g8dJP2+dsZFc3d3bHwaYtugIbYNG2BVpYqaR99IpJQcvHOQecfncSbiDB4lPBhbZyzdK3Y3TMK/tl/7sDYyGOoNhk6fg40B5+3JgUr0Jk59NvohpSTlWggJR46QcPgwCcePk3bnDgDC1habOnWwqVcXmzp1salTG3M3NyNHXLxIKdkXuo8FJxdwLuIcZe3KMrzWcHpX6o21ubV+G09NhL1fwV/zwNYVun4NNV7S9u0biUr0Jk59NoaTevs2CceOk3jiBIn//kvSxYuQng6Aedky2NSqjXXNmljXqol1jRqYOxv/Ts/UPUz4AacDOHn/JC7WLgyqPoh+VfrhZK3n4bR3TsGm1wtFoZVK9CZOfTbGk5GQQNL58ySeOk3iqZMknT1H6o0bj/ably6NdfXqWFevhlWVqlhXq4pFuXIIjcaIUZsmKSVH7x1l6Zml/HXrL6w11vSq1ItB1QdRwbGC/hpOT4OD82H3l6CxhA6fQIPhYGbYQY0q0Zs49dkULunR0SSdO0fSufMknde+Uq5de1RlKaytsapYEavKlbGqXAnLF17AqmJFLDw81C8AHbkcdZmfzv/E5uDNpGak0qxMM/pX609rz9aYm+np2UpEsLbvPmQ/lG+hfVhrwEIrlehNnPpsCr+MpCSSrwSTfPECyZevkHz5MsmXLpGWZUpuYWmJpbc3li+8gGUFbyzLl8fK2xuL8uXRODmpUT/5EJ4YzobLG1h3cR33Eu5R0rYkvSr2onfl3pSzL6f7BqWEf1dB4AeQlgS+70LzN0Cj/4fEKtHriEajoXbt2o/e+/n58c4779C4cWPmzJlDq1atAOjYsSOjR4+mb9++eHt7Y29vj5mZGaVKlWLlypWULl1ap3EVhs9GyZ/0mBiSg4NJvnKFlKvXSLl2jeRrV0kNvfWo7x/AzN4eSy8vLLzKYelZDgtPTyw8PbD09MS8TBnMLA00nryISstIY+/Nvay/vJ6/bv2FRNK4dGO6v9Cd9uXbY29pr9sGY+/C1nfg/CaDrWilEr2OlChRgri4uKe2Hzp0iFGjRnH8+HHWr1/PihUrCAzUro3p7e3N0aNHcXNzY8qUKcTFxTFv3jydxlUYPhtFt2RKCim3bpESEkLK9euk3rhJyo0bpNy8QertO5CapRxfCMzd3bEoWxaLsmUwL1MGi9JlsChTGvPSZbAoVRKNqyvCwH3GhdXd+LtsvLKRTcGbuBF7A0szS1qXa01H74608miFrYUOC6EMWGj1rERfJAcC3/3yS5LP63aaYqvq1Sg9ZUq+zm3SpAnNmzdn2rRprF69mqCgoGyPa9Wqlc6TvGKahKUlVhUqYFXh6YeIMj2dtHv3SLkZSurt26TeuqV93blD4tmzpAXtRKY+MS+Lubn2l0HJkpg/fLm7Y+7uhrm7OxpXV8zd3DB3cUFYGGAsuhGVtivN2LpjGVNnDGfCz7Dl6hYCQwIJuh6ElcaK5mWb06ZcG1p6tsTNpoBDZqv3AO+W2kKrv7/TJn4jFFoVyURvLImJidSrV+/R+/fff5/+/fsDMH36dMqVK8fEiROpVCn7BzBbtmx5rOtHUfJDaDSZd+/ZD+OTGRmkR0WReucuaXfvkHrvHmn3wki7d5e0+/dJvnaV+IMHyYiNzfZ8jaOjNvG7uqJxcUHj4oy5szMaZxc0zs5onJ3QODlh7qT9Kmxti+TzAyEEtd1rU9u9NpMbTebfsH/ZeWMnO6/vZPfN3QDUdK1J87LNaVa2GXXd6+ZvygUbJ23XTe2+sPkN7YpW9QdDR8MVWqmum+eQU9cNwO+//85rr71Go0aN2Lhx46PtD/voNRoNderUYd68eTjpeKrcwvDZKEVPRlISaeERpN0PIz0igrTwcNLCalHVggAACORJREFUI0iPjCAtIpK0iHDSI6NIj4wkPTpa+6AxG8LCAjMnR+0vCAdHNA4OaBwdMHNwRGNvj5mDvfarfebXEvZo7EtgVkL7ElZWheoXhZSSS1GX2Bu6l32h+zgTfoZ0mY61xpo67nVoUKoB9UvWp7Zb7efv209NhD0ztHf3tq7Q9Ruo0UsnhVaqj15Hckr08fHx1K9fn02bNjFy5Eg++OCDR2vDZu2j15fC8Nkopk2mpZEeE0P6gwekR0VpXw8e/PeKjiE9Olr7io0hIzqG9JiYHP9qeIy5ORo7O8xyetnaal92tpjZ2CBsbDCzscXM1ibL+8zvrW0ws7FGWFsjLCx08gskLiWOI3ePcOjuIY7fO87FqIuP1rf1dvCmhmsNqrpUpZJTJao4V6GUbanc29XDilYF7qMXQnQG5gIaYImUcsYT+62AlUBDIALoL6UMydz3PuAPpANvSCkD8/lzFFqffvop/fr1o1q1aixYsID+/fvTtm1brK31XIatKAYizM0xd3HB3MXluc6T6elkxMc/SvoZcXGkx8aRERdLelwcGbFxZMTFkREfT0Z8HOnx8drjY2NJvXeXjPgE7b6EBEhLe76gzcwQ1taYWVlpv1prfwGYWVpqfxFYWWr3WVpp/6qwstTus7RCWFpmef1/e/cXI1dZxnH8+9uhswu1pVuarWxXpcSqIOrSNFr/xBgUu6BxvTCxxqS9wHCDEY2JwXhR5cZsQvwXCAkBFIkBtVLcoNFAi/GmFIpaLBbbxS66WO26/SPdnb87jxfvu+l0naHT7kxn+87zSSZ7ztkzO+/TZ/rMzHvOnGcJg9ks67PXoCXvJt9XYXxmgvHcq7w8+Qpjh3fzl9KvKGeglIEl2R76Lu+nf8WbWb2sn1XL30jf8itZtbSP3ktX0tvdy7LV15H5wi7YfTf87tuho9WNd8L6rS35otVZC72kDHAPcCMwATwnaXReS8BbgONm9lZJm4ER4LOSrgU2A+8E+oGnJL3NzGa5CM2fox8aGmLLli3s2LGDffv2ATA4OMimTZsYGRlh27Zt7Rqqc4uCMpkwlbN8+YL/lhWLVGZmqORy4TaTw/JVy4U8lVz+9LZ8HssXqORzWKEYfp8vYPk8lWKByrFpysUiVihQKRSwueVi8cyzmmpYBrwr3v7fNHAo3s50qgtOZKDSBbNdUMl0UcmsYlaGjY6QX3oXH//1s5Btbr/cRt7RvxcYM7O/AUh6FBgGqgv9MPDNuLwduFvhs8sw8KiZFYDDksbi39vdnOFfWLOztV+fDh48eMZ69Zk14+PjrRyScx1D2SyZbPaCtIO0SgUrlULxr76VSqe3Vy+Xy1Xby6d/Vy6Ry53iVO44M7lT5AvTFArTlEsFZksFZkvh7zBbQbnXsEy56UUeGiv0a4DqvmsTwPvq7WNmZUkngSvi9mfm3XdNrQeRdCtwK+AdmJxzbaWuLtTdDd3d7R5KUzQyGVTrqML8I7j19mnkvmGj2X1mtsHMNjSzcbZzznW6Rgr9BFB9UYgB4J/19pF0CXA5cKzB+zZsMZ4h1G7+b+KcO5tGCv1zwDpJayVlCQdXR+ftMwpsjcufAXZZqECjwGZJ3ZLWAuuAZ89noD09PUxNTXlhq2JmTE1N+dk9zrnXddY5+jjn/kXgt4TTKx80sxcl3QnsNbNR4AHg4Xiw9RjhxYC4388IB27LwG3ne8bNwMAAExMTTFZd7c+FF8CBgYF2D8M5t4hdNF+Ycs45V9/rfWHKL2fnnHOJ80LvnHOJ80LvnHOJW5Rz9JImgVfO8+6rgP80cTgXg06MGToz7k6MGToz7nON+S1mVvNLSIuy0C+EpL31DkikqhNjhs6MuxNjhs6Mu5kx+9SNc84lzgu9c84lLsVCf1+7B9AGnRgzdGbcnRgzdGbcTYs5uTl655xzZ0rxHb1zzrkqXuidcy5xyRR6SUOS/ippTNId7R5Pq0h6k6SnJR2Q9KKk2+P2lZKelHQo/uxt91ibTVJG0h8lPRHX10raE2P+aby6alIkrZC0XdJLMefvTz3Xkr4Sn9v7JT0iqSfFXEt6UNJRSfurttXMrYIfxPr2gqT15/JYSRT6qr62NwHXAp+L/WpTVAa+ambXABuB22KsdwA7zWwdsDOup+Z24EDV+gjw3RjzcULv4tR8H/iNmb0DeA8h/mRzLWkN8CVgg5ldR7hi7lwf6tRy/SNgaN62erm9iXCZ93WETnz3nssDJVHoqepra2ZFYK6vbXLM7IiZ/SEuv0b4j7+GEO9DcbeHgE+3Z4StIWkA+ARwf1wXcAOhRzGkGfNy4MOEy4BjZkUzO0HiuSZcPv3S2MToMuAICebazH5PuKx7tXq5HQZ+bMEzwApJVzb6WKkU+lp9bWv2pk2JpKuA64E9wGozOwLhxQDoa9/IWuJ7wNeASly/AjhhZuW4nmLOrwYmgR/GKav7JS0l4Vyb2avAXcDfCQX+JPA86ed6Tr3cLqjGpVLoG+5NmwpJbwB+AXzZzP7b7vG0kqRPAkfN7PnqzTV2TS3nlwDrgXvN7HpgmoSmaWqJc9LDwFqgH1hKmLaYL7Vcn82Cnu+pFPqm9qZd7CQtIRT5n5jZY3Hzv+c+ysWfR9s1vhb4IPApSeOEabkbCO/wV8SP95BmzieACTPbE9e3Ewp/yrn+GHDYzCbNrAQ8BnyA9HM9p15uF1TjUin0jfS1TUKcm34AOGBm36n6VXXf3q3ALy/02FrFzL5uZgNmdhUht7vM7PPA04QexZBYzABm9i/gH5LeHjd9lNCWM9lcE6ZsNkq6LD7X52JOOtdV6uV2FNgSz77ZCJycm+JpiJklcQNuBg4CLwPfaPd4Whjnhwgf2V4A/hRvNxPmrHcCh+LPle0ea4vi/wjwRFy+mtBsfgz4OdDd7vG1IN5BYG/M9+NAb+q5Br4FvATsBx4GulPMNfAI4ThEifCO/ZZ6uSVM3dwT69ufCWclNfxYfgkE55xLXCpTN8455+rwQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4n7HzOFPfMjdA0aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "annealings = \"NO LINEAR COS EXP\".split()\n",
    "\n",
    "a = torch.arange(0, 100)\n",
    "p = torch.linspace(0.01,1,100)\n",
    "\n",
    "fns = [sched_no, sched_lin, sched_cos, sched_exp]\n",
    "for fn, t in zip(fns, annealings):\n",
    "    f = fn(2, 1e-2)\n",
    "    plt.plot(a, [f(o) for o in p], label=t)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + list(pcts))\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbde1d53eb8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 1.1), sched_cos(1.1, 0.1)])\n",
    "plt.plot(a, [sched(o) for o in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the learning rate scheduler used in training loop\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-5, 1e-3), sched_cos(1e-3, 1e-6)])\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, total_iters):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    :param total_iters: total_number of iterations\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "        \n",
    "        # learning rate annealing\n",
    "        current_iter = epoch*n_batches+i\n",
    "        \n",
    "        # learning rate anealing and visualization for later\n",
    "    \n",
    "        optimizer.param_groups[0]['lr'] = sched(current_iter/total_iters)*2    # this is the bias group\n",
    "\n",
    "        optimizer.param_groups[1]['lr'] = sched(current_iter/total_iters)      # this is the non-bias group\n",
    "\n",
    "        learning_rates.append(optimizer.param_groups[1]['lr'])\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(val_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i % print_freq == 0:\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      loss=losses))\n",
    "\n",
    "    print('\\n * LOSS - {loss.avg:.3f}\\n'.format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model with pre-trained weights\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/518]\tBatch Time 9.209 (9.209)\tData Time 2.000 (2.000)\tLoss 24.5340 (24.5340)\t\n",
      "Epoch: [0][200/518]\tBatch Time 0.608 (0.693)\tData Time 0.001 (0.012)\tLoss 14.5001 (17.7631)\t\n",
      "Epoch: [0][400/518]\tBatch Time 0.677 (0.672)\tData Time 0.009 (0.007)\tLoss 13.8959 (15.8954)\t\n",
      "[0/155]\tBatch Time 0.796 (0.796)\tLoss 13.7660 (13.7660)\t\n",
      "\n",
      " * LOSS - 13.630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SSD300. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VGGBase. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AuxiliaryConvolutions. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PredictionConvolutions. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/518]\tBatch Time 2.438 (2.438)\tData Time 1.799 (1.799)\tLoss 13.6981 (13.6981)\t\n",
      "Epoch: [1][200/518]\tBatch Time 0.720 (0.665)\tData Time 0.000 (0.014)\tLoss 13.4469 (13.5773)\t\n",
      "Epoch: [1][400/518]\tBatch Time 0.625 (0.658)\tData Time 0.000 (0.008)\tLoss 13.1389 (13.4586)\t\n",
      "[0/155]\tBatch Time 0.907 (0.907)\tLoss 11.9655 (11.9655)\t\n",
      "\n",
      " * LOSS - 12.222\n",
      "\n",
      "Epoch: [2][0/518]\tBatch Time 2.878 (2.878)\tData Time 2.244 (2.244)\tLoss 12.6683 (12.6683)\t\n",
      "Epoch: [2][200/518]\tBatch Time 0.607 (0.662)\tData Time 0.000 (0.015)\tLoss 10.1892 (11.5850)\t\n",
      "Epoch: [2][400/518]\tBatch Time 0.639 (0.658)\tData Time 0.000 (0.009)\tLoss 7.7842 (10.1653)\t\n",
      "[0/155]\tBatch Time 0.840 (0.840)\tLoss 6.9121 (6.9121)\t\n",
      "\n",
      " * LOSS - 6.843\n",
      "\n",
      "Epoch: [3][0/518]\tBatch Time 2.836 (2.836)\tData Time 2.198 (2.198)\tLoss 6.3921 (6.3921)\t\n",
      "Epoch: [3][200/518]\tBatch Time 0.628 (0.664)\tData Time 0.000 (0.014)\tLoss 6.9952 (6.7693)\t\n",
      "Epoch: [3][400/518]\tBatch Time 0.745 (0.656)\tData Time 0.006 (0.009)\tLoss 6.5625 (6.6241)\t\n",
      "[0/155]\tBatch Time 0.934 (0.934)\tLoss 6.3698 (6.3698)\t\n",
      "\n",
      " * LOSS - 6.189\n",
      "\n",
      "Epoch: [4][0/518]\tBatch Time 1.959 (1.959)\tData Time 1.338 (1.338)\tLoss 6.0218 (6.0218)\t\n",
      "Epoch: [4][200/518]\tBatch Time 0.626 (0.659)\tData Time 0.001 (0.011)\tLoss 6.1822 (6.2288)\t\n",
      "Epoch: [4][400/518]\tBatch Time 0.648 (0.654)\tData Time 0.002 (0.007)\tLoss 6.1850 (6.1833)\t\n",
      "[0/155]\tBatch Time 1.066 (1.066)\tLoss 6.0039 (6.0039)\t\n",
      "\n",
      " * LOSS - 6.011\n",
      "\n",
      "Epoch: [5][0/518]\tBatch Time 2.221 (2.221)\tData Time 1.464 (1.464)\tLoss 6.1586 (6.1586)\t\n",
      "Epoch: [5][200/518]\tBatch Time 0.622 (0.661)\tData Time 0.000 (0.014)\tLoss 6.2857 (6.0678)\t\n",
      "Epoch: [5][400/518]\tBatch Time 0.644 (0.655)\tData Time 0.009 (0.008)\tLoss 5.7940 (6.0431)\t\n",
      "[0/155]\tBatch Time 0.844 (0.844)\tLoss 6.2495 (6.2495)\t\n",
      "\n",
      " * LOSS - 5.952\n",
      "\n",
      "Epoch: [6][0/518]\tBatch Time 2.459 (2.459)\tData Time 1.830 (1.830)\tLoss 6.2411 (6.2411)\t\n",
      "Epoch: [6][200/518]\tBatch Time 0.698 (0.661)\tData Time 0.002 (0.012)\tLoss 5.8380 (5.9703)\t\n",
      "Epoch: [6][400/518]\tBatch Time 0.636 (0.654)\tData Time 0.022 (0.007)\tLoss 5.8647 (5.9511)\t\n",
      "[0/155]\tBatch Time 1.105 (1.105)\tLoss 5.7671 (5.7671)\t\n",
      "\n",
      " * LOSS - 5.871\n",
      "\n",
      "Epoch: [7][0/518]\tBatch Time 2.409 (2.409)\tData Time 1.635 (1.635)\tLoss 5.9511 (5.9511)\t\n",
      "Epoch: [7][200/518]\tBatch Time 0.631 (0.661)\tData Time 0.009 (0.014)\tLoss 6.2445 (5.8847)\t\n",
      "Epoch: [7][400/518]\tBatch Time 0.633 (0.656)\tData Time 0.000 (0.009)\tLoss 5.7876 (5.8862)\t\n",
      "[0/155]\tBatch Time 0.862 (0.862)\tLoss 5.5962 (5.5962)\t\n",
      "\n",
      " * LOSS - 5.779\n",
      "\n",
      "Epoch: [8][0/518]\tBatch Time 2.192 (2.192)\tData Time 1.459 (1.459)\tLoss 5.9609 (5.9609)\t\n",
      "Epoch: [8][200/518]\tBatch Time 0.644 (0.659)\tData Time 0.000 (0.012)\tLoss 5.5105 (5.8434)\t\n",
      "Epoch: [8][400/518]\tBatch Time 0.677 (0.655)\tData Time 0.000 (0.007)\tLoss 5.9674 (5.8259)\t\n",
      "[0/155]\tBatch Time 0.857 (0.857)\tLoss 5.7789 (5.7789)\t\n",
      "\n",
      " * LOSS - 5.729\n",
      "\n",
      "Epoch: [9][0/518]\tBatch Time 2.904 (2.904)\tData Time 2.265 (2.265)\tLoss 5.8498 (5.8498)\t\n",
      "Epoch: [9][200/518]\tBatch Time 0.611 (0.662)\tData Time 0.000 (0.014)\tLoss 5.7911 (5.7658)\t\n",
      "Epoch: [9][400/518]\tBatch Time 0.699 (0.656)\tData Time 0.001 (0.008)\tLoss 5.8770 (5.7525)\t\n",
      "[0/155]\tBatch Time 0.884 (0.884)\tLoss 5.3723 (5.3723)\t\n",
      "\n",
      " * LOSS - 5.637\n",
      "\n",
      "Epoch: [10][0/518]\tBatch Time 2.805 (2.805)\tData Time 2.074 (2.074)\tLoss 5.8159 (5.8159)\t\n",
      "Epoch: [10][200/518]\tBatch Time 0.614 (0.659)\tData Time 0.001 (0.013)\tLoss 5.4253 (5.6945)\t\n",
      "Epoch: [10][400/518]\tBatch Time 0.666 (0.656)\tData Time 0.001 (0.008)\tLoss 5.9306 (5.6733)\t\n",
      "[0/155]\tBatch Time 0.884 (0.884)\tLoss 5.2355 (5.2355)\t\n",
      "\n",
      " * LOSS - 5.527\n",
      "\n",
      "Epoch: [11][0/518]\tBatch Time 2.592 (2.592)\tData Time 1.935 (1.935)\tLoss 5.6216 (5.6216)\t\n",
      "Epoch: [11][200/518]\tBatch Time 0.690 (0.662)\tData Time 0.001 (0.012)\tLoss 5.7354 (5.5888)\t\n",
      "Epoch: [11][400/518]\tBatch Time 0.615 (0.657)\tData Time 0.000 (0.007)\tLoss 5.4878 (5.5542)\t\n",
      "[0/155]\tBatch Time 0.894 (0.894)\tLoss 5.4033 (5.4033)\t\n",
      "\n",
      " * LOSS - 5.459\n",
      "\n",
      "Epoch: [12][0/518]\tBatch Time 2.713 (2.713)\tData Time 2.093 (2.093)\tLoss 5.2085 (5.2085)\t\n",
      "Epoch: [12][200/518]\tBatch Time 0.671 (0.656)\tData Time 0.001 (0.015)\tLoss 5.9483 (5.4458)\t\n",
      "Epoch: [12][400/518]\tBatch Time 0.666 (0.655)\tData Time 0.000 (0.009)\tLoss 5.2329 (5.4364)\t\n",
      "[0/155]\tBatch Time 0.865 (0.865)\tLoss 5.3545 (5.3545)\t\n",
      "\n",
      " * LOSS - 5.411\n",
      "\n",
      "Epoch: [13][0/518]\tBatch Time 2.523 (2.523)\tData Time 1.800 (1.800)\tLoss 5.4081 (5.4081)\t\n",
      "Epoch: [13][200/518]\tBatch Time 0.700 (0.657)\tData Time 0.000 (0.012)\tLoss 5.0633 (5.3402)\t\n",
      "Epoch: [13][400/518]\tBatch Time 0.654 (0.653)\tData Time 0.017 (0.007)\tLoss 4.9022 (5.3187)\t\n",
      "[0/155]\tBatch Time 0.897 (0.897)\tLoss 5.3279 (5.3279)\t\n",
      "\n",
      " * LOSS - 5.227\n",
      "\n",
      "Epoch: [14][0/518]\tBatch Time 2.317 (2.317)\tData Time 1.680 (1.680)\tLoss 5.1454 (5.1454)\t\n",
      "Epoch: [14][200/518]\tBatch Time 0.745 (0.656)\tData Time 0.022 (0.012)\tLoss 5.0925 (5.1976)\t\n",
      "Epoch: [14][400/518]\tBatch Time 0.621 (0.650)\tData Time 0.002 (0.007)\tLoss 5.1415 (5.1958)\t\n",
      "[0/155]\tBatch Time 0.795 (0.795)\tLoss 5.1558 (5.1558)\t\n",
      "\n",
      " * LOSS - 5.066\n",
      "\n",
      "Epoch: [15][0/518]\tBatch Time 2.191 (2.191)\tData Time 1.423 (1.423)\tLoss 5.0045 (5.0045)\t\n",
      "Epoch: [15][200/518]\tBatch Time 0.615 (0.655)\tData Time 0.009 (0.011)\tLoss 5.1938 (5.0864)\t\n",
      "Epoch: [15][400/518]\tBatch Time 0.702 (0.652)\tData Time 0.009 (0.007)\tLoss 4.8086 (5.0801)\t\n",
      "[0/155]\tBatch Time 0.990 (0.990)\tLoss 4.4424 (4.4424)\t\n",
      "\n",
      " * LOSS - 4.940\n",
      "\n",
      "Epoch: [16][0/518]\tBatch Time 2.797 (2.797)\tData Time 2.060 (2.060)\tLoss 4.9222 (4.9222)\t\n",
      "Epoch: [16][200/518]\tBatch Time 0.692 (0.660)\tData Time 0.008 (0.013)\tLoss 4.8269 (5.0259)\t\n",
      "Epoch: [16][400/518]\tBatch Time 0.636 (0.654)\tData Time 0.000 (0.008)\tLoss 4.6348 (4.9841)\t\n",
      "[0/155]\tBatch Time 0.876 (0.876)\tLoss 4.7644 (4.7644)\t\n",
      "\n",
      " * LOSS - 4.842\n",
      "\n",
      "Epoch: [17][0/518]\tBatch Time 2.372 (2.372)\tData Time 1.660 (1.660)\tLoss 4.8431 (4.8431)\t\n",
      "Epoch: [17][200/518]\tBatch Time 0.698 (0.659)\tData Time 0.002 (0.013)\tLoss 4.9760 (4.8645)\t\n",
      "Epoch: [17][400/518]\tBatch Time 0.614 (0.652)\tData Time 0.001 (0.008)\tLoss 4.8038 (4.8684)\t\n",
      "[0/155]\tBatch Time 1.088 (1.088)\tLoss 4.7825 (4.7825)\t\n",
      "\n",
      " * LOSS - 4.814\n",
      "\n",
      "Epoch: [18][0/518]\tBatch Time 2.659 (2.659)\tData Time 2.031 (2.031)\tLoss 4.7667 (4.7667)\t\n",
      "Epoch: [18][200/518]\tBatch Time 0.602 (0.657)\tData Time 0.002 (0.013)\tLoss 4.1519 (4.7695)\t\n",
      "Epoch: [18][400/518]\tBatch Time 0.631 (0.650)\tData Time 0.002 (0.008)\tLoss 5.0528 (4.7481)\t\n",
      "[0/155]\tBatch Time 0.869 (0.869)\tLoss 4.8093 (4.8093)\t\n",
      "\n",
      " * LOSS - 4.648\n",
      "\n",
      "Epoch: [19][0/518]\tBatch Time 2.344 (2.344)\tData Time 1.606 (1.606)\tLoss 4.5678 (4.5678)\t\n",
      "Epoch: [19][200/518]\tBatch Time 0.699 (0.655)\tData Time 0.002 (0.011)\tLoss 4.5063 (4.6756)\t\n",
      "Epoch: [19][400/518]\tBatch Time 0.623 (0.651)\tData Time 0.000 (0.006)\tLoss 4.9076 (4.6612)\t\n",
      "[0/155]\tBatch Time 0.892 (0.892)\tLoss 4.5994 (4.5994)\t\n",
      "\n",
      " * LOSS - 4.545\n",
      "\n",
      "Epoch: [20][0/518]\tBatch Time 2.796 (2.796)\tData Time 2.079 (2.079)\tLoss 4.6073 (4.6073)\t\n",
      "Epoch: [20][200/518]\tBatch Time 0.726 (0.658)\tData Time 0.002 (0.013)\tLoss 4.4460 (4.5603)\t\n",
      "Epoch: [20][400/518]\tBatch Time 0.697 (0.653)\tData Time 0.002 (0.008)\tLoss 5.1238 (4.5536)\t\n",
      "[0/155]\tBatch Time 0.854 (0.854)\tLoss 4.0417 (4.0417)\t\n",
      "\n",
      " * LOSS - 4.424\n",
      "\n",
      "Epoch: [21][0/518]\tBatch Time 2.562 (2.562)\tData Time 1.794 (1.794)\tLoss 4.4712 (4.4712)\t\n",
      "Epoch: [21][200/518]\tBatch Time 0.794 (0.658)\tData Time 0.023 (0.012)\tLoss 4.3495 (4.5038)\t\n",
      "Epoch: [21][400/518]\tBatch Time 0.646 (0.656)\tData Time 0.017 (0.007)\tLoss 4.5194 (4.4831)\t\n",
      "[0/155]\tBatch Time 0.846 (0.846)\tLoss 4.4260 (4.4260)\t\n",
      "\n",
      " * LOSS - 4.363\n",
      "\n",
      "Epoch: [22][0/518]\tBatch Time 2.809 (2.809)\tData Time 2.178 (2.178)\tLoss 3.9925 (3.9925)\t\n",
      "Epoch: [22][200/518]\tBatch Time 0.636 (0.656)\tData Time 0.009 (0.014)\tLoss 4.2441 (4.4101)\t\n",
      "Epoch: [22][400/518]\tBatch Time 0.620 (0.652)\tData Time 0.000 (0.008)\tLoss 4.5921 (4.3865)\t\n",
      "[0/155]\tBatch Time 0.813 (0.813)\tLoss 4.2566 (4.2566)\t\n",
      "\n",
      " * LOSS - 4.284\n",
      "\n",
      "Epoch: [23][0/518]\tBatch Time 2.193 (2.193)\tData Time 1.564 (1.564)\tLoss 4.2252 (4.2252)\t\n",
      "Epoch: [23][200/518]\tBatch Time 0.618 (0.655)\tData Time 0.001 (0.010)\tLoss 4.5943 (4.3254)\t\n",
      "Epoch: [23][400/518]\tBatch Time 0.666 (0.653)\tData Time 0.000 (0.006)\tLoss 4.2825 (4.3093)\t\n",
      "[0/155]\tBatch Time 0.889 (0.889)\tLoss 4.2540 (4.2540)\t\n",
      "\n",
      " * LOSS - 4.157\n",
      "\n",
      "Epoch: [24][0/518]\tBatch Time 2.508 (2.508)\tData Time 1.876 (1.876)\tLoss 4.2766 (4.2766)\t\n",
      "Epoch: [24][200/518]\tBatch Time 0.706 (0.658)\tData Time 0.009 (0.012)\tLoss 4.0647 (4.2483)\t\n",
      "Epoch: [24][400/518]\tBatch Time 0.621 (0.653)\tData Time 0.000 (0.007)\tLoss 4.1196 (4.2373)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/155]\tBatch Time 0.857 (0.857)\tLoss 4.0714 (4.0714)\t\n",
      "\n",
      " * LOSS - 4.145\n",
      "\n",
      "Epoch: [25][0/518]\tBatch Time 2.711 (2.711)\tData Time 2.075 (2.075)\tLoss 4.4699 (4.4699)\t\n",
      "Epoch: [25][200/518]\tBatch Time 0.652 (0.660)\tData Time 0.009 (0.013)\tLoss 4.1209 (4.1520)\t\n",
      "Epoch: [25][400/518]\tBatch Time 0.634 (0.656)\tData Time 0.001 (0.008)\tLoss 4.2785 (4.1471)\t\n",
      "[0/155]\tBatch Time 0.936 (0.936)\tLoss 3.5932 (3.5932)\t\n",
      "\n",
      " * LOSS - 4.050\n",
      "\n",
      "Epoch: [26][0/518]\tBatch Time 2.713 (2.713)\tData Time 2.093 (2.093)\tLoss 4.0089 (4.0089)\t\n",
      "Epoch: [26][200/518]\tBatch Time 0.627 (0.659)\tData Time 0.000 (0.013)\tLoss 3.8164 (4.0942)\t\n",
      "Epoch: [26][400/518]\tBatch Time 0.749 (0.654)\tData Time 0.001 (0.008)\tLoss 4.1348 (4.0792)\t\n",
      "[0/155]\tBatch Time 1.276 (1.276)\tLoss 3.9009 (3.9009)\t\n",
      "\n",
      " * LOSS - 4.013\n",
      "\n",
      "Epoch: [27][0/518]\tBatch Time 2.504 (2.504)\tData Time 1.727 (1.727)\tLoss 3.8557 (3.8557)\t\n",
      "Epoch: [27][200/518]\tBatch Time 0.642 (0.659)\tData Time 0.009 (0.011)\tLoss 4.0580 (4.0104)\t\n",
      "Epoch: [27][400/518]\tBatch Time 0.630 (0.659)\tData Time 0.000 (0.007)\tLoss 3.9074 (3.9977)\t\n",
      "[0/155]\tBatch Time 0.892 (0.892)\tLoss 3.7632 (3.7632)\t\n",
      "\n",
      " * LOSS - 3.916\n",
      "\n",
      "Epoch: [28][0/518]\tBatch Time 2.653 (2.653)\tData Time 2.029 (2.029)\tLoss 4.0109 (4.0109)\t\n",
      "Epoch: [28][200/518]\tBatch Time 0.719 (0.662)\tData Time 0.000 (0.013)\tLoss 3.7041 (3.9751)\t\n",
      "Epoch: [28][400/518]\tBatch Time 0.626 (0.656)\tData Time 0.002 (0.008)\tLoss 3.7026 (3.9515)\t\n",
      "[0/155]\tBatch Time 0.900 (0.900)\tLoss 3.5991 (3.5991)\t\n",
      "\n",
      " * LOSS - 3.849\n",
      "\n",
      "Epoch: [29][0/518]\tBatch Time 2.439 (2.439)\tData Time 1.591 (1.591)\tLoss 3.9329 (3.9329)\t\n",
      "Epoch: [29][200/518]\tBatch Time 0.645 (0.664)\tData Time 0.000 (0.010)\tLoss 3.9012 (3.8682)\t\n",
      "Epoch: [29][400/518]\tBatch Time 0.635 (0.657)\tData Time 0.001 (0.007)\tLoss 4.2115 (3.8767)\t\n",
      "[0/155]\tBatch Time 0.929 (0.929)\tLoss 3.9047 (3.9047)\t\n",
      "\n",
      " * LOSS - 3.773\n",
      "\n",
      "Epoch: [30][0/518]\tBatch Time 2.312 (2.312)\tData Time 1.573 (1.573)\tLoss 3.8037 (3.8037)\t\n",
      "Epoch: [30][200/518]\tBatch Time 0.687 (0.663)\tData Time 0.002 (0.013)\tLoss 3.4763 (3.8285)\t\n",
      "Epoch: [30][400/518]\tBatch Time 0.615 (0.657)\tData Time 0.000 (0.007)\tLoss 3.6280 (3.8171)\t\n",
      "[0/155]\tBatch Time 1.064 (1.064)\tLoss 3.7782 (3.7782)\t\n",
      "\n",
      " * LOSS - 3.741\n",
      "\n",
      "Epoch: [31][0/518]\tBatch Time 2.727 (2.727)\tData Time 2.090 (2.090)\tLoss 3.6431 (3.6431)\t\n",
      "Epoch: [31][200/518]\tBatch Time 0.617 (0.658)\tData Time 0.002 (0.013)\tLoss 3.7940 (3.8071)\t\n",
      "Epoch: [31][400/518]\tBatch Time 0.661 (0.654)\tData Time 0.000 (0.008)\tLoss 3.4285 (3.7818)\t\n",
      "[0/155]\tBatch Time 0.832 (0.832)\tLoss 3.8661 (3.8661)\t\n",
      "\n",
      " * LOSS - 3.800\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [32][0/518]\tBatch Time 2.275 (2.275)\tData Time 1.620 (1.620)\tLoss 3.7292 (3.7292)\t\n",
      "Epoch: [32][200/518]\tBatch Time 0.624 (0.662)\tData Time 0.002 (0.010)\tLoss 3.4985 (3.7367)\t\n",
      "Epoch: [32][400/518]\tBatch Time 0.624 (0.660)\tData Time 0.009 (0.007)\tLoss 3.2792 (3.7226)\t\n",
      "[0/155]\tBatch Time 0.914 (0.914)\tLoss 3.3350 (3.3350)\t\n",
      "\n",
      " * LOSS - 3.680\n",
      "\n",
      "Epoch: [33][0/518]\tBatch Time 2.413 (2.413)\tData Time 1.719 (1.719)\tLoss 3.9299 (3.9299)\t\n",
      "Epoch: [33][200/518]\tBatch Time 0.612 (0.664)\tData Time 0.001 (0.011)\tLoss 3.9096 (3.7000)\t\n",
      "Epoch: [33][400/518]\tBatch Time 0.648 (0.657)\tData Time 0.020 (0.007)\tLoss 3.8777 (3.7024)\t\n",
      "[0/155]\tBatch Time 1.107 (1.107)\tLoss 3.4633 (3.4633)\t\n",
      "\n",
      " * LOSS - 3.646\n",
      "\n",
      "Epoch: [34][0/518]\tBatch Time 2.696 (2.696)\tData Time 2.066 (2.066)\tLoss 3.2919 (3.2919)\t\n",
      "Epoch: [34][200/518]\tBatch Time 0.615 (0.659)\tData Time 0.001 (0.013)\tLoss 3.6966 (3.6797)\t\n",
      "Epoch: [34][400/518]\tBatch Time 0.684 (0.658)\tData Time 0.000 (0.008)\tLoss 3.5218 (3.6515)\t\n",
      "[0/155]\tBatch Time 0.991 (0.991)\tLoss 3.5544 (3.5544)\t\n",
      "\n",
      " * LOSS - 3.617\n",
      "\n",
      "Epoch: [35][0/518]\tBatch Time 2.414 (2.414)\tData Time 1.726 (1.726)\tLoss 3.6889 (3.6889)\t\n",
      "Epoch: [35][200/518]\tBatch Time 0.632 (0.661)\tData Time 0.001 (0.011)\tLoss 3.6967 (3.6402)\t\n",
      "Epoch: [35][400/518]\tBatch Time 0.738 (0.655)\tData Time 0.000 (0.007)\tLoss 3.5332 (3.6219)\t\n",
      "[0/155]\tBatch Time 1.132 (1.132)\tLoss 3.4110 (3.4110)\t\n",
      "\n",
      " * LOSS - 3.603\n",
      "\n",
      "Epoch: [36][0/518]\tBatch Time 2.924 (2.924)\tData Time 2.216 (2.216)\tLoss 3.6586 (3.6586)\t\n",
      "Epoch: [36][200/518]\tBatch Time 0.631 (0.662)\tData Time 0.002 (0.015)\tLoss 3.3007 (3.5790)\t\n",
      "Epoch: [36][400/518]\tBatch Time 0.630 (0.657)\tData Time 0.000 (0.009)\tLoss 3.6816 (3.5685)\t\n",
      "[0/155]\tBatch Time 0.913 (0.913)\tLoss 3.6463 (3.6463)\t\n",
      "\n",
      " * LOSS - 3.568\n",
      "\n",
      "Epoch: [37][0/518]\tBatch Time 2.122 (2.122)\tData Time 1.449 (1.449)\tLoss 3.5829 (3.5829)\t\n",
      "Epoch: [37][200/518]\tBatch Time 0.603 (0.660)\tData Time 0.000 (0.010)\tLoss 3.3771 (3.5371)\t\n",
      "Epoch: [37][400/518]\tBatch Time 0.664 (0.655)\tData Time 0.009 (0.006)\tLoss 3.5645 (3.5520)\t\n",
      "[0/155]\tBatch Time 1.002 (1.002)\tLoss 3.5967 (3.5967)\t\n",
      "\n",
      " * LOSS - 3.472\n",
      "\n",
      "Epoch: [38][0/518]\tBatch Time 2.314 (2.314)\tData Time 1.629 (1.629)\tLoss 3.4435 (3.4435)\t\n",
      "Epoch: [38][200/518]\tBatch Time 0.621 (0.660)\tData Time 0.001 (0.012)\tLoss 3.4288 (3.4932)\t\n",
      "Epoch: [38][400/518]\tBatch Time 0.626 (0.657)\tData Time 0.001 (0.007)\tLoss 3.3256 (3.5013)\t\n",
      "[0/155]\tBatch Time 0.955 (0.955)\tLoss 3.5261 (3.5261)\t\n",
      "\n",
      " * LOSS - 3.507\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [39][0/518]\tBatch Time 2.231 (2.231)\tData Time 1.611 (1.611)\tLoss 3.7481 (3.7481)\t\n",
      "Epoch: [39][200/518]\tBatch Time 0.612 (0.659)\tData Time 0.000 (0.013)\tLoss 3.5111 (3.4791)\t\n",
      "Epoch: [39][400/518]\tBatch Time 0.639 (0.656)\tData Time 0.000 (0.008)\tLoss 2.9710 (3.4739)\t\n",
      "[0/155]\tBatch Time 0.933 (0.933)\tLoss 3.6127 (3.6127)\t\n",
      "\n",
      " * LOSS - 3.481\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [40][0/518]\tBatch Time 2.221 (2.221)\tData Time 1.594 (1.594)\tLoss 3.5593 (3.5593)\t\n",
      "Epoch: [40][200/518]\tBatch Time 0.727 (0.662)\tData Time 0.001 (0.012)\tLoss 3.2785 (3.4599)\t\n",
      "Epoch: [40][400/518]\tBatch Time 0.657 (0.657)\tData Time 0.001 (0.007)\tLoss 3.9006 (3.4487)\t\n",
      "[0/155]\tBatch Time 0.929 (0.929)\tLoss 3.3595 (3.3595)\t\n",
      "\n",
      " * LOSS - 3.428\n",
      "\n",
      "Epoch: [41][0/518]\tBatch Time 2.544 (2.544)\tData Time 1.851 (1.851)\tLoss 3.3659 (3.3659)\t\n",
      "Epoch: [41][200/518]\tBatch Time 0.606 (0.663)\tData Time 0.000 (0.012)\tLoss 3.5880 (3.4307)\t\n",
      "Epoch: [41][400/518]\tBatch Time 0.619 (0.657)\tData Time 0.001 (0.008)\tLoss 3.6886 (3.4370)\t\n",
      "[0/155]\tBatch Time 0.830 (0.830)\tLoss 3.3089 (3.3089)\t\n",
      "\n",
      " * LOSS - 3.390\n",
      "\n",
      "Epoch: [42][0/518]\tBatch Time 2.442 (2.442)\tData Time 1.660 (1.660)\tLoss 3.7173 (3.7173)\t\n",
      "Epoch: [42][200/518]\tBatch Time 0.625 (0.664)\tData Time 0.002 (0.013)\tLoss 3.6273 (3.3655)\t\n",
      "Epoch: [42][400/518]\tBatch Time 0.708 (0.658)\tData Time 0.001 (0.008)\tLoss 3.3467 (3.3818)\t\n",
      "[0/155]\tBatch Time 0.923 (0.923)\tLoss 3.4486 (3.4486)\t\n",
      "\n",
      " * LOSS - 3.389\n",
      "\n",
      "Epoch: [43][0/518]\tBatch Time 2.124 (2.124)\tData Time 1.484 (1.484)\tLoss 3.0279 (3.0279)\t\n",
      "Epoch: [43][200/518]\tBatch Time 0.703 (0.660)\tData Time 0.001 (0.011)\tLoss 3.5822 (3.3739)\t\n",
      "Epoch: [43][400/518]\tBatch Time 0.615 (0.657)\tData Time 0.002 (0.007)\tLoss 3.3437 (3.3750)\t\n",
      "[0/155]\tBatch Time 1.147 (1.147)\tLoss 3.4708 (3.4708)\t\n",
      "\n",
      " * LOSS - 3.396\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [44][0/518]\tBatch Time 2.721 (2.721)\tData Time 2.112 (2.112)\tLoss 3.3061 (3.3061)\t\n",
      "Epoch: [44][200/518]\tBatch Time 0.629 (0.667)\tData Time 0.000 (0.016)\tLoss 3.3366 (3.3438)\t\n",
      "Epoch: [44][400/518]\tBatch Time 0.716 (0.658)\tData Time 0.001 (0.009)\tLoss 3.5016 (3.3350)\t\n",
      "[0/155]\tBatch Time 0.878 (0.878)\tLoss 3.2243 (3.2243)\t\n",
      "\n",
      " * LOSS - 3.345\n",
      "\n",
      "Epoch: [45][0/518]\tBatch Time 2.994 (2.994)\tData Time 2.364 (2.364)\tLoss 3.1025 (3.1025)\t\n",
      "Epoch: [45][200/518]\tBatch Time 0.633 (0.666)\tData Time 0.000 (0.015)\tLoss 3.3464 (3.3238)\t\n",
      "Epoch: [45][400/518]\tBatch Time 0.679 (0.667)\tData Time 0.001 (0.009)\tLoss 3.0852 (3.3204)\t\n",
      "[0/155]\tBatch Time 0.888 (0.888)\tLoss 3.2298 (3.2298)\t\n",
      "\n",
      " * LOSS - 3.356\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [46][0/518]\tBatch Time 2.763 (2.763)\tData Time 2.123 (2.123)\tLoss 3.2131 (3.2131)\t\n",
      "Epoch: [46][200/518]\tBatch Time 0.702 (0.675)\tData Time 0.002 (0.014)\tLoss 3.5267 (3.3008)\t\n",
      "Epoch: [46][400/518]\tBatch Time 0.706 (0.662)\tData Time 0.000 (0.008)\tLoss 3.0506 (3.3008)\t\n",
      "[0/155]\tBatch Time 0.982 (0.982)\tLoss 3.1543 (3.1543)\t\n",
      "\n",
      " * LOSS - 3.288\n",
      "\n",
      "Epoch: [47][0/518]\tBatch Time 2.469 (2.469)\tData Time 1.765 (1.765)\tLoss 3.2673 (3.2673)\t\n",
      "Epoch: [47][200/518]\tBatch Time 0.709 (0.661)\tData Time 0.001 (0.012)\tLoss 3.3103 (3.2519)\t\n",
      "Epoch: [47][400/518]\tBatch Time 0.770 (0.655)\tData Time 0.000 (0.007)\tLoss 3.5159 (3.2671)\t\n",
      "[0/155]\tBatch Time 0.925 (0.925)\tLoss 3.3349 (3.3349)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 3.306\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [48][0/518]\tBatch Time 2.597 (2.597)\tData Time 1.857 (1.857)\tLoss 3.2661 (3.2661)\t\n",
      "Epoch: [48][200/518]\tBatch Time 0.685 (0.661)\tData Time 0.000 (0.013)\tLoss 3.1785 (3.2307)\t\n",
      "Epoch: [48][400/518]\tBatch Time 0.629 (0.654)\tData Time 0.002 (0.008)\tLoss 3.1959 (3.2371)\t\n",
      "[0/155]\tBatch Time 0.927 (0.927)\tLoss 3.1894 (3.1894)\t\n",
      "\n",
      " * LOSS - 3.330\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [49][0/518]\tBatch Time 2.820 (2.820)\tData Time 2.196 (2.196)\tLoss 3.7806 (3.7806)\t\n",
      "Epoch: [49][200/518]\tBatch Time 0.699 (0.658)\tData Time 0.002 (0.014)\tLoss 3.2476 (3.2265)\t\n",
      "Epoch: [49][400/518]\tBatch Time 0.629 (0.654)\tData Time 0.000 (0.008)\tLoss 3.1278 (3.2264)\t\n",
      "[0/155]\tBatch Time 1.116 (1.116)\tLoss 3.2511 (3.2511)\t\n",
      "\n",
      " * LOSS - 3.241\n",
      "\n",
      "Epoch: [50][0/518]\tBatch Time 2.316 (2.316)\tData Time 1.602 (1.602)\tLoss 3.8483 (3.8483)\t\n",
      "Epoch: [50][200/518]\tBatch Time 0.733 (0.656)\tData Time 0.001 (0.011)\tLoss 3.1651 (3.2168)\t\n",
      "Epoch: [50][400/518]\tBatch Time 0.628 (0.654)\tData Time 0.002 (0.007)\tLoss 2.9433 (3.2019)\t\n",
      "[0/155]\tBatch Time 0.854 (0.854)\tLoss 2.9916 (2.9916)\t\n",
      "\n",
      " * LOSS - 3.246\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [51][0/518]\tBatch Time 2.399 (2.399)\tData Time 1.768 (1.768)\tLoss 2.9535 (2.9535)\t\n",
      "Epoch: [51][200/518]\tBatch Time 0.661 (0.658)\tData Time 0.000 (0.011)\tLoss 2.9890 (3.1815)\t\n",
      "Epoch: [51][400/518]\tBatch Time 0.621 (0.652)\tData Time 0.000 (0.007)\tLoss 2.8760 (3.1974)\t\n",
      "[0/155]\tBatch Time 0.896 (0.896)\tLoss 3.1615 (3.1615)\t\n",
      "\n",
      " * LOSS - 3.224\n",
      "\n",
      "Epoch: [52][0/518]\tBatch Time 2.290 (2.290)\tData Time 1.608 (1.608)\tLoss 2.9256 (2.9256)\t\n",
      "Epoch: [52][200/518]\tBatch Time 0.629 (0.660)\tData Time 0.001 (0.011)\tLoss 2.8717 (3.1767)\t\n",
      "Epoch: [52][400/518]\tBatch Time 0.621 (0.656)\tData Time 0.000 (0.007)\tLoss 3.4887 (3.1753)\t\n",
      "[0/155]\tBatch Time 1.003 (1.003)\tLoss 3.2291 (3.2291)\t\n",
      "\n",
      " * LOSS - 3.194\n",
      "\n",
      "Epoch: [53][0/518]\tBatch Time 2.588 (2.588)\tData Time 1.935 (1.935)\tLoss 3.3864 (3.3864)\t\n",
      "Epoch: [53][200/518]\tBatch Time 0.627 (0.663)\tData Time 0.001 (0.014)\tLoss 3.1696 (3.1828)\t\n",
      "Epoch: [53][400/518]\tBatch Time 0.620 (0.656)\tData Time 0.001 (0.008)\tLoss 3.0279 (3.1714)\t\n",
      "[0/155]\tBatch Time 1.115 (1.115)\tLoss 3.2227 (3.2227)\t\n",
      "\n",
      " * LOSS - 3.249\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [54][0/518]\tBatch Time 2.380 (2.380)\tData Time 1.720 (1.720)\tLoss 3.1316 (3.1316)\t\n",
      "Epoch: [54][200/518]\tBatch Time 0.644 (0.658)\tData Time 0.000 (0.011)\tLoss 2.7929 (3.1463)\t\n",
      "Epoch: [54][400/518]\tBatch Time 0.671 (0.654)\tData Time 0.002 (0.007)\tLoss 2.9549 (3.1397)\t\n",
      "[0/155]\tBatch Time 1.058 (1.058)\tLoss 3.1494 (3.1494)\t\n",
      "\n",
      " * LOSS - 3.195\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [55][0/518]\tBatch Time 2.165 (2.165)\tData Time 1.529 (1.529)\tLoss 3.3029 (3.3029)\t\n",
      "Epoch: [55][200/518]\tBatch Time 0.654 (0.663)\tData Time 0.002 (0.012)\tLoss 3.1201 (3.1150)\t\n",
      "Epoch: [55][400/518]\tBatch Time 0.644 (0.656)\tData Time 0.000 (0.007)\tLoss 2.8621 (3.1130)\t\n",
      "[0/155]\tBatch Time 1.056 (1.056)\tLoss 2.7669 (2.7669)\t\n",
      "\n",
      " * LOSS - 3.152\n",
      "\n",
      "Epoch: [56][0/518]\tBatch Time 2.297 (2.297)\tData Time 1.660 (1.660)\tLoss 3.7031 (3.7031)\t\n",
      "Epoch: [56][200/518]\tBatch Time 0.631 (0.655)\tData Time 0.023 (0.011)\tLoss 3.2628 (3.1033)\t\n",
      "Epoch: [56][400/518]\tBatch Time 0.693 (0.652)\tData Time 0.000 (0.007)\tLoss 3.1998 (3.0982)\t\n",
      "[0/155]\tBatch Time 1.081 (1.081)\tLoss 3.3340 (3.3340)\t\n",
      "\n",
      " * LOSS - 3.198\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [57][0/518]\tBatch Time 2.800 (2.800)\tData Time 2.175 (2.175)\tLoss 2.8140 (2.8140)\t\n",
      "Epoch: [57][200/518]\tBatch Time 0.625 (0.663)\tData Time 0.000 (0.013)\tLoss 2.8516 (3.1050)\t\n",
      "Epoch: [57][400/518]\tBatch Time 0.618 (0.656)\tData Time 0.000 (0.008)\tLoss 3.1349 (3.0825)\t\n",
      "[0/155]\tBatch Time 1.169 (1.169)\tLoss 2.9463 (2.9463)\t\n",
      "\n",
      " * LOSS - 3.166\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [58][0/518]\tBatch Time 2.686 (2.686)\tData Time 1.962 (1.962)\tLoss 3.4497 (3.4497)\t\n",
      "Epoch: [58][200/518]\tBatch Time 0.756 (0.658)\tData Time 0.002 (0.012)\tLoss 3.0415 (3.0614)\t\n",
      "Epoch: [58][400/518]\tBatch Time 0.709 (0.654)\tData Time 0.001 (0.008)\tLoss 2.8643 (3.0601)\t\n",
      "[0/155]\tBatch Time 1.045 (1.045)\tLoss 2.9983 (2.9983)\t\n",
      "\n",
      " * LOSS - 3.161\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [59][0/518]\tBatch Time 2.032 (2.032)\tData Time 1.382 (1.382)\tLoss 2.9521 (2.9521)\t\n",
      "Epoch: [59][200/518]\tBatch Time 0.600 (0.659)\tData Time 0.001 (0.011)\tLoss 3.2018 (3.0923)\t\n",
      "Epoch: [59][400/518]\tBatch Time 0.654 (0.656)\tData Time 0.001 (0.007)\tLoss 3.1929 (3.0837)\t\n",
      "[0/155]\tBatch Time 1.026 (1.026)\tLoss 3.1684 (3.1684)\t\n",
      "\n",
      " * LOSS - 3.133\n",
      "\n",
      "Epoch: [60][0/518]\tBatch Time 2.307 (2.307)\tData Time 1.561 (1.561)\tLoss 3.1407 (3.1407)\t\n",
      "Epoch: [60][200/518]\tBatch Time 0.649 (0.659)\tData Time 0.009 (0.011)\tLoss 3.0348 (3.0487)\t\n",
      "Epoch: [60][400/518]\tBatch Time 0.626 (0.654)\tData Time 0.000 (0.007)\tLoss 3.0404 (3.0372)\t\n",
      "[0/155]\tBatch Time 0.956 (0.956)\tLoss 2.7745 (2.7745)\t\n",
      "\n",
      " * LOSS - 3.125\n",
      "\n",
      "Epoch: [61][0/518]\tBatch Time 2.656 (2.656)\tData Time 2.039 (2.039)\tLoss 2.8151 (2.8151)\t\n",
      "Epoch: [61][200/518]\tBatch Time 0.629 (0.658)\tData Time 0.001 (0.015)\tLoss 3.0559 (3.0133)\t\n",
      "Epoch: [61][400/518]\tBatch Time 0.619 (0.655)\tData Time 0.000 (0.008)\tLoss 2.6929 (3.0160)\t\n",
      "[0/155]\tBatch Time 1.163 (1.163)\tLoss 3.1633 (3.1633)\t\n",
      "\n",
      " * LOSS - 3.127\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [62][0/518]\tBatch Time 2.529 (2.529)\tData Time 1.828 (1.828)\tLoss 2.6829 (2.6829)\t\n",
      "Epoch: [62][200/518]\tBatch Time 0.641 (0.659)\tData Time 0.009 (0.013)\tLoss 2.8438 (3.0079)\t\n",
      "Epoch: [62][400/518]\tBatch Time 0.638 (0.655)\tData Time 0.000 (0.008)\tLoss 3.0537 (2.9878)\t\n",
      "[0/155]\tBatch Time 1.050 (1.050)\tLoss 2.8649 (2.8649)\t\n",
      "\n",
      " * LOSS - 3.107\n",
      "\n",
      "Epoch: [63][0/518]\tBatch Time 2.373 (2.373)\tData Time 1.731 (1.731)\tLoss 2.9306 (2.9306)\t\n",
      "Epoch: [63][200/518]\tBatch Time 0.646 (0.657)\tData Time 0.000 (0.012)\tLoss 2.8450 (2.9819)\t\n",
      "Epoch: [63][400/518]\tBatch Time 0.621 (0.654)\tData Time 0.000 (0.007)\tLoss 3.0350 (2.9956)\t\n",
      "[0/155]\tBatch Time 0.854 (0.854)\tLoss 3.3129 (3.3129)\t\n",
      "\n",
      " * LOSS - 3.085\n",
      "\n",
      "Epoch: [64][0/518]\tBatch Time 2.879 (2.879)\tData Time 2.251 (2.251)\tLoss 2.7206 (2.7206)\t\n",
      "Epoch: [64][200/518]\tBatch Time 0.659 (0.665)\tData Time 0.000 (0.015)\tLoss 3.2436 (2.9857)\t\n",
      "Epoch: [64][400/518]\tBatch Time 0.625 (0.657)\tData Time 0.002 (0.009)\tLoss 2.9669 (2.9867)\t\n",
      "[0/155]\tBatch Time 0.898 (0.898)\tLoss 3.2105 (3.2105)\t\n",
      "\n",
      " * LOSS - 3.124\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [65][0/518]\tBatch Time 2.539 (2.539)\tData Time 1.908 (1.908)\tLoss 3.5276 (3.5276)\t\n",
      "Epoch: [65][200/518]\tBatch Time 0.619 (0.658)\tData Time 0.000 (0.012)\tLoss 3.0117 (2.9797)\t\n",
      "Epoch: [65][400/518]\tBatch Time 0.609 (0.654)\tData Time 0.000 (0.007)\tLoss 3.1553 (2.9704)\t\n",
      "[0/155]\tBatch Time 0.946 (0.946)\tLoss 3.2170 (3.2170)\t\n",
      "\n",
      " * LOSS - 3.059\n",
      "\n",
      "Epoch: [66][0/518]\tBatch Time 2.845 (2.845)\tData Time 2.218 (2.218)\tLoss 3.0882 (3.0882)\t\n",
      "Epoch: [66][200/518]\tBatch Time 0.615 (0.662)\tData Time 0.000 (0.015)\tLoss 3.0425 (2.9556)\t\n",
      "Epoch: [66][400/518]\tBatch Time 0.719 (0.654)\tData Time 0.000 (0.009)\tLoss 3.4924 (2.9584)\t\n",
      "[0/155]\tBatch Time 0.988 (0.988)\tLoss 3.0949 (3.0949)\t\n",
      "\n",
      " * LOSS - 3.102\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [67][0/518]\tBatch Time 2.744 (2.744)\tData Time 2.092 (2.092)\tLoss 2.5411 (2.5411)\t\n",
      "Epoch: [67][200/518]\tBatch Time 0.644 (0.658)\tData Time 0.009 (0.013)\tLoss 2.9607 (2.9420)\t\n",
      "Epoch: [67][400/518]\tBatch Time 0.617 (0.656)\tData Time 0.009 (0.008)\tLoss 3.4280 (2.9310)\t\n",
      "[0/155]\tBatch Time 1.008 (1.008)\tLoss 2.9813 (2.9813)\t\n",
      "\n",
      " * LOSS - 3.057\n",
      "\n",
      "Epoch: [68][0/518]\tBatch Time 2.254 (2.254)\tData Time 1.528 (1.528)\tLoss 2.7042 (2.7042)\t\n",
      "Epoch: [68][200/518]\tBatch Time 0.626 (0.659)\tData Time 0.001 (0.012)\tLoss 2.6726 (2.9222)\t\n",
      "Epoch: [68][400/518]\tBatch Time 0.614 (0.653)\tData Time 0.000 (0.007)\tLoss 2.7358 (2.9287)\t\n",
      "[0/155]\tBatch Time 0.859 (0.859)\tLoss 3.1981 (3.1981)\t\n",
      "\n",
      " * LOSS - 3.059\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [69][0/518]\tBatch Time 2.830 (2.830)\tData Time 2.152 (2.152)\tLoss 2.3731 (2.3731)\t\n",
      "Epoch: [69][200/518]\tBatch Time 0.623 (0.660)\tData Time 0.002 (0.013)\tLoss 2.8249 (2.9254)\t\n",
      "Epoch: [69][400/518]\tBatch Time 0.696 (0.655)\tData Time 0.002 (0.008)\tLoss 3.4081 (2.9214)\t\n",
      "[0/155]\tBatch Time 0.879 (0.879)\tLoss 3.0726 (3.0726)\t\n",
      "\n",
      " * LOSS - 3.027\n",
      "\n",
      "Epoch: [70][0/518]\tBatch Time 2.663 (2.663)\tData Time 2.024 (2.024)\tLoss 2.9937 (2.9937)\t\n",
      "Epoch: [70][200/518]\tBatch Time 0.649 (0.664)\tData Time 0.002 (0.013)\tLoss 2.5946 (2.9027)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70][400/518]\tBatch Time 0.625 (0.656)\tData Time 0.002 (0.008)\tLoss 2.6674 (2.9161)\t\n",
      "[0/155]\tBatch Time 1.071 (1.071)\tLoss 2.9534 (2.9534)\t\n",
      "\n",
      " * LOSS - 3.032\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [71][0/518]\tBatch Time 2.235 (2.235)\tData Time 1.539 (1.539)\tLoss 2.6415 (2.6415)\t\n",
      "Epoch: [71][200/518]\tBatch Time 0.660 (0.663)\tData Time 0.000 (0.013)\tLoss 2.9368 (2.9045)\t\n",
      "Epoch: [71][400/518]\tBatch Time 0.625 (0.656)\tData Time 0.000 (0.008)\tLoss 3.1630 (2.8976)\t\n",
      "[0/155]\tBatch Time 1.073 (1.073)\tLoss 3.0414 (3.0414)\t\n",
      "\n",
      " * LOSS - 3.024\n",
      "\n",
      "Epoch: [72][0/518]\tBatch Time 2.016 (2.016)\tData Time 1.322 (1.322)\tLoss 2.9767 (2.9767)\t\n",
      "Epoch: [72][200/518]\tBatch Time 0.624 (0.654)\tData Time 0.000 (0.011)\tLoss 2.7096 (2.8835)\t\n",
      "Epoch: [72][400/518]\tBatch Time 0.617 (0.652)\tData Time 0.001 (0.006)\tLoss 2.9974 (2.8854)\t\n",
      "[0/155]\tBatch Time 1.086 (1.086)\tLoss 3.1351 (3.1351)\t\n",
      "\n",
      " * LOSS - 3.004\n",
      "\n",
      "Epoch: [73][0/518]\tBatch Time 2.487 (2.487)\tData Time 1.717 (1.717)\tLoss 2.8531 (2.8531)\t\n",
      "Epoch: [73][200/518]\tBatch Time 0.654 (0.660)\tData Time 0.009 (0.011)\tLoss 2.6316 (2.8773)\t\n",
      "Epoch: [73][400/518]\tBatch Time 0.625 (0.655)\tData Time 0.000 (0.007)\tLoss 2.9216 (2.8755)\t\n",
      "[0/155]\tBatch Time 1.096 (1.096)\tLoss 3.0534 (3.0534)\t\n",
      "\n",
      " * LOSS - 2.981\n",
      "\n",
      "Epoch: [74][0/518]\tBatch Time 2.789 (2.789)\tData Time 2.151 (2.151)\tLoss 2.9275 (2.9275)\t\n",
      "Epoch: [74][200/518]\tBatch Time 0.617 (0.661)\tData Time 0.001 (0.013)\tLoss 2.8430 (2.8550)\t\n",
      "Epoch: [74][400/518]\tBatch Time 0.639 (0.654)\tData Time 0.000 (0.008)\tLoss 2.6364 (2.8491)\t\n",
      "[0/155]\tBatch Time 1.123 (1.123)\tLoss 3.1493 (3.1493)\t\n",
      "\n",
      " * LOSS - 3.012\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [75][0/518]\tBatch Time 2.559 (2.559)\tData Time 1.918 (1.918)\tLoss 2.8125 (2.8125)\t\n",
      "Epoch: [75][200/518]\tBatch Time 0.643 (0.659)\tData Time 0.009 (0.012)\tLoss 2.8633 (2.8424)\t\n",
      "Epoch: [75][400/518]\tBatch Time 0.624 (0.656)\tData Time 0.000 (0.007)\tLoss 2.8915 (2.8456)\t\n",
      "[0/155]\tBatch Time 1.115 (1.115)\tLoss 3.0363 (3.0363)\t\n",
      "\n",
      " * LOSS - 3.015\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [76][0/518]\tBatch Time 2.317 (2.317)\tData Time 1.680 (1.680)\tLoss 2.9881 (2.9881)\t\n",
      "Epoch: [76][200/518]\tBatch Time 0.665 (0.659)\tData Time 0.000 (0.011)\tLoss 2.9278 (2.8553)\t\n",
      "Epoch: [76][400/518]\tBatch Time 0.624 (0.652)\tData Time 0.000 (0.007)\tLoss 2.8896 (2.8376)\t\n",
      "[0/155]\tBatch Time 1.088 (1.088)\tLoss 2.8354 (2.8354)\t\n",
      "\n",
      " * LOSS - 2.968\n",
      "\n",
      "Epoch: [77][0/518]\tBatch Time 2.860 (2.860)\tData Time 2.231 (2.231)\tLoss 2.7910 (2.7910)\t\n",
      "Epoch: [77][200/518]\tBatch Time 0.606 (0.661)\tData Time 0.002 (0.014)\tLoss 2.7424 (2.8399)\t\n",
      "Epoch: [77][400/518]\tBatch Time 0.690 (0.657)\tData Time 0.002 (0.008)\tLoss 2.8042 (2.8279)\t\n",
      "[0/155]\tBatch Time 0.744 (0.744)\tLoss 3.0377 (3.0377)\t\n",
      "\n",
      " * LOSS - 2.968\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [78][0/518]\tBatch Time 2.415 (2.415)\tData Time 1.790 (1.790)\tLoss 2.6953 (2.6953)\t\n",
      "Epoch: [78][200/518]\tBatch Time 0.682 (0.658)\tData Time 0.002 (0.011)\tLoss 3.0487 (2.8074)\t\n",
      "Epoch: [78][400/518]\tBatch Time 0.746 (0.653)\tData Time 0.001 (0.007)\tLoss 2.9279 (2.8164)\t\n",
      "[0/155]\tBatch Time 0.885 (0.885)\tLoss 2.9524 (2.9524)\t\n",
      "\n",
      " * LOSS - 2.970\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [79][0/518]\tBatch Time 2.439 (2.439)\tData Time 1.794 (1.794)\tLoss 2.3717 (2.3717)\t\n",
      "Epoch: [79][200/518]\tBatch Time 0.621 (0.659)\tData Time 0.000 (0.013)\tLoss 2.7497 (2.7914)\t\n",
      "Epoch: [79][400/518]\tBatch Time 0.622 (0.654)\tData Time 0.002 (0.008)\tLoss 2.9783 (2.8081)\t\n",
      "[0/155]\tBatch Time 0.958 (0.958)\tLoss 3.0654 (3.0654)\t\n",
      "\n",
      " * LOSS - 2.979\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [80][0/518]\tBatch Time 2.487 (2.487)\tData Time 1.849 (1.849)\tLoss 2.7873 (2.7873)\t\n",
      "Epoch: [80][200/518]\tBatch Time 0.686 (0.659)\tData Time 0.000 (0.012)\tLoss 2.7680 (2.8007)\t\n",
      "Epoch: [80][400/518]\tBatch Time 0.636 (0.655)\tData Time 0.002 (0.007)\tLoss 2.5367 (2.8029)\t\n",
      "[0/155]\tBatch Time 1.152 (1.152)\tLoss 3.3315 (3.3315)\t\n",
      "\n",
      " * LOSS - 2.960\n",
      "\n",
      "Epoch: [81][0/518]\tBatch Time 3.305 (3.305)\tData Time 2.667 (2.667)\tLoss 2.9612 (2.9612)\t\n",
      "Epoch: [81][200/518]\tBatch Time 0.707 (0.664)\tData Time 0.009 (0.016)\tLoss 2.8048 (2.8054)\t\n",
      "Epoch: [81][400/518]\tBatch Time 0.625 (0.657)\tData Time 0.000 (0.009)\tLoss 2.8527 (2.8000)\t\n",
      "[0/155]\tBatch Time 0.894 (0.894)\tLoss 2.9703 (2.9703)\t\n",
      "\n",
      " * LOSS - 2.982\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [82][0/518]\tBatch Time 2.519 (2.519)\tData Time 1.752 (1.752)\tLoss 2.9121 (2.9121)\t\n",
      "Epoch: [82][200/518]\tBatch Time 0.687 (0.660)\tData Time 0.002 (0.012)\tLoss 3.1578 (2.7870)\t\n",
      "Epoch: [82][400/518]\tBatch Time 0.685 (0.654)\tData Time 0.001 (0.007)\tLoss 2.7223 (2.7932)\t\n",
      "[0/155]\tBatch Time 0.902 (0.902)\tLoss 2.9351 (2.9351)\t\n",
      "\n",
      " * LOSS - 2.934\n",
      "\n",
      "Epoch: [83][0/518]\tBatch Time 2.364 (2.364)\tData Time 1.720 (1.720)\tLoss 2.5911 (2.5911)\t\n",
      "Epoch: [83][200/518]\tBatch Time 0.643 (0.659)\tData Time 0.001 (0.013)\tLoss 2.6452 (2.7621)\t\n",
      "Epoch: [83][400/518]\tBatch Time 0.639 (0.654)\tData Time 0.001 (0.008)\tLoss 3.0068 (2.7669)\t\n",
      "[0/155]\tBatch Time 0.874 (0.874)\tLoss 2.9135 (2.9135)\t\n",
      "\n",
      " * LOSS - 2.947\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [84][0/518]\tBatch Time 2.285 (2.285)\tData Time 1.578 (1.578)\tLoss 2.5882 (2.5882)\t\n",
      "Epoch: [84][200/518]\tBatch Time 0.628 (0.656)\tData Time 0.001 (0.010)\tLoss 2.7609 (2.7871)\t\n",
      "Epoch: [84][400/518]\tBatch Time 0.639 (0.654)\tData Time 0.002 (0.006)\tLoss 2.8499 (2.7911)\t\n",
      "[0/155]\tBatch Time 0.939 (0.939)\tLoss 3.2944 (3.2944)\t\n",
      "\n",
      " * LOSS - 2.933\n",
      "\n",
      "Epoch: [85][0/518]\tBatch Time 2.571 (2.571)\tData Time 1.874 (1.874)\tLoss 2.7060 (2.7060)\t\n",
      "Epoch: [85][200/518]\tBatch Time 0.624 (0.657)\tData Time 0.000 (0.012)\tLoss 2.6627 (2.7875)\t\n",
      "Epoch: [85][400/518]\tBatch Time 0.638 (0.653)\tData Time 0.009 (0.007)\tLoss 2.9083 (2.7707)\t\n",
      "[0/155]\tBatch Time 1.110 (1.110)\tLoss 2.6161 (2.6161)\t\n",
      "\n",
      " * LOSS - 2.922\n",
      "\n",
      "Epoch: [86][0/518]\tBatch Time 2.051 (2.051)\tData Time 1.408 (1.408)\tLoss 2.7657 (2.7657)\t\n",
      "Epoch: [86][200/518]\tBatch Time 0.620 (0.664)\tData Time 0.000 (0.014)\tLoss 2.5403 (2.7358)\t\n",
      "Epoch: [86][400/518]\tBatch Time 0.698 (0.657)\tData Time 0.009 (0.009)\tLoss 2.5500 (2.7546)\t\n",
      "[0/155]\tBatch Time 0.953 (0.953)\tLoss 3.5597 (3.5597)\t\n",
      "\n",
      " * LOSS - 2.949\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [87][0/518]\tBatch Time 2.758 (2.758)\tData Time 2.136 (2.136)\tLoss 2.8795 (2.8795)\t\n",
      "Epoch: [87][200/518]\tBatch Time 0.634 (0.662)\tData Time 0.002 (0.013)\tLoss 2.5805 (2.7454)\t\n",
      "Epoch: [87][400/518]\tBatch Time 0.646 (0.656)\tData Time 0.009 (0.008)\tLoss 2.4411 (2.7459)\t\n",
      "[0/155]\tBatch Time 1.082 (1.082)\tLoss 2.7035 (2.7035)\t\n",
      "\n",
      " * LOSS - 2.955\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [88][0/518]\tBatch Time 2.762 (2.762)\tData Time 2.045 (2.045)\tLoss 2.7902 (2.7902)\t\n",
      "Epoch: [88][200/518]\tBatch Time 0.643 (0.662)\tData Time 0.009 (0.013)\tLoss 2.7036 (2.7547)\t\n",
      "Epoch: [88][400/518]\tBatch Time 0.715 (0.656)\tData Time 0.002 (0.007)\tLoss 2.8682 (2.7510)\t\n",
      "[0/155]\tBatch Time 1.053 (1.053)\tLoss 3.1304 (3.1304)\t\n",
      "\n",
      " * LOSS - 2.913\n",
      "\n",
      "Epoch: [89][0/518]\tBatch Time 2.896 (2.896)\tData Time 2.263 (2.263)\tLoss 2.6964 (2.6964)\t\n",
      "Epoch: [89][200/518]\tBatch Time 0.634 (0.658)\tData Time 0.000 (0.013)\tLoss 3.0220 (2.7103)\t\n",
      "Epoch: [89][400/518]\tBatch Time 0.624 (0.655)\tData Time 0.002 (0.008)\tLoss 2.7954 (2.7150)\t\n",
      "[0/155]\tBatch Time 1.137 (1.137)\tLoss 3.1331 (3.1331)\t\n",
      "\n",
      " * LOSS - 2.925\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [90][0/518]\tBatch Time 2.343 (2.343)\tData Time 1.621 (1.621)\tLoss 2.5073 (2.5073)\t\n",
      "Epoch: [90][200/518]\tBatch Time 0.655 (0.660)\tData Time 0.001 (0.011)\tLoss 2.9147 (2.7358)\t\n",
      "Epoch: [90][400/518]\tBatch Time 0.723 (0.655)\tData Time 0.002 (0.007)\tLoss 2.2862 (2.7329)\t\n",
      "[0/155]\tBatch Time 1.039 (1.039)\tLoss 3.2295 (3.2295)\t\n",
      "\n",
      " * LOSS - 2.956\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [91][0/518]\tBatch Time 2.397 (2.397)\tData Time 1.617 (1.617)\tLoss 2.6679 (2.6679)\t\n",
      "Epoch: [91][200/518]\tBatch Time 0.621 (0.658)\tData Time 0.000 (0.011)\tLoss 2.6139 (2.7077)\t\n",
      "Epoch: [91][400/518]\tBatch Time 0.642 (0.655)\tData Time 0.006 (0.007)\tLoss 2.8653 (2.6951)\t\n",
      "[0/155]\tBatch Time 0.928 (0.928)\tLoss 3.0572 (3.0572)\t\n",
      "\n",
      " * LOSS - 2.895\n",
      "\n",
      "Epoch: [92][0/518]\tBatch Time 2.857 (2.857)\tData Time 2.247 (2.247)\tLoss 2.8221 (2.8221)\t\n",
      "Epoch: [92][200/518]\tBatch Time 0.676 (0.665)\tData Time 0.001 (0.015)\tLoss 2.8148 (2.6941)\t\n",
      "Epoch: [92][400/518]\tBatch Time 0.610 (0.658)\tData Time 0.000 (0.009)\tLoss 2.9034 (2.7053)\t\n",
      "[0/155]\tBatch Time 1.084 (1.084)\tLoss 3.0623 (3.0623)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 2.888\n",
      "\n",
      "Epoch: [93][0/518]\tBatch Time 2.449 (2.449)\tData Time 1.764 (1.764)\tLoss 2.4469 (2.4469)\t\n",
      "Epoch: [93][200/518]\tBatch Time 0.652 (0.657)\tData Time 0.002 (0.012)\tLoss 2.6289 (2.6846)\t\n",
      "Epoch: [93][400/518]\tBatch Time 0.629 (0.655)\tData Time 0.001 (0.008)\tLoss 2.8832 (2.6810)\t\n",
      "[0/155]\tBatch Time 1.203 (1.203)\tLoss 3.1680 (3.1680)\t\n",
      "\n",
      " * LOSS - 2.885\n",
      "\n",
      "Epoch: [94][0/518]\tBatch Time 2.121 (2.121)\tData Time 1.383 (1.383)\tLoss 2.8245 (2.8245)\t\n",
      "Epoch: [94][200/518]\tBatch Time 0.713 (0.656)\tData Time 0.006 (0.010)\tLoss 2.7127 (2.7028)\t\n",
      "Epoch: [94][400/518]\tBatch Time 0.724 (0.655)\tData Time 0.000 (0.006)\tLoss 2.6356 (2.7074)\t\n",
      "[0/155]\tBatch Time 0.992 (0.992)\tLoss 2.9368 (2.9368)\t\n",
      "\n",
      " * LOSS - 2.884\n",
      "\n",
      "Epoch: [95][0/518]\tBatch Time 2.801 (2.801)\tData Time 2.164 (2.164)\tLoss 2.5754 (2.5754)\t\n",
      "Epoch: [95][200/518]\tBatch Time 0.637 (0.659)\tData Time 0.002 (0.014)\tLoss 2.9402 (2.6898)\t\n",
      "Epoch: [95][400/518]\tBatch Time 0.621 (0.655)\tData Time 0.001 (0.008)\tLoss 2.8774 (2.6774)\t\n",
      "[0/155]\tBatch Time 0.916 (0.916)\tLoss 2.7440 (2.7440)\t\n",
      "\n",
      " * LOSS - 2.938\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [96][0/518]\tBatch Time 2.610 (2.610)\tData Time 1.936 (1.936)\tLoss 2.8190 (2.8190)\t\n",
      "Epoch: [96][200/518]\tBatch Time 0.606 (0.665)\tData Time 0.001 (0.012)\tLoss 2.6485 (2.6590)\t\n",
      "Epoch: [96][400/518]\tBatch Time 0.622 (0.657)\tData Time 0.000 (0.007)\tLoss 2.6298 (2.6727)\t\n",
      "[0/155]\tBatch Time 1.046 (1.046)\tLoss 3.0403 (3.0403)\t\n",
      "\n",
      " * LOSS - 2.923\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [97][0/518]\tBatch Time 2.517 (2.517)\tData Time 1.830 (1.830)\tLoss 2.7436 (2.7436)\t\n",
      "Epoch: [97][200/518]\tBatch Time 0.728 (0.662)\tData Time 0.002 (0.012)\tLoss 2.4873 (2.6768)\t\n",
      "Epoch: [97][400/518]\tBatch Time 0.626 (0.656)\tData Time 0.001 (0.007)\tLoss 2.7692 (2.6759)\t\n",
      "[0/155]\tBatch Time 0.960 (0.960)\tLoss 3.2360 (3.2360)\t\n",
      "\n",
      " * LOSS - 2.892\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [98][0/518]\tBatch Time 2.263 (2.263)\tData Time 1.636 (1.636)\tLoss 2.5859 (2.5859)\t\n",
      "Epoch: [98][200/518]\tBatch Time 0.650 (0.658)\tData Time 0.000 (0.011)\tLoss 2.3919 (2.6809)\t\n",
      "Epoch: [98][400/518]\tBatch Time 0.631 (0.655)\tData Time 0.001 (0.007)\tLoss 2.6535 (2.6659)\t\n",
      "[0/155]\tBatch Time 1.131 (1.131)\tLoss 2.6414 (2.6414)\t\n",
      "\n",
      " * LOSS - 2.916\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [99][0/518]\tBatch Time 2.143 (2.143)\tData Time 1.435 (1.435)\tLoss 2.6297 (2.6297)\t\n",
      "Epoch: [99][200/518]\tBatch Time 0.633 (0.658)\tData Time 0.002 (0.011)\tLoss 2.6669 (2.6353)\t\n",
      "Epoch: [99][400/518]\tBatch Time 0.627 (0.656)\tData Time 0.000 (0.007)\tLoss 2.8718 (2.6477)\t\n",
      "[0/155]\tBatch Time 0.860 (0.860)\tLoss 3.1536 (3.1536)\t\n",
      "\n",
      " * LOSS - 2.866\n",
      "\n",
      "Epoch: [100][0/518]\tBatch Time 2.891 (2.891)\tData Time 2.267 (2.267)\tLoss 2.5381 (2.5381)\t\n",
      "Epoch: [100][200/518]\tBatch Time 0.620 (0.662)\tData Time 0.002 (0.014)\tLoss 2.6677 (2.6523)\t\n",
      "Epoch: [100][400/518]\tBatch Time 0.705 (0.656)\tData Time 0.009 (0.008)\tLoss 2.4101 (2.6442)\t\n",
      "[0/155]\tBatch Time 1.009 (1.009)\tLoss 2.6680 (2.6680)\t\n",
      "\n",
      " * LOSS - 2.880\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [101][0/518]\tBatch Time 2.318 (2.318)\tData Time 1.667 (1.667)\tLoss 2.3201 (2.3201)\t\n",
      "Epoch: [101][200/518]\tBatch Time 0.650 (0.659)\tData Time 0.009 (0.012)\tLoss 2.5864 (2.6469)\t\n",
      "Epoch: [101][400/518]\tBatch Time 0.720 (0.655)\tData Time 0.000 (0.007)\tLoss 2.6293 (2.6483)\t\n",
      "[0/155]\tBatch Time 1.022 (1.022)\tLoss 2.6887 (2.6887)\t\n",
      "\n",
      " * LOSS - 2.880\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [102][0/518]\tBatch Time 2.410 (2.410)\tData Time 1.782 (1.782)\tLoss 2.7065 (2.7065)\t\n",
      "Epoch: [102][200/518]\tBatch Time 0.622 (0.657)\tData Time 0.000 (0.011)\tLoss 2.7322 (2.6441)\t\n",
      "Epoch: [102][400/518]\tBatch Time 0.620 (0.654)\tData Time 0.002 (0.007)\tLoss 2.4723 (2.6327)\t\n",
      "[0/155]\tBatch Time 0.965 (0.965)\tLoss 2.7043 (2.7043)\t\n",
      "\n",
      " * LOSS - 2.866\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [103][0/518]\tBatch Time 2.890 (2.890)\tData Time 2.260 (2.260)\tLoss 2.6277 (2.6277)\t\n",
      "Epoch: [103][200/518]\tBatch Time 0.646 (0.664)\tData Time 0.002 (0.014)\tLoss 2.4293 (2.6124)\t\n",
      "Epoch: [103][400/518]\tBatch Time 0.659 (0.658)\tData Time 0.000 (0.008)\tLoss 2.8394 (2.6194)\t\n",
      "[0/155]\tBatch Time 0.957 (0.957)\tLoss 2.8040 (2.8040)\t\n",
      "\n",
      " * LOSS - 2.838\n",
      "\n",
      "Epoch: [104][0/518]\tBatch Time 2.416 (2.416)\tData Time 1.792 (1.792)\tLoss 2.5154 (2.5154)\t\n",
      "Epoch: [104][200/518]\tBatch Time 0.651 (0.657)\tData Time 0.002 (0.012)\tLoss 2.6447 (2.6140)\t\n",
      "Epoch: [104][400/518]\tBatch Time 0.605 (0.654)\tData Time 0.001 (0.007)\tLoss 2.6816 (2.6209)\t\n",
      "[0/155]\tBatch Time 0.874 (0.874)\tLoss 2.7913 (2.7913)\t\n",
      "\n",
      " * LOSS - 2.867\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [105][0/518]\tBatch Time 2.164 (2.164)\tData Time 1.459 (1.459)\tLoss 2.4842 (2.4842)\t\n",
      "Epoch: [105][200/518]\tBatch Time 0.655 (0.661)\tData Time 0.000 (0.010)\tLoss 2.4412 (2.6005)\t\n",
      "Epoch: [105][400/518]\tBatch Time 0.735 (0.657)\tData Time 0.009 (0.006)\tLoss 2.6996 (2.6065)\t\n",
      "[0/155]\tBatch Time 0.916 (0.916)\tLoss 2.5049 (2.5049)\t\n",
      "\n",
      " * LOSS - 2.855\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [106][0/518]\tBatch Time 2.005 (2.005)\tData Time 1.386 (1.386)\tLoss 2.8285 (2.8285)\t\n",
      "Epoch: [106][200/518]\tBatch Time 0.619 (0.656)\tData Time 0.002 (0.011)\tLoss 2.8708 (2.6321)\t\n",
      "Epoch: [106][400/518]\tBatch Time 0.743 (0.654)\tData Time 0.000 (0.007)\tLoss 2.4683 (2.6216)\t\n",
      "[0/155]\tBatch Time 0.883 (0.883)\tLoss 2.8491 (2.8491)\t\n",
      "\n",
      " * LOSS - 2.845\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [107][0/518]\tBatch Time 2.509 (2.509)\tData Time 1.879 (1.879)\tLoss 2.7584 (2.7584)\t\n",
      "Epoch: [107][200/518]\tBatch Time 0.628 (0.657)\tData Time 0.002 (0.012)\tLoss 2.5501 (2.6104)\t\n",
      "Epoch: [107][400/518]\tBatch Time 0.678 (0.653)\tData Time 0.001 (0.008)\tLoss 2.6752 (2.6089)\t\n",
      "[0/155]\tBatch Time 0.973 (0.973)\tLoss 2.7016 (2.7016)\t\n",
      "\n",
      " * LOSS - 2.828\n",
      "\n",
      "Epoch: [108][0/518]\tBatch Time 2.588 (2.588)\tData Time 1.900 (1.900)\tLoss 2.4686 (2.4686)\t\n",
      "Epoch: [108][200/518]\tBatch Time 0.606 (0.658)\tData Time 0.000 (0.012)\tLoss 2.4007 (2.5671)\t\n",
      "Epoch: [108][400/518]\tBatch Time 0.629 (0.654)\tData Time 0.000 (0.008)\tLoss 2.6291 (2.5919)\t\n",
      "[0/155]\tBatch Time 0.938 (0.938)\tLoss 2.7701 (2.7701)\t\n",
      "\n",
      " * LOSS - 2.840\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [109][0/518]\tBatch Time 2.430 (2.430)\tData Time 1.654 (1.654)\tLoss 2.2725 (2.2725)\t\n",
      "Epoch: [109][200/518]\tBatch Time 0.712 (0.659)\tData Time 0.000 (0.011)\tLoss 2.5514 (2.5968)\t\n",
      "Epoch: [109][400/518]\tBatch Time 0.629 (0.656)\tData Time 0.002 (0.007)\tLoss 2.6482 (2.5886)\t\n",
      "[0/155]\tBatch Time 0.829 (0.829)\tLoss 3.0005 (3.0005)\t\n",
      "\n",
      " * LOSS - 2.840\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [110][0/518]\tBatch Time 2.285 (2.285)\tData Time 1.541 (1.541)\tLoss 2.6709 (2.6709)\t\n",
      "Epoch: [110][200/518]\tBatch Time 0.623 (0.660)\tData Time 0.002 (0.011)\tLoss 3.0060 (2.6026)\t\n",
      "Epoch: [110][400/518]\tBatch Time 0.662 (0.654)\tData Time 0.001 (0.007)\tLoss 2.7591 (2.5909)\t\n",
      "[0/155]\tBatch Time 0.964 (0.964)\tLoss 2.8235 (2.8235)\t\n",
      "\n",
      " * LOSS - 2.820\n",
      "\n",
      "Epoch: [111][0/518]\tBatch Time 1.953 (1.953)\tData Time 1.324 (1.324)\tLoss 2.2856 (2.2856)\t\n",
      "Epoch: [111][200/518]\tBatch Time 0.695 (0.661)\tData Time 0.002 (0.012)\tLoss 2.5861 (2.5661)\t\n",
      "Epoch: [111][400/518]\tBatch Time 0.628 (0.653)\tData Time 0.000 (0.007)\tLoss 2.8873 (2.5649)\t\n",
      "[0/155]\tBatch Time 0.921 (0.921)\tLoss 2.8597 (2.8597)\t\n",
      "\n",
      " * LOSS - 2.846\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [112][0/518]\tBatch Time 2.818 (2.818)\tData Time 2.203 (2.203)\tLoss 2.0425 (2.0425)\t\n",
      "Epoch: [112][200/518]\tBatch Time 0.635 (0.664)\tData Time 0.000 (0.014)\tLoss 2.8183 (2.5579)\t\n",
      "Epoch: [112][400/518]\tBatch Time 0.627 (0.657)\tData Time 0.001 (0.008)\tLoss 2.4390 (2.5620)\t\n",
      "[0/155]\tBatch Time 0.836 (0.836)\tLoss 2.5304 (2.5304)\t\n",
      "\n",
      " * LOSS - 2.833\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [113][0/518]\tBatch Time 2.816 (2.816)\tData Time 2.099 (2.099)\tLoss 2.5975 (2.5975)\t\n",
      "Epoch: [113][200/518]\tBatch Time 0.652 (0.661)\tData Time 0.000 (0.013)\tLoss 2.4761 (2.5795)\t\n",
      "Epoch: [113][400/518]\tBatch Time 0.610 (0.656)\tData Time 0.000 (0.008)\tLoss 2.6915 (2.5670)\t\n",
      "[0/155]\tBatch Time 1.131 (1.131)\tLoss 2.6303 (2.6303)\t\n",
      "\n",
      " * LOSS - 2.817\n",
      "\n",
      "Epoch: [114][0/518]\tBatch Time 2.711 (2.711)\tData Time 2.082 (2.082)\tLoss 2.9986 (2.9986)\t\n",
      "Epoch: [114][200/518]\tBatch Time 0.631 (0.661)\tData Time 0.001 (0.013)\tLoss 2.9256 (2.5464)\t\n",
      "Epoch: [114][400/518]\tBatch Time 0.631 (0.659)\tData Time 0.009 (0.008)\tLoss 2.5278 (2.5557)\t\n",
      "[0/155]\tBatch Time 1.046 (1.046)\tLoss 2.8252 (2.8252)\t\n",
      "\n",
      " * LOSS - 2.849\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [115][0/518]\tBatch Time 2.901 (2.901)\tData Time 2.273 (2.273)\tLoss 2.3536 (2.3536)\t\n",
      "Epoch: [115][200/518]\tBatch Time 0.650 (0.666)\tData Time 0.009 (0.014)\tLoss 2.4044 (2.5817)\t\n",
      "Epoch: [115][400/518]\tBatch Time 0.663 (0.658)\tData Time 0.002 (0.008)\tLoss 2.5519 (2.5585)\t\n",
      "[0/155]\tBatch Time 0.933 (0.933)\tLoss 2.7600 (2.7600)\t\n",
      "\n",
      " * LOSS - 2.807\n",
      "\n",
      "Epoch: [116][0/518]\tBatch Time 2.790 (2.790)\tData Time 2.163 (2.163)\tLoss 2.6675 (2.6675)\t\n",
      "Epoch: [116][200/518]\tBatch Time 0.633 (0.665)\tData Time 0.024 (0.013)\tLoss 2.5521 (2.5471)\t\n",
      "Epoch: [116][400/518]\tBatch Time 0.697 (0.659)\tData Time 0.009 (0.008)\tLoss 2.4437 (2.5582)\t\n",
      "[0/155]\tBatch Time 1.154 (1.154)\tLoss 2.8803 (2.8803)\t\n",
      "\n",
      " * LOSS - 2.812\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [117][0/518]\tBatch Time 2.535 (2.535)\tData Time 1.751 (1.751)\tLoss 2.7602 (2.7602)\t\n",
      "Epoch: [117][200/518]\tBatch Time 0.642 (0.661)\tData Time 0.009 (0.013)\tLoss 2.2876 (2.5414)\t\n",
      "Epoch: [117][400/518]\tBatch Time 0.639 (0.656)\tData Time 0.002 (0.008)\tLoss 2.5491 (2.5366)\t\n",
      "[0/155]\tBatch Time 1.078 (1.078)\tLoss 2.9311 (2.9311)\t\n",
      "\n",
      " * LOSS - 2.824\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [118][0/518]\tBatch Time 2.432 (2.432)\tData Time 1.674 (1.674)\tLoss 2.7340 (2.7340)\t\n",
      "Epoch: [118][200/518]\tBatch Time 0.686 (0.660)\tData Time 0.001 (0.011)\tLoss 2.1816 (2.5378)\t\n",
      "Epoch: [118][400/518]\tBatch Time 0.654 (0.654)\tData Time 0.002 (0.007)\tLoss 2.8273 (2.5247)\t\n",
      "[0/155]\tBatch Time 0.981 (0.981)\tLoss 3.0840 (3.0840)\t\n",
      "\n",
      " * LOSS - 2.822\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [119][0/518]\tBatch Time 2.807 (2.807)\tData Time 2.174 (2.174)\tLoss 2.3444 (2.3444)\t\n",
      "Epoch: [119][200/518]\tBatch Time 0.652 (0.663)\tData Time 0.002 (0.013)\tLoss 2.2081 (2.5306)\t\n",
      "Epoch: [119][400/518]\tBatch Time 0.615 (0.656)\tData Time 0.002 (0.008)\tLoss 2.3555 (2.5258)\t\n",
      "[0/155]\tBatch Time 0.872 (0.872)\tLoss 2.8782 (2.8782)\t\n",
      "\n",
      " * LOSS - 2.798\n",
      "\n",
      "Epoch: [120][0/518]\tBatch Time 2.606 (2.606)\tData Time 1.973 (1.973)\tLoss 2.8067 (2.8067)\t\n",
      "Epoch: [120][200/518]\tBatch Time 0.619 (0.663)\tData Time 0.000 (0.013)\tLoss 2.3492 (2.4994)\t\n",
      "Epoch: [120][400/518]\tBatch Time 0.684 (0.657)\tData Time 0.000 (0.008)\tLoss 2.4803 (2.5090)\t\n",
      "[0/155]\tBatch Time 0.853 (0.853)\tLoss 3.7444 (3.7444)\t\n",
      "\n",
      " * LOSS - 2.790\n",
      "\n",
      "Epoch: [121][0/518]\tBatch Time 2.551 (2.551)\tData Time 1.874 (1.874)\tLoss 2.7224 (2.7224)\t\n",
      "Epoch: [121][200/518]\tBatch Time 0.712 (0.666)\tData Time 0.000 (0.013)\tLoss 2.7990 (2.4869)\t\n",
      "Epoch: [121][400/518]\tBatch Time 0.625 (0.658)\tData Time 0.006 (0.008)\tLoss 2.4352 (2.4988)\t\n",
      "[0/155]\tBatch Time 0.995 (0.995)\tLoss 2.9540 (2.9540)\t\n",
      "\n",
      " * LOSS - 2.824\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [122][0/518]\tBatch Time 2.737 (2.737)\tData Time 2.040 (2.040)\tLoss 2.5047 (2.5047)\t\n",
      "Epoch: [122][200/518]\tBatch Time 0.679 (0.662)\tData Time 0.001 (0.013)\tLoss 2.5101 (2.5099)\t\n",
      "Epoch: [122][400/518]\tBatch Time 0.704 (0.656)\tData Time 0.002 (0.008)\tLoss 2.4805 (2.5189)\t\n",
      "[0/155]\tBatch Time 1.129 (1.129)\tLoss 2.9384 (2.9384)\t\n",
      "\n",
      " * LOSS - 2.812\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [123][0/518]\tBatch Time 2.393 (2.393)\tData Time 1.751 (1.751)\tLoss 2.7202 (2.7202)\t\n",
      "Epoch: [123][200/518]\tBatch Time 0.623 (0.659)\tData Time 0.001 (0.011)\tLoss 2.3485 (2.4940)\t\n",
      "Epoch: [123][400/518]\tBatch Time 0.627 (0.654)\tData Time 0.000 (0.007)\tLoss 2.9958 (2.5092)\t\n",
      "[0/155]\tBatch Time 0.965 (0.965)\tLoss 2.5966 (2.5966)\t\n",
      "\n",
      " * LOSS - 2.812\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [124][0/518]\tBatch Time 2.393 (2.393)\tData Time 1.762 (1.762)\tLoss 2.4782 (2.4782)\t\n",
      "Epoch: [124][200/518]\tBatch Time 0.632 (0.659)\tData Time 0.004 (0.012)\tLoss 2.4894 (2.4964)\t\n",
      "Epoch: [124][400/518]\tBatch Time 0.626 (0.656)\tData Time 0.001 (0.007)\tLoss 2.5541 (2.4985)\t\n",
      "[0/155]\tBatch Time 0.805 (0.805)\tLoss 2.8351 (2.8351)\t\n",
      "\n",
      " * LOSS - 2.808\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [125][0/518]\tBatch Time 2.490 (2.490)\tData Time 1.804 (1.804)\tLoss 2.7673 (2.7673)\t\n",
      "Epoch: [125][200/518]\tBatch Time 0.624 (0.662)\tData Time 0.000 (0.011)\tLoss 2.4227 (2.4979)\t\n",
      "Epoch: [125][400/518]\tBatch Time 0.638 (0.656)\tData Time 0.002 (0.007)\tLoss 2.5778 (2.5005)\t\n",
      "[0/155]\tBatch Time 0.996 (0.996)\tLoss 2.6174 (2.6174)\t\n",
      "\n",
      " * LOSS - 2.817\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [126][0/518]\tBatch Time 2.668 (2.668)\tData Time 2.039 (2.039)\tLoss 2.5947 (2.5947)\t\n",
      "Epoch: [126][200/518]\tBatch Time 0.638 (0.663)\tData Time 0.002 (0.013)\tLoss 2.8875 (2.5086)\t\n",
      "Epoch: [126][400/518]\tBatch Time 0.654 (0.657)\tData Time 0.009 (0.008)\tLoss 2.0621 (2.4851)\t\n",
      "[0/155]\tBatch Time 0.913 (0.913)\tLoss 2.9492 (2.9492)\t\n",
      "\n",
      " * LOSS - 2.806\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [127][0/518]\tBatch Time 2.081 (2.081)\tData Time 1.450 (1.450)\tLoss 2.5265 (2.5265)\t\n",
      "Epoch: [127][200/518]\tBatch Time 0.692 (0.661)\tData Time 0.002 (0.010)\tLoss 2.5036 (2.5014)\t\n",
      "Epoch: [127][400/518]\tBatch Time 0.604 (0.656)\tData Time 0.000 (0.006)\tLoss 2.5655 (2.4901)\t\n",
      "[0/155]\tBatch Time 0.906 (0.906)\tLoss 2.7159 (2.7159)\t\n",
      "\n",
      " * LOSS - 2.771\n",
      "\n",
      "Epoch: [128][0/518]\tBatch Time 2.390 (2.390)\tData Time 1.685 (1.685)\tLoss 2.4220 (2.4220)\t\n",
      "Epoch: [128][200/518]\tBatch Time 0.678 (0.662)\tData Time 0.001 (0.012)\tLoss 2.3793 (2.4861)\t\n",
      "Epoch: [128][400/518]\tBatch Time 0.625 (0.656)\tData Time 0.000 (0.007)\tLoss 2.3122 (2.4882)\t\n",
      "[0/155]\tBatch Time 0.970 (0.970)\tLoss 2.5544 (2.5544)\t\n",
      "\n",
      " * LOSS - 2.787\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [129][0/518]\tBatch Time 2.723 (2.723)\tData Time 2.016 (2.016)\tLoss 2.6609 (2.6609)\t\n",
      "Epoch: [129][200/518]\tBatch Time 0.627 (0.664)\tData Time 0.001 (0.013)\tLoss 2.7335 (2.4676)\t\n",
      "Epoch: [129][400/518]\tBatch Time 0.724 (0.659)\tData Time 0.001 (0.008)\tLoss 2.8825 (2.4746)\t\n",
      "[0/155]\tBatch Time 0.941 (0.941)\tLoss 2.9095 (2.9095)\t\n",
      "\n",
      " * LOSS - 2.776\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [130][0/518]\tBatch Time 2.484 (2.484)\tData Time 1.780 (1.780)\tLoss 2.4624 (2.4624)\t\n",
      "Epoch: [130][200/518]\tBatch Time 0.640 (0.661)\tData Time 0.009 (0.015)\tLoss 2.4219 (2.4579)\t\n",
      "Epoch: [130][400/518]\tBatch Time 0.646 (0.657)\tData Time 0.000 (0.008)\tLoss 2.6630 (2.4638)\t\n",
      "[0/155]\tBatch Time 0.845 (0.845)\tLoss 3.0241 (3.0241)\t\n",
      "\n",
      " * LOSS - 2.785\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [131][0/518]\tBatch Time 2.746 (2.746)\tData Time 2.030 (2.030)\tLoss 2.3632 (2.3632)\t\n",
      "Epoch: [131][200/518]\tBatch Time 0.617 (0.660)\tData Time 0.002 (0.012)\tLoss 2.6480 (2.4549)\t\n",
      "Epoch: [131][400/518]\tBatch Time 0.696 (0.655)\tData Time 0.019 (0.007)\tLoss 2.6560 (2.4611)\t\n",
      "[0/155]\tBatch Time 0.962 (0.962)\tLoss 2.6888 (2.6888)\t\n",
      "\n",
      " * LOSS - 2.782\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [132][0/518]\tBatch Time 2.228 (2.228)\tData Time 1.472 (1.472)\tLoss 2.2033 (2.2033)\t\n",
      "Epoch: [132][200/518]\tBatch Time 0.634 (0.658)\tData Time 0.002 (0.009)\tLoss 2.3712 (2.4796)\t\n",
      "Epoch: [132][400/518]\tBatch Time 0.625 (0.653)\tData Time 0.002 (0.006)\tLoss 2.0586 (2.4547)\t\n",
      "[0/155]\tBatch Time 0.963 (0.963)\tLoss 2.5341 (2.5341)\t\n",
      "\n",
      " * LOSS - 2.785\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [133][0/518]\tBatch Time 2.129 (2.129)\tData Time 1.502 (1.502)\tLoss 2.2337 (2.2337)\t\n",
      "Epoch: [133][200/518]\tBatch Time 0.736 (0.661)\tData Time 0.009 (0.010)\tLoss 2.3350 (2.4631)\t\n",
      "Epoch: [133][400/518]\tBatch Time 0.625 (0.656)\tData Time 0.000 (0.006)\tLoss 2.3768 (2.4716)\t\n",
      "[0/155]\tBatch Time 1.134 (1.134)\tLoss 2.9953 (2.9953)\t\n",
      "\n",
      " * LOSS - 2.774\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [134][0/518]\tBatch Time 2.522 (2.522)\tData Time 1.809 (1.809)\tLoss 2.2784 (2.2784)\t\n",
      "Epoch: [134][200/518]\tBatch Time 0.627 (0.659)\tData Time 0.000 (0.011)\tLoss 2.4601 (2.4332)\t\n",
      "Epoch: [134][400/518]\tBatch Time 0.626 (0.656)\tData Time 0.002 (0.007)\tLoss 2.6316 (2.4454)\t\n",
      "[0/155]\tBatch Time 1.069 (1.069)\tLoss 2.6840 (2.6840)\t\n",
      "\n",
      " * LOSS - 2.804\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [135][0/518]\tBatch Time 1.950 (1.950)\tData Time 1.335 (1.335)\tLoss 2.5063 (2.5063)\t\n",
      "Epoch: [135][200/518]\tBatch Time 0.626 (0.659)\tData Time 0.009 (0.012)\tLoss 2.5257 (2.4314)\t\n",
      "Epoch: [135][400/518]\tBatch Time 0.642 (0.657)\tData Time 0.002 (0.007)\tLoss 2.4341 (2.4327)\t\n",
      "[0/155]\tBatch Time 1.002 (1.002)\tLoss 2.5496 (2.5496)\t\n",
      "\n",
      " * LOSS - 2.789\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [136][0/518]\tBatch Time 2.421 (2.421)\tData Time 1.736 (1.736)\tLoss 2.2458 (2.2458)\t\n",
      "Epoch: [136][200/518]\tBatch Time 0.618 (0.660)\tData Time 0.002 (0.011)\tLoss 2.7558 (2.4195)\t\n",
      "Epoch: [136][400/518]\tBatch Time 0.639 (0.657)\tData Time 0.009 (0.007)\tLoss 2.3773 (2.4341)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/155]\tBatch Time 0.903 (0.903)\tLoss 2.3933 (2.3933)\t\n",
      "\n",
      " * LOSS - 2.767\n",
      "\n",
      "Epoch: [137][0/518]\tBatch Time 1.925 (1.925)\tData Time 1.283 (1.283)\tLoss 2.5530 (2.5530)\t\n",
      "Epoch: [137][200/518]\tBatch Time 0.638 (0.659)\tData Time 0.009 (0.010)\tLoss 2.6679 (2.4429)\t\n",
      "Epoch: [137][400/518]\tBatch Time 0.657 (0.656)\tData Time 0.009 (0.007)\tLoss 2.0779 (2.4313)\t\n",
      "[0/155]\tBatch Time 1.139 (1.139)\tLoss 2.6684 (2.6684)\t\n",
      "\n",
      " * LOSS - 2.790\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [138][0/518]\tBatch Time 2.374 (2.374)\tData Time 1.734 (1.734)\tLoss 2.5161 (2.5161)\t\n",
      "Epoch: [138][200/518]\tBatch Time 0.643 (0.661)\tData Time 0.002 (0.011)\tLoss 2.3510 (2.4217)\t\n",
      "Epoch: [138][400/518]\tBatch Time 0.673 (0.653)\tData Time 0.002 (0.007)\tLoss 2.6365 (2.4245)\t\n",
      "[0/155]\tBatch Time 0.983 (0.983)\tLoss 2.5915 (2.5915)\t\n",
      "\n",
      " * LOSS - 2.775\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [139][0/518]\tBatch Time 2.339 (2.339)\tData Time 1.708 (1.708)\tLoss 2.3081 (2.3081)\t\n",
      "Epoch: [139][200/518]\tBatch Time 0.626 (0.663)\tData Time 0.009 (0.011)\tLoss 2.4229 (2.4150)\t\n",
      "Epoch: [139][400/518]\tBatch Time 0.611 (0.656)\tData Time 0.001 (0.007)\tLoss 2.6884 (2.4209)\t\n",
      "[0/155]\tBatch Time 0.967 (0.967)\tLoss 2.8920 (2.8920)\t\n",
      "\n",
      " * LOSS - 2.769\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [140][0/518]\tBatch Time 2.692 (2.692)\tData Time 2.058 (2.058)\tLoss 2.3433 (2.3433)\t\n",
      "Epoch: [140][200/518]\tBatch Time 0.605 (0.662)\tData Time 0.001 (0.014)\tLoss 2.3136 (2.4144)\t\n",
      "Epoch: [140][400/518]\tBatch Time 0.631 (0.656)\tData Time 0.010 (0.008)\tLoss 3.0252 (2.4110)\t\n",
      "[0/155]\tBatch Time 0.939 (0.939)\tLoss 2.7566 (2.7566)\t\n",
      "\n",
      " * LOSS - 2.775\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [141][0/518]\tBatch Time 2.184 (2.184)\tData Time 1.470 (1.470)\tLoss 2.9993 (2.9993)\t\n",
      "Epoch: [141][200/518]\tBatch Time 0.714 (0.660)\tData Time 0.022 (0.012)\tLoss 2.2299 (2.4310)\t\n",
      "Epoch: [141][400/518]\tBatch Time 0.652 (0.654)\tData Time 0.002 (0.007)\tLoss 2.5031 (2.4174)\t\n",
      "[0/155]\tBatch Time 1.159 (1.159)\tLoss 2.4603 (2.4603)\t\n",
      "\n",
      " * LOSS - 2.761\n",
      "\n",
      "Epoch: [142][0/518]\tBatch Time 2.819 (2.819)\tData Time 2.109 (2.109)\tLoss 2.3976 (2.3976)\t\n",
      "Epoch: [142][200/518]\tBatch Time 0.631 (0.660)\tData Time 0.002 (0.013)\tLoss 2.5160 (2.4086)\t\n",
      "Epoch: [142][400/518]\tBatch Time 0.679 (0.654)\tData Time 0.002 (0.008)\tLoss 2.1765 (2.3995)\t\n",
      "[0/155]\tBatch Time 0.970 (0.970)\tLoss 2.2092 (2.2092)\t\n",
      "\n",
      " * LOSS - 2.772\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [143][0/518]\tBatch Time 2.489 (2.489)\tData Time 1.736 (1.736)\tLoss 2.6608 (2.6608)\t\n",
      "Epoch: [143][200/518]\tBatch Time 0.635 (0.657)\tData Time 0.001 (0.012)\tLoss 2.5426 (2.4044)\t\n",
      "Epoch: [143][400/518]\tBatch Time 0.627 (0.654)\tData Time 0.000 (0.007)\tLoss 2.1846 (2.4056)\t\n",
      "[0/155]\tBatch Time 0.896 (0.896)\tLoss 3.0517 (3.0517)\t\n",
      "\n",
      " * LOSS - 2.762\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [144][0/518]\tBatch Time 2.162 (2.162)\tData Time 1.526 (1.526)\tLoss 2.3406 (2.3406)\t\n",
      "Epoch: [144][200/518]\tBatch Time 0.723 (0.660)\tData Time 0.009 (0.011)\tLoss 2.4570 (2.4019)\t\n",
      "Epoch: [144][400/518]\tBatch Time 0.632 (0.655)\tData Time 0.002 (0.007)\tLoss 2.1375 (2.4002)\t\n",
      "[0/155]\tBatch Time 1.148 (1.148)\tLoss 2.6503 (2.6503)\t\n",
      "\n",
      " * LOSS - 2.761\n",
      "\n",
      "Epoch: [145][0/518]\tBatch Time 2.497 (2.497)\tData Time 1.869 (1.869)\tLoss 2.4449 (2.4449)\t\n",
      "Epoch: [145][200/518]\tBatch Time 0.607 (0.662)\tData Time 0.000 (0.012)\tLoss 2.3229 (2.3980)\t\n",
      "Epoch: [145][400/518]\tBatch Time 0.629 (0.656)\tData Time 0.000 (0.007)\tLoss 2.6419 (2.3872)\t\n",
      "[0/155]\tBatch Time 1.204 (1.204)\tLoss 2.3520 (2.3520)\t\n",
      "\n",
      " * LOSS - 2.752\n",
      "\n",
      "Epoch: [146][0/518]\tBatch Time 1.973 (1.973)\tData Time 1.339 (1.339)\tLoss 2.2080 (2.2080)\t\n",
      "Epoch: [146][200/518]\tBatch Time 0.621 (0.660)\tData Time 0.000 (0.010)\tLoss 2.3235 (2.3593)\t\n",
      "Epoch: [146][400/518]\tBatch Time 0.705 (0.657)\tData Time 0.001 (0.006)\tLoss 2.1839 (2.3784)\t\n",
      "[0/155]\tBatch Time 1.088 (1.088)\tLoss 2.8147 (2.8147)\t\n",
      "\n",
      " * LOSS - 2.754\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [147][0/518]\tBatch Time 2.461 (2.461)\tData Time 1.794 (1.794)\tLoss 2.0658 (2.0658)\t\n",
      "Epoch: [147][200/518]\tBatch Time 0.637 (0.661)\tData Time 0.000 (0.012)\tLoss 2.6249 (2.3744)\t\n",
      "Epoch: [147][400/518]\tBatch Time 0.625 (0.656)\tData Time 0.000 (0.007)\tLoss 2.5038 (2.3839)\t\n",
      "[0/155]\tBatch Time 1.124 (1.124)\tLoss 2.9804 (2.9804)\t\n",
      "\n",
      " * LOSS - 2.771\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [148][0/518]\tBatch Time 2.381 (2.381)\tData Time 1.654 (1.654)\tLoss 2.2679 (2.2679)\t\n",
      "Epoch: [148][200/518]\tBatch Time 0.715 (0.664)\tData Time 0.017 (0.012)\tLoss 2.3528 (2.3744)\t\n",
      "Epoch: [148][400/518]\tBatch Time 0.610 (0.657)\tData Time 0.001 (0.007)\tLoss 2.6381 (2.3842)\t\n",
      "[0/155]\tBatch Time 1.097 (1.097)\tLoss 3.0008 (3.0008)\t\n",
      "\n",
      " * LOSS - 2.759\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [149][0/518]\tBatch Time 2.140 (2.140)\tData Time 1.446 (1.446)\tLoss 2.0900 (2.0900)\t\n",
      "Epoch: [149][200/518]\tBatch Time 0.617 (0.660)\tData Time 0.000 (0.011)\tLoss 2.3927 (2.3709)\t\n",
      "Epoch: [149][400/518]\tBatch Time 0.624 (0.659)\tData Time 0.000 (0.007)\tLoss 2.3220 (2.3787)\t\n",
      "[0/155]\tBatch Time 0.852 (0.852)\tLoss 2.7836 (2.7836)\t\n",
      "\n",
      " * LOSS - 2.750\n",
      "\n",
      "Epoch: [150][0/518]\tBatch Time 2.096 (2.096)\tData Time 1.474 (1.474)\tLoss 2.2646 (2.2646)\t\n",
      "Epoch: [150][200/518]\tBatch Time 0.720 (0.665)\tData Time 0.001 (0.011)\tLoss 2.4859 (2.3796)\t\n",
      "Epoch: [150][400/518]\tBatch Time 0.652 (0.661)\tData Time 0.009 (0.007)\tLoss 2.1006 (2.3777)\t\n",
      "[0/155]\tBatch Time 1.053 (1.053)\tLoss 2.9172 (2.9172)\t\n",
      "\n",
      " * LOSS - 2.769\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [151][0/518]\tBatch Time 2.004 (2.004)\tData Time 1.346 (1.346)\tLoss 2.1892 (2.1892)\t\n",
      "Epoch: [151][200/518]\tBatch Time 0.649 (0.660)\tData Time 0.001 (0.011)\tLoss 2.0344 (2.3839)\t\n",
      "Epoch: [151][400/518]\tBatch Time 0.748 (0.655)\tData Time 0.001 (0.007)\tLoss 2.3236 (2.3709)\t\n",
      "[0/155]\tBatch Time 0.863 (0.863)\tLoss 2.8331 (2.8331)\t\n",
      "\n",
      " * LOSS - 2.755\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [152][0/518]\tBatch Time 2.940 (2.940)\tData Time 2.299 (2.299)\tLoss 2.1867 (2.1867)\t\n",
      "Epoch: [152][200/518]\tBatch Time 0.630 (0.665)\tData Time 0.009 (0.015)\tLoss 2.4231 (2.3754)\t\n",
      "Epoch: [152][400/518]\tBatch Time 0.749 (0.657)\tData Time 0.022 (0.009)\tLoss 2.5554 (2.3809)\t\n",
      "[0/155]\tBatch Time 0.925 (0.925)\tLoss 2.8964 (2.8964)\t\n",
      "\n",
      " * LOSS - 2.740\n",
      "\n",
      "Epoch: [153][0/518]\tBatch Time 2.064 (2.064)\tData Time 1.351 (1.351)\tLoss 2.1745 (2.1745)\t\n",
      "Epoch: [153][200/518]\tBatch Time 0.644 (0.658)\tData Time 0.010 (0.011)\tLoss 1.9963 (2.3637)\t\n",
      "Epoch: [153][400/518]\tBatch Time 0.634 (0.655)\tData Time 0.000 (0.007)\tLoss 2.2262 (2.3694)\t\n",
      "[0/155]\tBatch Time 0.879 (0.879)\tLoss 2.7157 (2.7157)\t\n",
      "\n",
      " * LOSS - 2.750\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [154][0/518]\tBatch Time 2.603 (2.603)\tData Time 1.980 (1.980)\tLoss 2.3916 (2.3916)\t\n",
      "Epoch: [154][200/518]\tBatch Time 0.624 (0.665)\tData Time 0.000 (0.014)\tLoss 2.5346 (2.3695)\t\n",
      "Epoch: [154][400/518]\tBatch Time 0.647 (0.658)\tData Time 0.009 (0.008)\tLoss 2.4006 (2.3674)\t\n",
      "[0/155]\tBatch Time 0.863 (0.863)\tLoss 2.6448 (2.6448)\t\n",
      "\n",
      " * LOSS - 2.741\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [155][0/518]\tBatch Time 2.138 (2.138)\tData Time 1.447 (1.447)\tLoss 2.2303 (2.2303)\t\n",
      "Epoch: [155][200/518]\tBatch Time 0.644 (0.664)\tData Time 0.001 (0.011)\tLoss 2.5245 (2.3818)\t\n",
      "Epoch: [155][400/518]\tBatch Time 0.624 (0.658)\tData Time 0.002 (0.007)\tLoss 2.0108 (2.3626)\t\n",
      "[0/155]\tBatch Time 0.795 (0.795)\tLoss 2.5008 (2.5008)\t\n",
      "\n",
      " * LOSS - 2.741\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [156][0/518]\tBatch Time 1.891 (1.891)\tData Time 1.269 (1.269)\tLoss 2.3859 (2.3859)\t\n",
      "Epoch: [156][200/518]\tBatch Time 0.685 (0.657)\tData Time 0.000 (0.009)\tLoss 1.9945 (2.3539)\t\n",
      "Epoch: [156][400/518]\tBatch Time 0.630 (0.655)\tData Time 0.000 (0.006)\tLoss 2.6892 (2.3568)\t\n",
      "[0/155]\tBatch Time 1.036 (1.036)\tLoss 2.6298 (2.6298)\t\n",
      "\n",
      " * LOSS - 2.749\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [157][0/518]\tBatch Time 2.344 (2.344)\tData Time 1.628 (1.628)\tLoss 2.4097 (2.4097)\t\n",
      "Epoch: [157][200/518]\tBatch Time 0.630 (0.658)\tData Time 0.002 (0.011)\tLoss 2.4358 (2.3473)\t\n",
      "Epoch: [157][400/518]\tBatch Time 0.658 (0.656)\tData Time 0.009 (0.007)\tLoss 2.1814 (2.3556)\t\n",
      "[0/155]\tBatch Time 0.820 (0.820)\tLoss 2.8907 (2.8907)\t\n",
      "\n",
      " * LOSS - 2.750\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [158][0/518]\tBatch Time 2.317 (2.317)\tData Time 1.600 (1.600)\tLoss 2.4217 (2.4217)\t\n",
      "Epoch: [158][200/518]\tBatch Time 0.627 (0.659)\tData Time 0.000 (0.010)\tLoss 2.3344 (2.3335)\t\n",
      "Epoch: [158][400/518]\tBatch Time 0.658 (0.654)\tData Time 0.002 (0.006)\tLoss 2.3544 (2.3332)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/155]\tBatch Time 0.903 (0.903)\tLoss 2.8918 (2.8918)\t\n",
      "\n",
      " * LOSS - 2.745\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [159][0/518]\tBatch Time 2.584 (2.584)\tData Time 1.835 (1.835)\tLoss 2.2915 (2.2915)\t\n",
      "Epoch: [159][200/518]\tBatch Time 0.633 (0.663)\tData Time 0.002 (0.012)\tLoss 2.5159 (2.3690)\t\n",
      "Epoch: [159][400/518]\tBatch Time 0.635 (0.656)\tData Time 0.000 (0.007)\tLoss 2.5616 (2.3403)\t\n",
      "[0/155]\tBatch Time 0.842 (0.842)\tLoss 2.6754 (2.6754)\t\n",
      "\n",
      " * LOSS - 2.730\n",
      "\n",
      "Epoch: [160][0/518]\tBatch Time 2.380 (2.380)\tData Time 1.597 (1.597)\tLoss 1.9169 (1.9169)\t\n",
      "Epoch: [160][200/518]\tBatch Time 0.656 (0.662)\tData Time 0.001 (0.011)\tLoss 2.8049 (2.3409)\t\n",
      "Epoch: [160][400/518]\tBatch Time 0.641 (0.658)\tData Time 0.002 (0.007)\tLoss 2.4532 (2.3334)\t\n",
      "[0/155]\tBatch Time 1.072 (1.072)\tLoss 2.6506 (2.6506)\t\n",
      "\n",
      " * LOSS - 2.748\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [161][0/518]\tBatch Time 2.335 (2.335)\tData Time 1.545 (1.545)\tLoss 2.3955 (2.3955)\t\n",
      "Epoch: [161][200/518]\tBatch Time 0.630 (0.664)\tData Time 0.000 (0.010)\tLoss 2.2337 (2.3467)\t\n",
      "Epoch: [161][400/518]\tBatch Time 0.621 (0.658)\tData Time 0.000 (0.006)\tLoss 2.2557 (2.3449)\t\n",
      "[0/155]\tBatch Time 1.063 (1.063)\tLoss 2.9866 (2.9866)\t\n",
      "\n",
      " * LOSS - 2.745\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [162][0/518]\tBatch Time 3.095 (3.095)\tData Time 2.468 (2.468)\tLoss 2.3342 (2.3342)\t\n",
      "Epoch: [162][200/518]\tBatch Time 0.720 (0.665)\tData Time 0.002 (0.015)\tLoss 2.4206 (2.3437)\t\n",
      "Epoch: [162][400/518]\tBatch Time 0.686 (0.656)\tData Time 0.002 (0.009)\tLoss 2.1916 (2.3407)\t\n",
      "[0/155]\tBatch Time 1.032 (1.032)\tLoss 2.8285 (2.8285)\t\n",
      "\n",
      " * LOSS - 2.732\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [163][0/518]\tBatch Time 2.619 (2.619)\tData Time 1.996 (1.996)\tLoss 2.5787 (2.5787)\t\n",
      "Epoch: [163][200/518]\tBatch Time 0.707 (0.663)\tData Time 0.001 (0.013)\tLoss 2.2082 (2.3198)\t\n",
      "Epoch: [163][400/518]\tBatch Time 0.631 (0.657)\tData Time 0.002 (0.007)\tLoss 2.4419 (2.3235)\t\n",
      "[0/155]\tBatch Time 1.011 (1.011)\tLoss 2.6664 (2.6664)\t\n",
      "\n",
      " * LOSS - 2.730\n",
      "\n",
      "Epoch: [164][0/518]\tBatch Time 2.550 (2.550)\tData Time 1.925 (1.925)\tLoss 2.4150 (2.4150)\t\n",
      "Epoch: [164][200/518]\tBatch Time 0.633 (0.660)\tData Time 0.000 (0.012)\tLoss 2.4551 (2.3478)\t\n",
      "Epoch: [164][400/518]\tBatch Time 0.697 (0.656)\tData Time 0.000 (0.007)\tLoss 2.5440 (2.3352)\t\n",
      "[0/155]\tBatch Time 0.989 (0.989)\tLoss 2.5058 (2.5058)\t\n",
      "\n",
      " * LOSS - 2.743\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [165][0/518]\tBatch Time 2.519 (2.519)\tData Time 1.802 (1.802)\tLoss 2.5002 (2.5002)\t\n",
      "Epoch: [165][200/518]\tBatch Time 0.728 (0.662)\tData Time 0.001 (0.011)\tLoss 2.0894 (2.3266)\t\n",
      "Epoch: [165][400/518]\tBatch Time 0.638 (0.657)\tData Time 0.000 (0.007)\tLoss 2.3051 (2.3249)\t\n",
      "[0/155]\tBatch Time 0.997 (0.997)\tLoss 2.8908 (2.8908)\t\n",
      "\n",
      " * LOSS - 2.736\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [166][0/518]\tBatch Time 1.920 (1.920)\tData Time 1.294 (1.294)\tLoss 2.2245 (2.2245)\t\n",
      "Epoch: [166][200/518]\tBatch Time 0.673 (0.663)\tData Time 0.001 (0.009)\tLoss 2.5062 (2.3302)\t\n",
      "Epoch: [166][400/518]\tBatch Time 0.691 (0.658)\tData Time 0.000 (0.006)\tLoss 2.2750 (2.3103)\t\n",
      "[0/155]\tBatch Time 0.924 (0.924)\tLoss 2.4801 (2.4801)\t\n",
      "\n",
      " * LOSS - 2.753\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [167][0/518]\tBatch Time 2.534 (2.534)\tData Time 1.807 (1.807)\tLoss 2.5532 (2.5532)\t\n",
      "Epoch: [167][200/518]\tBatch Time 0.616 (0.663)\tData Time 0.002 (0.014)\tLoss 2.0544 (2.3176)\t\n",
      "Epoch: [167][400/518]\tBatch Time 0.712 (0.659)\tData Time 0.000 (0.008)\tLoss 2.0232 (2.3166)\t\n",
      "[0/155]\tBatch Time 0.988 (0.988)\tLoss 2.8410 (2.8410)\t\n",
      "\n",
      " * LOSS - 2.737\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [168][0/518]\tBatch Time 2.789 (2.789)\tData Time 2.171 (2.171)\tLoss 2.5184 (2.5184)\t\n",
      "Epoch: [168][200/518]\tBatch Time 0.635 (0.668)\tData Time 0.001 (0.014)\tLoss 2.0301 (2.3241)\t\n",
      "Epoch: [168][400/518]\tBatch Time 0.649 (0.662)\tData Time 0.000 (0.009)\tLoss 2.0479 (2.3172)\t\n",
      "[0/155]\tBatch Time 1.090 (1.090)\tLoss 2.5899 (2.5899)\t\n",
      "\n",
      " * LOSS - 2.752\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [169][0/518]\tBatch Time 2.077 (2.077)\tData Time 1.440 (1.440)\tLoss 2.3893 (2.3893)\t\n",
      "Epoch: [169][200/518]\tBatch Time 0.625 (0.662)\tData Time 0.002 (0.010)\tLoss 2.2712 (2.3275)\t\n",
      "Epoch: [169][400/518]\tBatch Time 0.697 (0.656)\tData Time 0.009 (0.006)\tLoss 2.3893 (2.3208)\t\n",
      "[0/155]\tBatch Time 0.872 (0.872)\tLoss 3.1642 (3.1642)\t\n",
      "\n",
      " * LOSS - 2.733\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [170][0/518]\tBatch Time 2.365 (2.365)\tData Time 1.701 (1.701)\tLoss 2.2288 (2.2288)\t\n",
      "Epoch: [170][200/518]\tBatch Time 0.629 (0.661)\tData Time 0.001 (0.011)\tLoss 2.2086 (2.3143)\t\n",
      "Epoch: [170][400/518]\tBatch Time 0.695 (0.657)\tData Time 0.001 (0.007)\tLoss 2.3019 (2.3159)\t\n",
      "[0/155]\tBatch Time 0.869 (0.869)\tLoss 3.1472 (3.1472)\t\n",
      "\n",
      " * LOSS - 2.733\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [171][0/518]\tBatch Time 2.412 (2.412)\tData Time 1.644 (1.644)\tLoss 2.2075 (2.2075)\t\n",
      "Epoch: [171][200/518]\tBatch Time 0.625 (0.660)\tData Time 0.002 (0.010)\tLoss 2.5405 (2.3180)\t\n",
      "Epoch: [171][400/518]\tBatch Time 0.631 (0.656)\tData Time 0.009 (0.006)\tLoss 2.3308 (2.3132)\t\n",
      "[0/155]\tBatch Time 1.203 (1.203)\tLoss 2.4358 (2.4358)\t\n",
      "\n",
      " * LOSS - 2.722\n",
      "\n",
      "Epoch: [172][0/518]\tBatch Time 2.613 (2.613)\tData Time 1.848 (1.848)\tLoss 2.6131 (2.6131)\t\n",
      "Epoch: [172][200/518]\tBatch Time 0.701 (0.661)\tData Time 0.002 (0.011)\tLoss 2.2570 (2.2968)\t\n",
      "Epoch: [172][400/518]\tBatch Time 0.631 (0.657)\tData Time 0.009 (0.007)\tLoss 2.6880 (2.2941)\t\n",
      "[0/155]\tBatch Time 1.043 (1.043)\tLoss 2.4741 (2.4741)\t\n",
      "\n",
      " * LOSS - 2.741\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [173][0/518]\tBatch Time 2.466 (2.466)\tData Time 1.837 (1.837)\tLoss 2.0989 (2.0989)\t\n",
      "Epoch: [173][200/518]\tBatch Time 0.714 (0.666)\tData Time 0.002 (0.012)\tLoss 2.5590 (2.3168)\t\n",
      "Epoch: [173][400/518]\tBatch Time 0.611 (0.659)\tData Time 0.000 (0.007)\tLoss 2.2003 (2.3033)\t\n",
      "[0/155]\tBatch Time 0.987 (0.987)\tLoss 2.4046 (2.4046)\t\n",
      "\n",
      " * LOSS - 2.736\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [174][0/518]\tBatch Time 2.599 (2.599)\tData Time 1.813 (1.813)\tLoss 2.4008 (2.4008)\t\n",
      "Epoch: [174][200/518]\tBatch Time 0.625 (0.666)\tData Time 0.009 (0.012)\tLoss 2.3130 (2.3026)\t\n",
      "Epoch: [174][400/518]\tBatch Time 0.698 (0.659)\tData Time 0.000 (0.007)\tLoss 2.3273 (2.2950)\t\n",
      "[0/155]\tBatch Time 1.142 (1.142)\tLoss 2.9838 (2.9838)\t\n",
      "\n",
      " * LOSS - 2.728\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [175][0/518]\tBatch Time 2.829 (2.829)\tData Time 2.166 (2.166)\tLoss 2.4955 (2.4955)\t\n",
      "Epoch: [175][200/518]\tBatch Time 0.626 (0.661)\tData Time 0.002 (0.014)\tLoss 2.4208 (2.2720)\t\n",
      "Epoch: [175][400/518]\tBatch Time 0.660 (0.657)\tData Time 0.002 (0.008)\tLoss 2.0160 (2.2870)\t\n",
      "[0/155]\tBatch Time 0.862 (0.862)\tLoss 2.5744 (2.5744)\t\n",
      "\n",
      " * LOSS - 2.750\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [176][0/518]\tBatch Time 2.631 (2.631)\tData Time 1.986 (1.986)\tLoss 2.8021 (2.8021)\t\n",
      "Epoch: [176][200/518]\tBatch Time 0.638 (0.661)\tData Time 0.002 (0.013)\tLoss 2.3496 (2.2888)\t\n",
      "Epoch: [176][400/518]\tBatch Time 0.682 (0.658)\tData Time 0.002 (0.007)\tLoss 2.4394 (2.2934)\t\n",
      "[0/155]\tBatch Time 0.904 (0.904)\tLoss 2.7741 (2.7741)\t\n",
      "\n",
      " * LOSS - 2.724\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [177][0/518]\tBatch Time 2.366 (2.366)\tData Time 1.689 (1.689)\tLoss 2.3022 (2.3022)\t\n",
      "Epoch: [177][200/518]\tBatch Time 0.634 (0.661)\tData Time 0.000 (0.012)\tLoss 2.4539 (2.2774)\t\n",
      "Epoch: [177][400/518]\tBatch Time 0.625 (0.658)\tData Time 0.000 (0.007)\tLoss 2.1244 (2.2812)\t\n",
      "[0/155]\tBatch Time 0.911 (0.911)\tLoss 3.1772 (3.1772)\t\n",
      "\n",
      " * LOSS - 2.735\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [178][0/518]\tBatch Time 2.758 (2.758)\tData Time 2.124 (2.124)\tLoss 2.2161 (2.2161)\t\n",
      "Epoch: [178][200/518]\tBatch Time 0.648 (0.662)\tData Time 0.009 (0.013)\tLoss 2.6513 (2.2760)\t\n",
      "Epoch: [178][400/518]\tBatch Time 0.691 (0.656)\tData Time 0.001 (0.008)\tLoss 2.1205 (2.2811)\t\n",
      "[0/155]\tBatch Time 0.766 (0.766)\tLoss 2.4837 (2.4837)\t\n",
      "\n",
      " * LOSS - 2.731\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [179][0/518]\tBatch Time 2.757 (2.757)\tData Time 2.118 (2.118)\tLoss 2.3430 (2.3430)\t\n",
      "Epoch: [179][200/518]\tBatch Time 0.636 (0.663)\tData Time 0.009 (0.013)\tLoss 2.4050 (2.2885)\t\n",
      "Epoch: [179][400/518]\tBatch Time 0.729 (0.658)\tData Time 0.002 (0.008)\tLoss 2.2353 (2.2825)\t\n",
      "[0/155]\tBatch Time 1.024 (1.024)\tLoss 2.7708 (2.7708)\t\n",
      "\n",
      " * LOSS - 2.724\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [180][0/518]\tBatch Time 2.387 (2.387)\tData Time 1.648 (1.648)\tLoss 2.4023 (2.4023)\t\n",
      "Epoch: [180][200/518]\tBatch Time 0.694 (0.663)\tData Time 0.000 (0.011)\tLoss 2.1754 (2.2778)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [180][400/518]\tBatch Time 0.631 (0.659)\tData Time 0.000 (0.007)\tLoss 2.1808 (2.2809)\t\n",
      "[0/155]\tBatch Time 1.007 (1.007)\tLoss 2.7688 (2.7688)\t\n",
      "\n",
      " * LOSS - 2.722\n",
      "\n",
      "Epoch: [181][0/518]\tBatch Time 2.740 (2.740)\tData Time 2.023 (2.023)\tLoss 2.3847 (2.3847)\t\n",
      "Epoch: [181][200/518]\tBatch Time 0.622 (0.664)\tData Time 0.002 (0.013)\tLoss 2.3952 (2.2828)\t\n",
      "Epoch: [181][400/518]\tBatch Time 0.635 (0.659)\tData Time 0.009 (0.008)\tLoss 2.3477 (2.2743)\t\n",
      "[0/155]\tBatch Time 0.933 (0.933)\tLoss 2.9181 (2.9181)\t\n",
      "\n",
      " * LOSS - 2.725\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [182][0/518]\tBatch Time 2.871 (2.871)\tData Time 2.231 (2.231)\tLoss 2.2237 (2.2237)\t\n",
      "Epoch: [182][200/518]\tBatch Time 0.638 (0.666)\tData Time 0.002 (0.014)\tLoss 2.2384 (2.2909)\t\n",
      "Epoch: [182][400/518]\tBatch Time 0.701 (0.658)\tData Time 0.000 (0.008)\tLoss 2.3986 (2.2879)\t\n",
      "[0/155]\tBatch Time 0.838 (0.838)\tLoss 3.8186 (3.8186)\t\n",
      "\n",
      " * LOSS - 2.728\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [183][0/518]\tBatch Time 2.560 (2.560)\tData Time 1.940 (1.940)\tLoss 2.2347 (2.2347)\t\n",
      "Epoch: [183][200/518]\tBatch Time 0.677 (0.663)\tData Time 0.002 (0.014)\tLoss 2.3479 (2.2685)\t\n",
      "Epoch: [183][400/518]\tBatch Time 0.649 (0.658)\tData Time 0.001 (0.008)\tLoss 2.2629 (2.2884)\t\n",
      "[0/155]\tBatch Time 1.095 (1.095)\tLoss 2.5762 (2.5762)\t\n",
      "\n",
      " * LOSS - 2.719\n",
      "\n",
      "Epoch: [184][0/518]\tBatch Time 2.450 (2.450)\tData Time 1.732 (1.732)\tLoss 2.2302 (2.2302)\t\n",
      "Epoch: [184][200/518]\tBatch Time 0.635 (0.664)\tData Time 0.000 (0.011)\tLoss 2.7077 (2.2963)\t\n",
      "Epoch: [184][400/518]\tBatch Time 0.706 (0.658)\tData Time 0.001 (0.006)\tLoss 2.1928 (2.2843)\t\n",
      "[0/155]\tBatch Time 1.037 (1.037)\tLoss 3.1847 (3.1847)\t\n",
      "\n",
      " * LOSS - 2.731\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [185][0/518]\tBatch Time 2.665 (2.665)\tData Time 1.957 (1.957)\tLoss 2.7156 (2.7156)\t\n",
      "Epoch: [185][200/518]\tBatch Time 0.642 (0.668)\tData Time 0.014 (0.013)\tLoss 2.3594 (2.2610)\t\n",
      "Epoch: [185][400/518]\tBatch Time 0.630 (0.661)\tData Time 0.001 (0.008)\tLoss 2.1355 (2.2770)\t\n",
      "[0/155]\tBatch Time 1.025 (1.025)\tLoss 3.0021 (3.0021)\t\n",
      "\n",
      " * LOSS - 2.718\n",
      "\n",
      "Epoch: [186][0/518]\tBatch Time 2.646 (2.646)\tData Time 2.022 (2.022)\tLoss 2.0047 (2.0047)\t\n",
      "Epoch: [186][200/518]\tBatch Time 0.688 (0.664)\tData Time 0.002 (0.013)\tLoss 2.3941 (2.2505)\t\n",
      "Epoch: [186][400/518]\tBatch Time 0.644 (0.659)\tData Time 0.002 (0.008)\tLoss 2.1286 (2.2844)\t\n",
      "[0/155]\tBatch Time 1.147 (1.147)\tLoss 2.7898 (2.7898)\t\n",
      "\n",
      " * LOSS - 2.721\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [187][0/518]\tBatch Time 2.538 (2.538)\tData Time 1.790 (1.790)\tLoss 2.1398 (2.1398)\t\n",
      "Epoch: [187][200/518]\tBatch Time 0.696 (0.659)\tData Time 0.002 (0.012)\tLoss 2.2255 (2.2703)\t\n",
      "Epoch: [187][400/518]\tBatch Time 0.646 (0.656)\tData Time 0.009 (0.007)\tLoss 2.3682 (2.2777)\t\n",
      "[0/155]\tBatch Time 0.818 (0.818)\tLoss 3.1546 (3.1546)\t\n",
      "\n",
      " * LOSS - 2.726\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [188][0/518]\tBatch Time 2.256 (2.256)\tData Time 1.550 (1.550)\tLoss 2.2744 (2.2744)\t\n",
      "Epoch: [188][200/518]\tBatch Time 0.649 (0.660)\tData Time 0.000 (0.011)\tLoss 2.4033 (2.2848)\t\n",
      "Epoch: [188][400/518]\tBatch Time 0.756 (0.657)\tData Time 0.009 (0.007)\tLoss 2.1419 (2.2892)\t\n",
      "[0/155]\tBatch Time 0.955 (0.955)\tLoss 2.9062 (2.9062)\t\n",
      "\n",
      " * LOSS - 2.722\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [189][0/518]\tBatch Time 2.244 (2.244)\tData Time 1.590 (1.590)\tLoss 2.4142 (2.4142)\t\n",
      "Epoch: [189][200/518]\tBatch Time 0.643 (0.658)\tData Time 0.000 (0.011)\tLoss 2.2358 (2.2828)\t\n",
      "Epoch: [189][400/518]\tBatch Time 0.702 (0.655)\tData Time 0.001 (0.007)\tLoss 2.0664 (2.2682)\t\n",
      "[0/155]\tBatch Time 1.095 (1.095)\tLoss 2.7795 (2.7795)\t\n",
      "\n",
      " * LOSS - 2.726\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [190][0/518]\tBatch Time 2.028 (2.028)\tData Time 1.403 (1.403)\tLoss 2.4776 (2.4776)\t\n",
      "Epoch: [190][200/518]\tBatch Time 0.768 (0.660)\tData Time 0.009 (0.010)\tLoss 2.0526 (2.2867)\t\n",
      "Epoch: [190][400/518]\tBatch Time 0.635 (0.655)\tData Time 0.002 (0.006)\tLoss 1.9291 (2.2893)\t\n",
      "[0/155]\tBatch Time 1.054 (1.054)\tLoss 2.5779 (2.5779)\t\n",
      "\n",
      " * LOSS - 2.727\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [191][0/518]\tBatch Time 2.109 (2.109)\tData Time 1.444 (1.444)\tLoss 2.1228 (2.1228)\t\n",
      "Epoch: [191][200/518]\tBatch Time 0.636 (0.662)\tData Time 0.001 (0.011)\tLoss 2.0581 (2.2991)\t\n",
      "Epoch: [191][400/518]\tBatch Time 0.631 (0.658)\tData Time 0.001 (0.007)\tLoss 2.4459 (2.2776)\t\n",
      "[0/155]\tBatch Time 1.231 (1.231)\tLoss 2.7939 (2.7939)\t\n",
      "\n",
      " * LOSS - 2.723\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [192][0/518]\tBatch Time 2.358 (2.358)\tData Time 1.715 (1.715)\tLoss 2.4448 (2.4448)\t\n",
      "Epoch: [192][200/518]\tBatch Time 0.631 (0.665)\tData Time 0.000 (0.015)\tLoss 2.0045 (2.2923)\t\n",
      "Epoch: [192][400/518]\tBatch Time 0.657 (0.658)\tData Time 0.002 (0.008)\tLoss 1.9744 (2.2941)\t\n",
      "[0/155]\tBatch Time 0.985 (0.985)\tLoss 2.6679 (2.6679)\t\n",
      "\n",
      " * LOSS - 2.724\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [193][0/518]\tBatch Time 2.091 (2.091)\tData Time 1.395 (1.395)\tLoss 2.2552 (2.2552)\t\n",
      "Epoch: [193][200/518]\tBatch Time 0.700 (0.660)\tData Time 0.000 (0.010)\tLoss 2.6095 (2.2999)\t\n",
      "Epoch: [193][400/518]\tBatch Time 0.645 (0.657)\tData Time 0.000 (0.006)\tLoss 2.3600 (2.2768)\t\n",
      "[0/155]\tBatch Time 1.029 (1.029)\tLoss 3.0672 (3.0672)\t\n",
      "\n",
      " * LOSS - 2.716\n",
      "\n",
      "Epoch: [194][0/518]\tBatch Time 3.193 (3.193)\tData Time 2.560 (2.560)\tLoss 2.2312 (2.2312)\t\n",
      "Epoch: [194][200/518]\tBatch Time 0.632 (0.662)\tData Time 0.002 (0.016)\tLoss 2.3025 (2.2737)\t\n",
      "Epoch: [194][400/518]\tBatch Time 0.607 (0.656)\tData Time 0.001 (0.009)\tLoss 2.4595 (2.2793)\t\n",
      "[0/155]\tBatch Time 0.935 (0.935)\tLoss 2.9715 (2.9715)\t\n",
      "\n",
      " * LOSS - 2.723\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [195][0/518]\tBatch Time 2.652 (2.652)\tData Time 1.963 (1.963)\tLoss 2.2563 (2.2563)\t\n",
      "Epoch: [195][200/518]\tBatch Time 0.646 (0.660)\tData Time 0.002 (0.013)\tLoss 2.2501 (2.2849)\t\n",
      "Epoch: [195][400/518]\tBatch Time 0.624 (0.658)\tData Time 0.001 (0.008)\tLoss 2.4615 (2.2697)\t\n",
      "[0/155]\tBatch Time 0.919 (0.919)\tLoss 2.9326 (2.9326)\t\n",
      "\n",
      " * LOSS - 2.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [196][0/518]\tBatch Time 2.838 (2.838)\tData Time 2.049 (2.049)\tLoss 2.2779 (2.2779)\t\n",
      "Epoch: [196][200/518]\tBatch Time 0.688 (0.684)\tData Time 0.006 (0.013)\tLoss 2.2694 (2.2731)\t\n",
      "Epoch: [196][400/518]\tBatch Time 0.631 (0.672)\tData Time 0.001 (0.008)\tLoss 2.1439 (2.2798)\t\n",
      "[0/155]\tBatch Time 0.999 (0.999)\tLoss 2.5436 (2.5436)\t\n",
      "\n",
      " * LOSS - 2.722\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [197][0/518]\tBatch Time 2.574 (2.574)\tData Time 1.870 (1.870)\tLoss 2.4376 (2.4376)\t\n",
      "Epoch: [197][200/518]\tBatch Time 0.645 (0.657)\tData Time 0.000 (0.013)\tLoss 2.5604 (2.2796)\t\n",
      "Epoch: [197][400/518]\tBatch Time 0.619 (0.655)\tData Time 0.002 (0.008)\tLoss 1.8148 (2.2751)\t\n",
      "[0/155]\tBatch Time 0.863 (0.863)\tLoss 2.9081 (2.9081)\t\n",
      "\n",
      " * LOSS - 2.720\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [198][0/518]\tBatch Time 2.844 (2.844)\tData Time 2.202 (2.202)\tLoss 2.1370 (2.1370)\t\n",
      "Epoch: [198][200/518]\tBatch Time 0.604 (0.667)\tData Time 0.001 (0.014)\tLoss 2.3968 (2.2958)\t\n",
      "Epoch: [198][400/518]\tBatch Time 0.681 (0.662)\tData Time 0.001 (0.008)\tLoss 2.2868 (2.2777)\t\n",
      "[0/155]\tBatch Time 1.106 (1.106)\tLoss 2.7688 (2.7688)\t\n",
      "\n",
      " * LOSS - 2.718\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [199][0/518]\tBatch Time 2.503 (2.503)\tData Time 1.884 (1.884)\tLoss 2.2811 (2.2811)\t\n",
      "Epoch: [199][200/518]\tBatch Time 0.644 (0.667)\tData Time 0.008 (0.012)\tLoss 2.3180 (2.2782)\t\n",
      "Epoch: [199][400/518]\tBatch Time 0.803 (0.663)\tData Time 0.002 (0.007)\tLoss 2.6031 (2.2728)\t\n",
      "[0/155]\tBatch Time 0.988 (0.988)\tLoss 2.4867 (2.4867)\t\n",
      "\n",
      " * LOSS - 2.720\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training SSD300 with Kaiming_uniform_ and learning rate annealing\n",
    "global epochs_since_improvement, start_epoch, label_map, best_loss, epoch, checkpoint\n",
    "\n",
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "n_classes = len(label_map)\n",
    "\n",
    "# Training parameters\n",
    "checkpoint = None  # path to model checkpoint if consider resume training from there\n",
    "batch_size = 32\n",
    "start_epoch = 0    # start at this epoch\n",
    "epochs = 200       # total training epochs to run without early-stopping\n",
    "epochs_since_improvement = 0 # record the no. of epochs since last improvement\n",
    "best_loss = 100.   # assume a hight loss at first\n",
    "workers = 4        # number of workers for loading data in the DataLoader\n",
    "print_freq = 200   # print training or validation status every __ batches\n",
    "lr = 1e-3/2          # learning rate\n",
    "momentum = 0.9     \n",
    "weight_decay = 5e-4\n",
    "grad_clip = None   # consider clipping the gradient when using high learning_rate\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# initialize model or load checkpoint\n",
    "if checkpoint is None:\n",
    "    model = SSD300(n_classes)\n",
    "    # Initialize the optimizer, with twice the default learning rate for biases\n",
    "    biases = list()\n",
    "    not_biases = list()\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param_name.endswith('.bias'):\n",
    "                biases.append(param)\n",
    "            else:\n",
    "                not_biases.append(param)\n",
    "    optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    \n",
    "# move to  default device\n",
    "model = model.to(device)      # model to GPU\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device) # Loss function to GPU\n",
    "\n",
    "# Custom dataloaders\n",
    "train_dataset = PascalVOCDataset(data_folder,'train',keep_difficult)\n",
    "val_dataset   = PascalVOCDataset(data_folder,'test' ,keep_difficult)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                            collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True) # pass in our collate function here\n",
    "val_loader    = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                             collate_fn=val_dataset.collate_fn, num_workers=workers,\n",
    "                                             pin_memory=True)\n",
    "# used to schedule learning rate later\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "learning_rates = list()\n",
    "total_iters = epochs*n_batches\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "            \n",
    "    train(train_loader=train_loader,\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          epoch=epoch,\n",
    "          total_iters=total_iters\n",
    "          )\n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss = validate(val_loader=val_loader,\n",
    "                        model=model,\n",
    "                        criterion=criterion)\n",
    "\n",
    "    # Did validation loss improve?\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, epochs_since_improvement, model, optimizer, val_loss, best_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.000000000000187e-05,\n",
       " 1.0000002524850225e-05,\n",
       " 1.0000010108144347e-05,\n",
       " 1.0000022749884181e-05,\n",
       " 1.000004045006962e-05,\n",
       " 1.000006320870044e-05,\n",
       " 1.0000091025776478e-05,\n",
       " 1.0000123901297348e-05,\n",
       " 1.0000161739844803e-05,\n",
       " 1.0000204720324108e-05,\n",
       " 1.0000252878525715e-05,\n",
       " 1.0000305987822292e-05,\n",
       " 1.0000364155561556e-05,\n",
       " 1.0000427226673505e-05,\n",
       " 1.0000495499365984e-05,\n",
       " 1.0000568830499284e-05,\n",
       " 1.0000647220072527e-05,\n",
       " 1.0000730870875586e-05,\n",
       " 1.0000819389256701e-05,\n",
       " 1.0000912739424158e-05,\n",
       " 1.0001011362748924e-05,\n",
       " 1.0001115044509182e-05,\n",
       " 1.0001223784703831e-05,\n",
       " 1.000133785770403e-05,\n",
       " 1.000145644039196e-05,\n",
       " 1.0001580355883023e-05,\n",
       " 1.0001709329803751e-05,\n",
       " 1.0001843362152881e-05,\n",
       " 1.0001982452928928e-05,\n",
       " 1.0002126602130574e-05,\n",
       " 1.0002275809756278e-05,\n",
       " 1.0002430075804558e-05,\n",
       " 1.0002589400273821e-05,\n",
       " 1.0002753783162418e-05,\n",
       " 1.000292281889197e-05,\n",
       " 1.000309772419096e-05,\n",
       " 1.000327728232739e-05,\n",
       " 1.0003461898876117e-05,\n",
       " 1.0003651573835277e-05,\n",
       " 1.0003845841975062e-05,\n",
       " 1.000404562181907e-05,\n",
       " 1.0004250460067517e-05,\n",
       " 1.0004460857736814e-05,\n",
       " 1.0004675824718065e-05,\n",
       " 1.0004895850097381e-05,\n",
       " 1.0005120397063453e-05,\n",
       " 1.0005350527301842e-05,\n",
       " 1.0005585715931429e-05,\n",
       " 1.0005825962949684e-05,\n",
       " 1.0006071852883883e-05,\n",
       " 1.000632163214258e-05,\n",
       " 1.0006577054312165e-05,\n",
       " 1.0006837534860305e-05,\n",
       " 1.000710307378442e-05,\n",
       " 1.0007373671081759e-05,\n",
       " 1.000764932674952e-05,\n",
       " 1.0007930040784956e-05,\n",
       " 1.000821581318521e-05,\n",
       " 1.0008506643947314e-05,\n",
       " 1.0008802533068298e-05,\n",
       " 1.0009102764812354e-05,\n",
       " 1.0009409486374767e-05,\n",
       " 1.0009720550554043e-05,\n",
       " 1.0010036673079835e-05,\n",
       " 1.0010357853948845e-05,\n",
       " 1.001068331777477e-05,\n",
       " 1.001101460339036e-05,\n",
       " 1.0011350947339223e-05,\n",
       " 1.0011693160791023e-05,\n",
       " 1.0012039633326065e-05,\n",
       " 1.0012391164183884e-05,\n",
       " 1.001274690639771e-05,\n",
       " 1.0013108541960287e-05,\n",
       " 1.0013475235834701e-05,\n",
       " 1.0013847870770315e-05,\n",
       " 1.0014224693187109e-05,\n",
       " 1.0014605667291393e-05,\n",
       " 1.0014992594375246e-05,\n",
       " 1.0015384579751759e-05,\n",
       " 1.001578162341681e-05,\n",
       " 1.0016183725366442e-05,\n",
       " 1.0016590885596532e-05,\n",
       " 1.0017003104102852e-05,\n",
       " 1.0017420380881278e-05,\n",
       " 1.0017842715927467e-05,\n",
       " 1.0018270109237136e-05,\n",
       " 1.001870256080594e-05,\n",
       " 1.0019140070629376e-05,\n",
       " 1.0019582638703099e-05,\n",
       " 1.0020030265022495e-05,\n",
       " 1.0020482949583058e-05,\n",
       " 1.00209396068472e-05,\n",
       " 1.0021402395946277e-05,\n",
       " 1.0021871352664916e-05,\n",
       " 1.0022344270143153e-05,\n",
       " 1.0022822245838926e-05,\n",
       " 1.0023305279747288e-05,\n",
       " 1.0023792214751841e-05,\n",
       " 1.0024285353140794e-05,\n",
       " 1.002478354972739e-05,\n",
       " 1.0025287997407098e-05,\n",
       " 1.0025796322303333e-05,\n",
       " 1.0026308488621909e-05,\n",
       " 1.0026826917947628e-05,\n",
       " 1.0027350405445048e-05,\n",
       " 1.0027878951108784e-05,\n",
       " 1.002841255493345e-05,\n",
       " 1.0028951216913606e-05,\n",
       " 1.00294949370437e-05,\n",
       " 1.0030043715318184e-05,\n",
       " 1.0030597551731504e-05,\n",
       " 1.0031156446277892e-05,\n",
       " 1.0031720398951797e-05,\n",
       " 1.0032289409747339e-05,\n",
       " 1.003286347865869e-05,\n",
       " 1.003344260568003e-05,\n",
       " 1.0034026790805472e-05,\n",
       " 1.0034614638353742e-05,\n",
       " 1.0035210335344607e-05,\n",
       " 1.0035809694746206e-05,\n",
       " 1.0036414112227675e-05,\n",
       " 1.003702358778286e-05,\n",
       " 1.0037638121405494e-05,\n",
       " 1.0038256245837574e-05,\n",
       " 1.0038880883646895e-05,\n",
       " 1.0039512070615172e-05,\n",
       " 1.0040146836444407e-05,\n",
       " 1.0040786660309161e-05,\n",
       " 1.0041430015304509e-05,\n",
       " 1.0042079943291432e-05,\n",
       " 1.0042734929294149e-05,\n",
       " 1.0043394973305949e-05,\n",
       " 1.004406164993564e-05,\n",
       " 1.0044730235329884e-05,\n",
       " 1.0045405453328333e-05,\n",
       " 1.004608572930861e-05,\n",
       " 1.0046771063263736e-05,\n",
       " 1.004746145518673e-05,\n",
       " 1.0048156905070562e-05,\n",
       " 1.0048857412908084e-05,\n",
       " 1.0049562978692097e-05,\n",
       " 1.0050273602415513e-05,\n",
       " 1.005098928407091e-05,\n",
       " 1.0051708317843155e-05,\n",
       " 1.0052435821148697e-05,\n",
       " 1.00531666765563e-05,\n",
       " 1.005390258986632e-05,\n",
       " 1.0054643561071393e-05,\n",
       " 1.0055389590163826e-05,\n",
       " 1.0056138899754117e-05,\n",
       " 1.0056895032669537e-05,\n",
       " 1.0057658024689163e-05,\n",
       " 1.0058424285254531e-05,\n",
       " 1.0059195603668625e-05,\n",
       " 1.005997014289722e-05,\n",
       " 1.0060751565056373e-05,\n",
       " 1.0061538045040458e-05,\n",
       " 1.0062329582841556e-05,\n",
       " 1.0063128063193208e-05,\n",
       " 1.0063927831862154e-05,\n",
       " 1.0064734543065388e-05,\n",
       " 1.0065546312052884e-05,\n",
       " 1.0066363138816341e-05,\n",
       " 1.0067185023347518e-05,\n",
       " 1.0068011965637896e-05,\n",
       " 1.006884396567907e-05,\n",
       " 1.0069681023462571e-05,\n",
       " 1.0070523138979776e-05,\n",
       " 1.0071370312222163e-05,\n",
       " 1.0072222543180996e-05,\n",
       " 1.0073079831847592e-05,\n",
       " 1.0073942178213213e-05,\n",
       " 1.007480958226901e-05,\n",
       " 1.0075682044006135e-05,\n",
       " 1.0076557487844332e-05,\n",
       " 1.0077440052988869e-05,\n",
       " 1.0078329775216182e-05,\n",
       " 1.0079222467588989e-05,\n",
       " 1.0080120217598007e-05,\n",
       " 1.0081023025234111e-05,\n",
       " 1.0081928743346312e-05,\n",
       " 1.0082841654280519e-05,\n",
       " 1.008375962281395e-05,\n",
       " 1.008468483186387e-05,\n",
       " 1.0085612927495876e-05,\n",
       " 1.0086543873915728e-05,\n",
       " 1.0087482072751863e-05,\n",
       " 1.0088425329139797e-05,\n",
       " 1.0089373643069963e-05,\n",
       " 1.0090327014532584e-05,\n",
       " 1.0091285443517984e-05,\n",
       " 1.0092248930016383e-05,\n",
       " 1.0093217474017835e-05,\n",
       " 1.0094191075512555e-05,\n",
       " 1.0095169734490542e-05,\n",
       " 1.0096153450941794e-05,\n",
       " 1.0097142224856255e-05,\n",
       " 1.0098136056223866e-05,\n",
       " 1.0099134945034406e-05,\n",
       " 1.010013889127771e-05,\n",
       " 1.0101145509267963e-05,\n",
       " 1.0102159558418082e-05,\n",
       " 1.010318107450121e-05,\n",
       " 1.0104205250372412e-05,\n",
       " 1.0105234483624497e-05,\n",
       " 1.0106268774247024e-05,\n",
       " 1.0107305664987184e-05,\n",
       " 1.0108350058391119e-05,\n",
       " 1.0109401990231198e-05,\n",
       " 1.0110456510229156e-05,\n",
       " 1.0111516087544248e-05,\n",
       " 1.0112578205285042e-05,\n",
       " 1.0113647885274102e-05,\n",
       " 1.0114722622547598e-05,\n",
       " 1.011580241709454e-05,\n",
       " 1.0116889833494465e-05,\n",
       " 1.0117977177964628e-05,\n",
       " 1.0119072144265518e-05,\n",
       " 1.0120172167795448e-05,\n",
       " 1.0121277248543153e-05,\n",
       " 1.0122387386497311e-05,\n",
       " 1.0123502581646603e-05,\n",
       " 1.0124622833979595e-05,\n",
       " 1.0125748143484913e-05,\n",
       " 1.0126878510151018e-05,\n",
       " 1.0128013933966365e-05,\n",
       " 1.012915171915624e-05,\n",
       " 1.0130299952998303e-05,\n",
       " 1.0131450548191535e-05,\n",
       " 1.0132606200487244e-05,\n",
       " 1.013376690987367e-05,\n",
       " 1.0134932676338994e-05,\n",
       " 1.0136100732545463e-05,\n",
       " 1.0137276601205649e-05,\n",
       " 1.0138460318088461e-05,\n",
       " 1.0139646312749477e-05,\n",
       " 1.014083736442927e-05,\n",
       " 1.0142030646155195e-05,\n",
       " 1.0143231799909056e-05,\n",
       " 1.0144438010645039e-05,\n",
       " 1.014564927835078e-05,\n",
       " 1.0146868477681698e-05,\n",
       " 1.0148086984622186e-05,\n",
       " 1.0149313423162846e-05,\n",
       " 1.0150544918623529e-05,\n",
       " 1.0151781470991538e-05,\n",
       " 1.0153023080254346e-05,\n",
       " 1.0154269746399203e-05,\n",
       " 1.0155521469413412e-05,\n",
       " 1.0156778249284116e-05,\n",
       " 1.0158040085998507e-05,\n",
       " 1.0159306979543673e-05,\n",
       " 1.0160578929906699e-05,\n",
       " 1.0161855937074613e-05,\n",
       " 1.0163138001034285e-05,\n",
       " 1.0164425121772686e-05,\n",
       " 1.0165717299276574e-05,\n",
       " 1.0167014533532868e-05,\n",
       " 1.0168313747138152e-05,\n",
       " 1.0169624172249316e-05,\n",
       " 1.017093657668287e-05,\n",
       " 1.0172254037815412e-05,\n",
       " 1.017357655563348e-05,\n",
       " 1.0174900993102727e-05,\n",
       " 1.0176233612325225e-05,\n",
       " 1.0177571288192579e-05,\n",
       " 1.017891719349013e-05,\n",
       " 1.0180264994532159e-05,\n",
       " 1.0181614655526949e-05,\n",
       " 1.018297255783662e-05,\n",
       " 1.0184335516722232e-05,\n",
       " 1.0185703532169996e-05,\n",
       " 1.0187076604165783e-05,\n",
       " 1.0188454732695692e-05,\n",
       " 1.0189837917745544e-05,\n",
       " 1.0191226159301269e-05,\n",
       " 1.019261945734869e-05,\n",
       " 1.0194017811873464e-05,\n",
       " 1.019542122286141e-05,\n",
       " 1.0196829690298186e-05,\n",
       " 1.0198243214169283e-05,\n",
       " 1.0199661794460412e-05,\n",
       " 1.0201085431156957e-05,\n",
       " 1.0202514124244408e-05,\n",
       " 1.0203944486274957e-05,\n",
       " 1.0205386679533553e-05,\n",
       " 1.0206830541705956e-05,\n",
       " 1.0208279460210518e-05,\n",
       " 1.0209733435032508e-05,\n",
       " 1.0211189019097313e-05,\n",
       " 1.0212653094584243e-05,\n",
       " 1.021412222634387e-05,\n",
       " 1.021559989719636e-05,\n",
       " 1.0217079153381389e-05,\n",
       " 1.0218559959108617e-05,\n",
       " 1.022004931580839e-05,\n",
       " 1.0221543728705295e-05,\n",
       " 1.0223043197784053e-05,\n",
       " 1.0224551277414408e-05,\n",
       " 1.0226057304425743e-05,\n",
       " 1.0227571941957843e-05,\n",
       " 1.0229091635610192e-05,\n",
       " 1.023061638536729e-05,\n",
       " 1.0232146191213531e-05,\n",
       " 1.0233681053133253e-05,\n",
       " 1.0235220971110736e-05,\n",
       " 1.0236765945130375e-05,\n",
       " 1.023831597517623e-05,\n",
       " 1.0239871061232587e-05,\n",
       " 1.0241427517754963e-05,\n",
       " 1.024299640131294e-05,\n",
       " 1.0244566655305065e-05,\n",
       " 1.0246141965243775e-05,\n",
       " 1.0247722331112915e-05,\n",
       " 1.0249307752896436e-05,\n",
       " 1.0250894473503361e-05,\n",
       " 1.0252489995142595e-05,\n",
       " 1.0254094353570513e-05,\n",
       " 1.0255699998848783e-05,\n",
       " 1.0257310699959826e-05,\n",
       " 1.0258922640191734e-05,\n",
       " 1.026054344099494e-05,\n",
       " 1.0262169297581402e-05,\n",
       " 1.026380020993458e-05,\n",
       " 1.0265440042429093e-05,\n",
       " 1.0267077201874254e-05,\n",
       " 1.0268723281427283e-05,\n",
       " 1.027037441667998e-05,\n",
       " 1.0272030607615586e-05,\n",
       " 1.0273691854217062e-05,\n",
       " 1.0275358156467482e-05,\n",
       " 1.0277029514349865e-05,\n",
       " 1.0278705927847011e-05,\n",
       " 1.0280387396941882e-05,\n",
       " 1.0282073921617277e-05,\n",
       " 1.0283761506332194e-05,\n",
       " 1.028546213764056e-05,\n",
       " 1.028716382895388e-05,\n",
       " 1.0288870575778481e-05,\n",
       " 1.0290582378096885e-05,\n",
       " 1.0292299235891615e-05,\n",
       " 1.0294017082081438e-05,\n",
       " 1.0295748117839883e-05,\n",
       " 1.0297480141958136e-05,\n",
       " 1.0299217221482263e-05,\n",
       " 1.0300959356394459e-05,\n",
       " 1.0302702419997724e-05,\n",
       " 1.030445465370965e-05,\n",
       " 1.0306211942756173e-05,\n",
       " 1.0307978449567367e-05,\n",
       " 1.0309745861151938e-05,\n",
       " 1.0311514141723336e-05,\n",
       " 1.0313291651927987e-05,\n",
       " 1.0315074217376995e-05,\n",
       " 1.0316861838052007e-05,\n",
       " 1.0318654513934777e-05,\n",
       " 1.0320452245007057e-05,\n",
       " 1.0322255031250384e-05,\n",
       " 1.0324062872646405e-05,\n",
       " 1.032587576917665e-05,\n",
       " 1.0327693720822492e-05,\n",
       " 1.0329516727565464e-05,\n",
       " 1.0331344789386883e-05,\n",
       " 1.0333177906268116e-05,\n",
       " 1.0335016078190317e-05,\n",
       " 1.0336859305134851e-05,\n",
       " 1.0338707587082763e-05,\n",
       " 1.0340556546990584e-05,\n",
       " 1.0342419315913314e-05,\n",
       " 1.034428276275798e-05,\n",
       " 1.0346151264530236e-05,\n",
       " 1.0348024821210959e-05,\n",
       " 1.0349898996146415e-05,\n",
       " 1.0351782650664754e-05,\n",
       " 1.0353671360033966e-05,\n",
       " 1.0355569596634761e-05,\n",
       " 1.0357468427569575e-05,\n",
       " 1.0359367817053945e-05,\n",
       " 1.0361276745633367e-05,\n",
       " 1.0363190728966667e-05,\n",
       " 1.0365109767034277e-05,\n",
       " 1.0367033859816688e-05,\n",
       " 1.0368963007294116e-05,\n",
       " 1.0370897209446944e-05,\n",
       " 1.037283646625533e-05,\n",
       " 1.0374780777699492e-05,\n",
       " 1.0376730143759533e-05,\n",
       " 1.0378684564415563e-05,\n",
       " 1.0380644039647629e-05,\n",
       " 1.0382608569435676e-05,\n",
       " 1.0384578153759586e-05,\n",
       " 1.0386552792599302e-05,\n",
       " 1.0388532485934655e-05,\n",
       " 1.0390517233745367e-05,\n",
       " 1.0392507036011159e-05,\n",
       " 1.0394501892711696e-05,\n",
       " 1.0396501803826644e-05,\n",
       " 1.039850676933545e-05,\n",
       " 1.0400512042658588e-05,\n",
       " 1.0402527104973155e-05,\n",
       " 1.0404551992020476e-05,\n",
       " 1.0406577174899676e-05,\n",
       " 1.0408607412069802e-05,\n",
       " 1.0410637897348367e-05,\n",
       " 1.041267823111784e-05,\n",
       " 1.0414723619115863e-05,\n",
       " 1.0416774061321664e-05,\n",
       " 1.0418834411557196e-05,\n",
       " 1.0420890108272513e-05,\n",
       " 1.0422955712975521e-05,\n",
       " 1.0425026371802114e-05,\n",
       " 1.0427102084731189e-05,\n",
       " 1.0429182851741477e-05,\n",
       " 1.043126867281171e-05,\n",
       " 1.0433359547920621e-05,\n",
       " 1.0435455477046776e-05,\n",
       " 1.04375564601688e-05,\n",
       " 1.0439662497265256e-05,\n",
       " 1.0441773588314548e-05,\n",
       " 1.0443889733295135e-05,\n",
       " 1.0446010932185417e-05,\n",
       " 1.0448137184963686e-05,\n",
       " 1.0450268491608183e-05,\n",
       " 1.0452404852097196e-05,\n",
       " 1.0454541209957828e-05,\n",
       " 1.0456687666150952e-05,\n",
       " 1.0458844256412619e-05,\n",
       " 1.046100083206076e-05,\n",
       " 1.046316246144367e-05,\n",
       " 1.0465324028492252e-05,\n",
       " 1.0467495753359396e-05,\n",
       " 1.0469672531895035e-05,\n",
       " 1.0471859515881007e-05,\n",
       " 1.0474046413605672e-05,\n",
       " 1.04762331892899e-05,\n",
       " 1.0478430182276335e-05,\n",
       " 1.048063222881943e-05,\n",
       " 1.048283932889676e-05,\n",
       " 1.048505148248574e-05,\n",
       " 1.0487268689563782e-05,\n",
       " 1.0489490950108134e-05,\n",
       " 1.049171826409621e-05,\n",
       " 1.0493950631505148e-05,\n",
       " 1.049618805231225e-05,\n",
       " 1.0498430526494546e-05,\n",
       " 1.0500678054029119e-05,\n",
       " 1.0502930634893105e-05,\n",
       " 1.0505188269063371e-05,\n",
       " 1.0507450956516944e-05,\n",
       " 1.0509718697230576e-05,\n",
       " 1.0511986124873224e-05,\n",
       " 1.0514269338345587e-05,\n",
       " 1.0516552238700417e-05,\n",
       " 1.0518840192222378e-05,\n",
       " 1.052113319888811e-05,\n",
       " 1.052342583277697e-05,\n",
       " 1.0525728933742195e-05,\n",
       " 1.052803708778074e-05,\n",
       " 1.053035575651896e-05,\n",
       " 1.0532674028550852e-05,\n",
       " 1.053499186810031e-05,\n",
       " 1.0537320234195878e-05,\n",
       " 1.0539653653246334e-05,\n",
       " 1.0541992125227885e-05,\n",
       " 1.0544335650116514e-05,\n",
       " 1.0546684227888424e-05,\n",
       " 1.054903785851949e-05,\n",
       " 1.0551396541985696e-05,\n",
       " 1.0553760278263026e-05,\n",
       " 1.0556129067327193e-05,\n",
       " 1.0558502909154065e-05,\n",
       " 1.0560881803719356e-05,\n",
       " 1.0563265750998774e-05,\n",
       " 1.0565654750967973e-05,\n",
       " 1.05680488036025e-05,\n",
       " 1.0570447908877896e-05,\n",
       " 1.057284639064174e-05,\n",
       " 1.0575261277253203e-05,\n",
       " 1.0577675540303984e-05,\n",
       " 1.058009485589721e-05,\n",
       " 1.058251922400826e-05,\n",
       " 1.0584942908902637e-05,\n",
       " 1.0587377370058696e-05,\n",
       " 1.0589822643200008e-05,\n",
       " 1.0592267221133897e-05,\n",
       " 1.0594716851461136e-05,\n",
       " 1.0597165738866911e-05,\n",
       " 1.0599625461989934e-05,\n",
       " 1.0602090237431125e-05,\n",
       " 1.0604560065165371e-05,\n",
       " 1.0607040788120406e-05,\n",
       " 1.0609514877411849e-05,\n",
       " 1.0611999861873468e-05,\n",
       " 1.0614489898526856e-05,\n",
       " 1.0616984987346516e-05,\n",
       " 1.0619485128306946e-05,\n",
       " 1.0621990321382647e-05,\n",
       " 1.0624500566548009e-05,\n",
       " 1.0627015863777314e-05,\n",
       " 1.062953621304495e-05,\n",
       " 1.0632061614325146e-05,\n",
       " 1.0634592067592016e-05,\n",
       " 1.0637127572819785e-05,\n",
       " 1.0639668129982515e-05,\n",
       " 1.064221373905421e-05,\n",
       " 1.0644764400008933e-05,\n",
       " 1.0647320112820523e-05,\n",
       " 1.0649874831980423e-05,\n",
       " 1.0652440636512913e-05,\n",
       " 1.065501756213546e-05,\n",
       " 1.0657593482113097e-05,\n",
       " 1.0660174453816476e-05,\n",
       " 1.0662754372164417e-05,\n",
       " 1.0665345435326004e-05,\n",
       " 1.0667941550134198e-05,\n",
       " 1.0670542716562454e-05,\n",
       " 1.0673155087295864e-05,\n",
       " 1.0675760204172758e-05,\n",
       " 1.06783765253015e-05,\n",
       " 1.0680997897943686e-05,\n",
       " 1.0683624322072558e-05,\n",
       " 1.0686255797661186e-05,\n",
       " 1.0688898548877741e-05,\n",
       " 1.0691533903110268e-05,\n",
       " 1.0694180532916757e-05,\n",
       " 1.0696832214075177e-05,\n",
       " 1.0699488946558435e-05,\n",
       " 1.0702150730339384e-05,\n",
       " 1.0704817565390763e-05,\n",
       " 1.0707489451685427e-05,\n",
       " 1.0710166389196008e-05,\n",
       " 1.0712848377895135e-05,\n",
       " 1.071553541775544e-05,\n",
       " 1.0718221153535798e-05,\n",
       " 1.072092465084968e-05,\n",
       " 1.072362684402849e-05,\n",
       " 1.0726334088258349e-05,\n",
       " 1.0729046383511557e-05,\n",
       " 1.0731757314982608e-05,\n",
       " 1.0734479700286657e-05,\n",
       " 1.073720713653085e-05,\n",
       " 1.073994607420272e-05,\n",
       " 1.074268362415589e-05,\n",
       " 1.0745419750625023e-05,\n",
       " 1.0748167390350405e-05,\n",
       " 1.0750920080876016e-05,\n",
       " 1.0753677822173717e-05,\n",
       " 1.0756440614215371e-05,\n",
       " 1.0759208456972676e-05,\n",
       " 1.0761981350417438e-05,\n",
       " 1.0764759294521247e-05,\n",
       " 1.076754228925569e-05,\n",
       " 1.077033033459241e-05,\n",
       " 1.0773123430502883e-05,\n",
       " 1.0775921576958532e-05,\n",
       " 1.0778724773930837e-05,\n",
       " 1.0781533021391057e-05,\n",
       " 1.078434631931061e-05,\n",
       " 1.0787164667660648e-05,\n",
       " 1.078998140151149e-05,\n",
       " 1.0792816515536988e-05,\n",
       " 1.0795650015005584e-05,\n",
       " 1.0798488564789147e-05,\n",
       " 1.0801332164858716e-05,\n",
       " 1.0804174090728916e-05,\n",
       " 1.080702777937238e-05,\n",
       " 1.0809886518214577e-05,\n",
       " 1.0812757067415047e-05,\n",
       " 1.0815625918477737e-05,\n",
       " 1.0818493035641158e-05,\n",
       " 1.0821371974985653e-05,\n",
       " 1.0824255964382312e-05,\n",
       " 1.0827145003801573e-05,\n",
       " 1.0830045924865828e-05,\n",
       " 1.0832938232590126e-05,\n",
       " 1.0835842421900118e-05,\n",
       " 1.083875166111439e-05,\n",
       " 1.0841665950203213e-05,\n",
       " 1.0844585289136796e-05,\n",
       " 1.0847509677885301e-05,\n",
       " 1.0850439116418885e-05,\n",
       " 1.0853373604707544e-05,\n",
       " 1.0856313142721327e-05,\n",
       " 1.0859257730430226e-05,\n",
       " 1.086220736780407e-05,\n",
       " 1.0865162054812801e-05,\n",
       " 1.086812179142619e-05,\n",
       " 1.0871086577613959e-05,\n",
       " 1.087405641334588e-05,\n",
       " 1.0877031298591509e-05,\n",
       " 1.0880004199232097e-05,\n",
       " 1.0882989171504994e-05,\n",
       " 1.0885986251106841e-05,\n",
       " 1.0888981334103007e-05,\n",
       " 1.0891981466460486e-05,\n",
       " 1.0894979554515498e-05,\n",
       " 1.089798977359469e-05,\n",
       " 1.0901005041943091e-05,\n",
       " 1.090402535952987e-05,\n",
       " 1.0907057867591447e-05,\n",
       " 1.0910081142295015e-05,\n",
       " 1.0913116607411503e-05,\n",
       " 1.0916157121642608e-05,\n",
       " 1.0919202684957224e-05,\n",
       " 1.0922253297324247e-05,\n",
       " 1.092530895871257e-05,\n",
       " 1.0928369669090814e-05,\n",
       " 1.0931435428427819e-05,\n",
       " 1.093450623669226e-05,\n",
       " 1.0937582093852703e-05,\n",
       " 1.0940662999877713e-05,\n",
       " 1.0943748954735853e-05,\n",
       " 1.094683995839558e-05,\n",
       " 1.094993601082524e-05,\n",
       " 1.0953037111993288e-05,\n",
       " 1.0956143261867956e-05,\n",
       " 1.0959247116745545e-05,\n",
       " 1.0962370707610272e-05,\n",
       " 1.0965492003414278e-05,\n",
       " 1.0968618347797628e-05,\n",
       " 1.0971749740728392e-05,\n",
       " 1.0974878778967672e-05,\n",
       " 1.0978020256990483e-05,\n",
       " 1.0981166783464647e-05,\n",
       " 1.0984325797285038e-05,\n",
       " 1.0987482432471902e-05,\n",
       " 1.0990636653273483e-05,\n",
       " 1.099380337323101e-05,\n",
       " 1.099697514147854e-05,\n",
       " 1.1000151957983758e-05,\n",
       " 1.1003333822714129e-05,\n",
       " 1.1006520735637227e-05,\n",
       " 1.1009712696720356e-05,\n",
       " 1.1012909705931037e-05,\n",
       " 1.1016111763236458e-05,\n",
       " 1.1019318868603978e-05,\n",
       " 1.1022531022000844e-05,\n",
       " 1.1025748223394189e-05,\n",
       " 1.1028970472751152e-05,\n",
       " 1.1032197770038812e-05,\n",
       " 1.1035430115224143e-05,\n",
       " 1.1038667508274117e-05,\n",
       " 1.1041902295950956e-05,\n",
       " 1.1045157437835816e-05,\n",
       " 1.1048409974281101e-05,\n",
       " 1.1051667558458473e-05,\n",
       " 1.1054930190334575e-05,\n",
       " 1.1058190157146328e-05,\n",
       " 1.1061462872415069e-05,\n",
       " 1.1064740635282465e-05,\n",
       " 1.1068031194158774e-05,\n",
       " 1.1071319064027431e-05,\n",
       " 1.1074604209141291e-05,\n",
       " 1.1077902162067743e-05,\n",
       " 1.1081205162424902e-05,\n",
       " 1.1084513210178918e-05,\n",
       " 1.1087826305295989e-05,\n",
       " 1.1091152279516057e-05,\n",
       " 1.1094467637484004e-05,\n",
       " 1.1097795874487022e-05,\n",
       " 1.110112915871741e-05,\n",
       " 1.1104467490141097e-05,\n",
       " 1.11078108687239e-05,\n",
       " 1.1111159294431745e-05,\n",
       " 1.1114512767230395e-05,\n",
       " 1.111787128708556e-05,\n",
       " 1.1121234853962887e-05,\n",
       " 1.112460346782809e-05,\n",
       " 1.1127977128646705e-05,\n",
       " 1.113135583638417e-05,\n",
       " 1.1134739591006079e-05,\n",
       " 1.1138128392477811e-05,\n",
       " 1.1141522240764745e-05,\n",
       " 1.1144913113632876e-05,\n",
       " 1.1148317043543415e-05,\n",
       " 1.1151734066169592e-05,\n",
       " 1.1155148101369897e-05,\n",
       " 1.115856718321152e-05,\n",
       " 1.1161983229947688e-05,\n",
       " 1.1165412393064717e-05,\n",
       " 1.11688466027181e-05,\n",
       " 1.1172285858872772e-05,\n",
       " 1.1175738290813808e-05,\n",
       " 1.117917951054531e-05,\n",
       " 1.1182633905992717e-05,\n",
       " 1.118609334780051e-05,\n",
       " 1.1189557835933352e-05,\n",
       " 1.1193027370355795e-05,\n",
       " 1.1196501951032447e-05,\n",
       " 1.119998157792775e-05,\n",
       " 1.1203466251006151e-05,\n",
       " 1.1206955970232094e-05,\n",
       " 1.1210450735569797e-05,\n",
       " 1.1213950546983653e-05,\n",
       " 1.1217455404437886e-05,\n",
       " 1.1220965307896662e-05,\n",
       " 1.1224480257324096e-05,\n",
       " 1.12280002526843e-05,\n",
       " 1.123152529394128e-05,\n",
       " 1.1235047049445562e-05,\n",
       " 1.1238582170487485e-05,\n",
       " 1.1242130692732364e-05,\n",
       " 1.1245675917215717e-05,\n",
       " 1.1249226187415257e-05,\n",
       " 1.125277311217962e-05,\n",
       " 1.1256333461802491e-05,\n",
       " 1.1259898857032576e-05,\n",
       " 1.1263477724648416e-05,\n",
       " 1.1267053222883468e-05,\n",
       " 1.1270625316001742e-05,\n",
       " 1.1274210893295985e-05,\n",
       " 1.1277801516014713e-05,\n",
       " 1.1281397184121322e-05,\n",
       " 1.1284997897579049e-05,\n",
       " 1.1288603656351127e-05,\n",
       " 1.1292214460400625e-05,\n",
       " 1.1295830309690724e-05,\n",
       " 1.1299451204184438e-05,\n",
       " 1.130307714384478e-05,\n",
       " 1.1306708128634713e-05,\n",
       " 1.131034415851703e-05,\n",
       " 1.131398523345469e-05,\n",
       " 1.1317631353410487e-05,\n",
       " 1.1321282518347054e-05,\n",
       " 1.1324938728227182e-05,\n",
       " 1.1328591342043465e-05,\n",
       " 1.1332266282668485e-05,\n",
       " 1.1335937627154752e-05,\n",
       " 1.1339614016434827e-05,\n",
       " 1.1343295450471064e-05,\n",
       " 1.1346973228765648e-05,\n",
       " 1.1350664740303568e-05,\n",
       " 1.1354361296484773e-05,\n",
       " 1.1358071633424802e-05,\n",
       " 1.1361778290676686e-05,\n",
       " 1.1365481232509932e-05,\n",
       " 1.1369197966885896e-05,\n",
       " 1.1372919745715765e-05,\n",
       " 1.1376646568961454e-05,\n",
       " 1.1380378436584878e-05,\n",
       " 1.138412416798207e-05,\n",
       " 1.138785730481243e-05,\n",
       " 1.1391604305340114e-05,\n",
       " 1.13953563500927e-05,\n",
       " 1.1399113439031831e-05,\n",
       " 1.140287557211909e-05,\n",
       " 1.140664274931612e-05,\n",
       " 1.1410414970584341e-05,\n",
       " 1.1414192235885282e-05,\n",
       " 1.1417974545180256e-05,\n",
       " 1.1421761898430628e-05,\n",
       " 1.1425545345330847e-05,\n",
       " 1.1429351736642811e-05,\n",
       " 1.1433154221527026e-05,\n",
       " 1.1436961750211551e-05,\n",
       " 1.144077432265748e-05,\n",
       " 1.144458292908034e-05,\n",
       " 1.1448405577036675e-05,\n",
       " 1.145224230217361e-05,\n",
       " 1.1456075049274966e-05,\n",
       " 1.1459912839942356e-05,\n",
       " 1.1463746604914914e-05,\n",
       " 1.1467594470701754e-05,\n",
       " 1.1471447379936966e-05,\n",
       " 1.147530533258115e-05,\n",
       " 1.147917744539593e-05,\n",
       " 1.1483036367938705e-05,\n",
       " 1.1486909450573049e-05,\n",
       " 1.1490787576458365e-05,\n",
       " 1.1494670745555029e-05,\n",
       " 1.1498558957823309e-05,\n",
       " 1.1502452213223581e-05,\n",
       " 1.1506350511715946e-05,\n",
       " 1.1510253853260563e-05,\n",
       " 1.1514162237817643e-05,\n",
       " 1.1518075665347176e-05,\n",
       " 1.1521994135809212e-05,\n",
       " 1.1525917649163687e-05,\n",
       " 1.152984620537048e-05,\n",
       " 1.1533779804389423e-05,\n",
       " 1.153771844618045e-05,\n",
       " 1.154166213070317e-05,\n",
       " 1.1545601538948937e-05,\n",
       " 1.1549555296921212e-05,\n",
       " 1.1553523440258546e-05,\n",
       " 1.1557487295304766e-05,\n",
       " 1.1561456192880662e-05,\n",
       " 1.1565420754513337e-05,\n",
       " 1.156939972513442e-05,\n",
       " 1.1573383738163559e-05,\n",
       " 1.1577372793559976e-05,\n",
       " 1.1581376317284795e-05,\n",
       " 1.1585366031291484e-05,\n",
       " 1.158937021354491e-05,\n",
       " 1.1593379438002283e-05,\n",
       " 1.1597393704622554e-05,\n",
       " 1.1601413013364782e-05,\n",
       " 1.160544686154116e-05,\n",
       " 1.1609466757050616e-05,\n",
       " 1.1613501191911954e-05,\n",
       " 1.1617540668730599e-05,\n",
       " 1.1621585187465279e-05,\n",
       " 1.1625634748074666e-05,\n",
       " 1.162968935051738e-05,\n",
       " 1.1633748994751983e-05,\n",
       " 1.1637813680736982e-05,\n",
       " 1.1641883408430832e-05,\n",
       " 1.1645958177791985e-05,\n",
       " 1.165002836065176e-05,\n",
       " 1.1654122841349467e-05,\n",
       " 1.1658212735462373e-05,\n",
       " 1.1662307671075735e-05,\n",
       " 1.1666407648147622e-05,\n",
       " 1.1670502979057743e-05,\n",
       " 1.167461302703104e-05,\n",
       " 1.1678728116337121e-05,\n",
       " 1.1682857970181963e-05,\n",
       " 1.1686983153917178e-05,\n",
       " 1.1691103631831395e-05,\n",
       " 1.1695238886047658e-05,\n",
       " 1.1699379181386005e-05,\n",
       " 1.1703524517804012e-05,\n",
       " 1.1707674895259416e-05,\n",
       " 1.1711840120183509e-05,\n",
       " 1.1715990773112548e-05,\n",
       " 1.1720156273425367e-05,\n",
       " 1.1724326814605551e-05,\n",
       " 1.172850239661051e-05,\n",
       " 1.1732683019397592e-05,\n",
       " 1.1736868682924046e-05,\n",
       " 1.1741059387147115e-05,\n",
       " 1.1745255132023989e-05,\n",
       " 1.1749455917511747e-05,\n",
       " 1.1753661743567523e-05,\n",
       " 1.1757862672929202e-05,\n",
       " 1.176208851721106e-05,\n",
       " 1.176630946471265e-05,\n",
       " 1.1770535452610027e-05,\n",
       " 1.1774766480859994e-05,\n",
       " 1.1778992552761853e-05,\n",
       " 1.1783233649699818e-05,\n",
       " 1.1787479786860568e-05,\n",
       " 1.1791740996519987e-05,\n",
       " 1.1795997225883215e-05,\n",
       " 1.1800248439245534e-05,\n",
       " 1.1804514736863102e-05,\n",
       " 1.1808786074485993e-05,\n",
       " 1.1813062452070572e-05,\n",
       " 1.181735397321319e-05,\n",
       " 1.1821640442476462e-05,\n",
       " 1.1825921824157154e-05,\n",
       " 1.1830218361150925e-05,\n",
       " 1.1834519937887385e-05,\n",
       " 1.1838826554322568e-05,\n",
       " 1.1843138210412399e-05,\n",
       " 1.1847454906112915e-05,\n",
       " 1.1851776641379929e-05,\n",
       " 1.1856103416169313e-05,\n",
       " 1.1860435230436826e-05,\n",
       " 1.1864772084138231e-05,\n",
       " 1.186911397722923e-05,\n",
       " 1.1873460909665365e-05,\n",
       " 1.1877812881402288e-05,\n",
       " 1.1882169892395481e-05,\n",
       " 1.188653194260049e-05,\n",
       " 1.1890888726305176e-05,\n",
       " 1.1895260842915252e-05,\n",
       " 1.189964832803998e-05,\n",
       " 1.1904030534645775e-05,\n",
       " 1.1908417780239933e-05,\n",
       " 1.1912799699687837e-05,\n",
       " 1.1917197011240051e-05,\n",
       " 1.1921599361646096e-05,\n",
       " 1.1926006750860959e-05,\n",
       " 1.193042959146528e-05,\n",
       " 1.1934847070046342e-05,\n",
       " 1.1939259150907887e-05,\n",
       " 1.1943686694907172e-05,\n",
       " 1.1948119277489573e-05,\n",
       " 1.1952556898609862e-05,\n",
       " 1.1956999558222532e-05,\n",
       " 1.1961447256282358e-05,\n",
       " 1.196589999274378e-05,\n",
       " 1.1970357767561349e-05,\n",
       " 1.1974820580689452e-05,\n",
       " 1.197928843208253e-05,\n",
       " 1.1983761321694913e-05,\n",
       " 1.1988239249480881e-05,\n",
       " 1.199272221539471e-05,\n",
       " 1.1997210219390564e-05,\n",
       " 1.2001703261422557e-05,\n",
       " 1.200619072683856e-05,\n",
       " 1.2010704459411349e-05,\n",
       " 1.2015212615276152e-05,\n",
       " 1.2019725808993156e-05,\n",
       " 1.2024244040516252e-05,\n",
       " 1.2028756635784569e-05,\n",
       " 1.2033284930899865e-05,\n",
       " 1.2037818263682652e-05,\n",
       " 1.2042367343745031e-05,\n",
       " 1.2046910763604824e-05,\n",
       " 1.2051448487572643e-05,\n",
       " 1.2056001970561739e-05,\n",
       " 1.2060560490986195e-05,\n",
       " 1.2065124048799408e-05,\n",
       " 1.2069692643954772e-05,\n",
       " 1.207427706922876e-05,\n",
       " 1.2078844946105113e-05,\n",
       " 1.2083428653006608e-05,\n",
       " 1.2088017397063186e-05,\n",
       " 1.2092611178227914e-05,\n",
       " 1.209720999645397e-05,\n",
       " 1.2101813851694202e-05,\n",
       " 1.2106422743901678e-05,\n",
       " 1.21110366730293e-05,\n",
       " 1.2115655639029804e-05,\n",
       " 1.2120279641856152e-05,\n",
       " 1.2124897757989146e-05,\n",
       " 1.2129542757796937e-05,\n",
       " 1.2134181870816793e-05,\n",
       " 1.2138826020473116e-05,\n",
       " 1.2143475206718362e-05,\n",
       " 1.2148118446639325e-05,\n",
       " 1.2152777694041504e-05,\n",
       " 1.2157441977890055e-05,\n",
       " 1.2162122316638253e-05,\n",
       " 1.2166796685114747e-05,\n",
       " 1.2171476089894392e-05,\n",
       " 1.217614947679441e-05,\n",
       " 1.2180838942159007e-05,\n",
       " 1.2185533443683268e-05,\n",
       " 1.2190232981319162e-05,\n",
       " 1.219494865666417e-05,\n",
       " 1.2199647164733886e-05,\n",
       " 1.2204361810416488e-05,\n",
       " 1.2209081492018323e-05,\n",
       " 1.2213806209491192e-05,\n",
       " 1.2218535962786844e-05,\n",
       " 1.2223270751856866e-05,\n",
       " 1.2228010576652947e-05,\n",
       " 1.2232755437126619e-05,\n",
       " 1.2237505333229409e-05,\n",
       " 1.2242260264912736e-05,\n",
       " 1.2247020232128076e-05,\n",
       " 1.2251785234826736e-05,\n",
       " 1.2256555272960024e-05,\n",
       " 1.2261330346479196e-05,\n",
       " 1.2266110455335505e-05,\n",
       " 1.2270884307838915e-05,\n",
       " 1.227567447534726e-05,\n",
       " 1.2280480993438148e-05,\n",
       " 1.2285281243153842e-05,\n",
       " 1.2290086527961846e-05,\n",
       " 1.229488549679558e-05,\n",
       " 1.229970083976596e-05,\n",
       " 1.2304521217681367e-05,\n",
       " 1.2309346630492449e-05,\n",
       " 1.231418847666652e-05,\n",
       " 1.231902397099553e-05,\n",
       " 1.2323853077806736e-05,\n",
       " 1.2328698629707125e-05,\n",
       " 1.2333549216256162e-05,\n",
       " 1.2338404837404274e-05,\n",
       " 1.2343265493101896e-05,\n",
       " 1.2348131183299234e-05,\n",
       " 1.2353001907946718e-05,\n",
       " 1.235787766699445e-05,\n",
       " 1.2362758460392693e-05,\n",
       " 1.2367644288091493e-05,\n",
       " 1.2372535150041004e-05,\n",
       " 1.237743104619116e-05,\n",
       " 1.2382331976491955e-05,\n",
       " 1.2387237940893375e-05,\n",
       " 1.239214893934519e-05,\n",
       " 1.2397053371458849e-05,\n",
       " 1.2401974425988451e-05,\n",
       " 1.240691213850116e-05,\n",
       " 1.2411843272652375e-05,\n",
       " 1.2416779440602562e-05,\n",
       " 1.2421720642301326e-05,\n",
       " 1.2426655206126646e-05,\n",
       " 1.2431606463299229e-05,\n",
       " 1.2436574449383658e-05,\n",
       " 1.244153578557114e-05,\n",
       " 1.2446490436196412e-05,\n",
       " 1.2451461827453016e-05,\n",
       " 1.2456438252103731e-05,\n",
       " 1.2461419710097723e-05,\n",
       " 1.2466406201384048e-05,\n",
       " 1.2471409504324693e-05,\n",
       " 1.2476394283629975e-05,\n",
       " 1.2481395874487469e-05,\n",
       " 1.2486402498433189e-05,\n",
       " 1.2491414155416025e-05,\n",
       " 1.249643084538465e-05,\n",
       " 1.2501452568287898e-05,\n",
       " 1.2506479324074388e-05,\n",
       " 1.2511511112692787e-05,\n",
       " 1.251654793409166e-05,\n",
       " 1.2521589788219567e-05,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbde1d0d1d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Parameters\n",
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = './BEST_checkpoint_ssd300.pth.tar'\n",
    "\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=workers,\n",
    "                                          pin_memory=True)\n",
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate\n",
    "\n",
    "    test_loader: test data loader\n",
    "    model:  trained model\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes  = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels= list()\n",
    "    true_scores= list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult' when we calculate 'mAP' score\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # for each batch\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating model')):\n",
    "            images = images.to(device) # (N, 3, 300, 300)\n",
    "\n",
    "            # Forward prop\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                        min_score=0.01, max_overlap=0.45,\n",
    "                                                                                        top_k=200)\n",
    "            \n",
    "            # Store this batch's results for mAP calculation\n",
    "            # True boxes & labels\n",
    "            boxes  = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Cakcukate mAP\n",
    "        # Calculate mAP\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "        # Print AP for each class\n",
    "        pp.pprint(APs)\n",
    "\n",
    "        print('\\nMean Average Precision (mAP): %.3f' % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|██████████| 78/78 [09:16<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aeroplane': 0.7841536998748779,\n",
      " 'bicycle': 0.8343356251716614,\n",
      " 'bird': 0.7544137239456177,\n",
      " 'boat': 0.6724541783332825,\n",
      " 'bottle': 0.4219571053981781,\n",
      " 'bus': 0.8412512540817261,\n",
      " 'car': 0.8449738621711731,\n",
      " 'cat': 0.879683792591095,\n",
      " 'chair': 0.544768214225769,\n",
      " 'cow': 0.8194712996482849,\n",
      " 'diningtable': 0.7313601970672607,\n",
      " 'dog': 0.8598443865776062,\n",
      " 'horse': 0.8697940111160278,\n",
      " 'motorbike': 0.8257168531417847,\n",
      " 'person': 0.7651263475418091,\n",
      " 'pottedplant': 0.4683242738246918,\n",
      " 'sheep': 0.7657483220100403,\n",
      " 'sofa': 0.7707215547561646,\n",
      " 'train': 0.8342307209968567,\n",
      " 'tvmonitor': 0.7343186736106873}\n",
      "\n",
      "Mean Average Precision (mAP): 0.751\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note\n",
    "At this point, we have already achieved better results than the orginal paper(which is 77.2 mAP)<br>\n",
    "\n",
    "We are not done yet.<br>\n",
    "There are a few tricks that we may try to make the result even better. See next notebook, Phase III--Gradual unfreezing and fine tune transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
