{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase IV\n",
    "In the last [notebook](https://github.com/Sylar257/Single-Shot-Detection-project/blob/master/Phase%20III--Gradual%20unfreezing%20and%20gradient%20clipping.ipynb), we built an SSD with batchnorm implemented for both base architecture and auxiliary layers that achieves pretty good resuilt(mAP = 0.791).<br>\n",
    "In this notebook, we will implement some more modern technique and hopefully push the performance a little further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning optimal learning rate\n",
    "\n",
    "SSD has two loss functions that we want to minimize: confidence loss & location loss (details included in later section).\n",
    "\n",
    "As we are training the network jointly with the two loss, a natural question to ask is that what’s the optimal ratio of these two losses to back-propagate so that we have the best training efficiency. In the [original SSD paper](https://arxiv.org/pdf/1512.02325.pdf), this question is not answered by simply assigned `ratio = 1` to the two losses and it seemed to work just fine.\n",
    "\n",
    "In the phase IV of this repo, we will be experimenting setting this ratio as two learnable parameters(one for each loss) following the guide in [this paper](https://zpascal.net/cvpr2017/Kendall_Geometric_Loss_Functions_CVPR_2017_paper.pdf).:\n",
    "$$\n",
    "L_\\alpha(I) = L_c(I)\\exp({-\\hat{s_c}})+\\hat{s_c}+L_l(I)\\exp({-\\hat{s_l}})+\\hat{s_l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm implementation\n",
    "  \n",
    "  The original SSD architecture is Batchnorm-free. The inspiration is from this paper:[ScratchDet: Training Single-Shot Object Detectors from Scratch](https://arxiv.org/pdf/1810.08425.pdf). Objection detection’s current state-of-the-art detectors are generally fine-tuned from high accuracy classification networks. *e.g.*, VGGNet(which we will be using), ResNet and GoogLeNet **pre-trained** on ImageNet. There  both are advantages and disadvantages to use pre-trained base net. One of the primary reason not to train from scratch is that the optimization landscape is bumpy and not idea for training. Fortunately, now we have Batchnorm that could greatly mitigate this issue. This idea is introduced in [this paper](https://arxiv.org/pdf/1810.08425.pdf), and in this project’s Phase-III we shall utilize the idea of batchnorm to further improve our training result.\n",
    "  \n",
    "  Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called \"internal covariate shift\".(which was found out untrue in this paper: [How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)) However, what it actually does, from a certain perspective, is to make the loss surface much more smoother so that our optimizer will have an easier time locating the global minimum.\n",
    "  \n",
    "![Loss_surfaces](images/Loss_surfaces.png)\n",
    "  \n",
    "   This paper has also shown that, “add batchnorm layers to every CNN layer” is more beneficial to if we train-from-scratch as compared to using a pre-trained model. The below table shows the performance of different result under various training conditions:\n",
    "  \n",
    "![pretraining_bn_backbong_head](images/pretraining_bn_backbong_head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import transform\n",
    "# specify GPU for cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase_BN(nn.Module):\n",
    "    \"\"\"\n",
    "    We implement VGG-16 here for low-level feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase_BN, self).__init__()\n",
    "\n",
    "        # Stabdard convolutional layers in VGG16\n",
    "        # We have an input size of 300 by 300\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # stride = 1, output = (300+2-3)/1+1 = 300\n",
    "        self.bn_1_1  = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # output = 300 as before\n",
    "        self.bn_1_2  = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (300-2)/2+1 = 150\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64,  128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.bn_2_1  = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.bn_2_2  = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (150-2)/2 +1  = 75\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_1  = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_2  = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.bn_3_3  = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # output = ceil((75-2)/2)-1 = 38   if floor we would be getting 37 here which is an odd number\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_1  = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_2  = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.bn_4_3  = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)           # output = (38-2)/2 +1  = 19\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_1  = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_2  = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.bn_5_3  = nn.BatchNorm2d(512)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # We retain the size at this step with padding and stride of 1\n",
    "        # output = (19+2-3)/1+1 = 19\n",
    "\n",
    "        # Here we replace the FC6 and FC7 with the technique introduce by sgrvinod(same with the original paper)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # output = (19+12-3-2*(6-1))/1+1 = 19\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)                       # output = (19-1)/1+1 = 19\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward run with an image input of size 300 by 300\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn_1_1(self.conv1_1(image)))   # (N,64,300,300)\n",
    "        out = F.relu(self.bn_1_2(self.conv1_2(out)))     # (N,64,300,300)\n",
    "        out = self.pool1(out)               # (N,64,150,150)\n",
    "\n",
    "        out = F.relu(self.bn_2_1(self.conv2_1(out)))  # (N,128,150,150)\n",
    "        out = F.relu(self.bn_2_2(self.conv2_2(out)))  # (N,128,150,150)\n",
    "        out = self.pool2(out)            # (N,128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.bn_3_1(self.conv3_1(out)))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.bn_3_2(self.conv3_2(out)))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.bn_3_3(self.conv3_3(out)))  # (N,256, 75, 75)\n",
    "        out = self.pool3(out)            # (N,256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.bn_4_1(self.conv4_1(out)))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.bn_4_2(self.conv4_2(out)))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.bn_4_3(self.conv4_3(out)))  # (N, 512, 38, 38)\n",
    "        # here we extract the feature from conv4_3\n",
    "        conv4_3_feats = out              # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)            # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.bn_5_1(self.conv5_1(out)))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.bn_5_2(self.conv5_2(out)))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.bn_5_3(self.conv5_3(out)))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)            # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))    # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        Use pre-trained wieght from Torch Vsion. \n",
    "        Convert fc6 and fc7 weights into conv6 and conv7\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16_bn(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
    "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
    "        # ...operating on the 2D image of size (C, H, W) without padding\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model with pre-trained weights\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializa AuxiliaryConvolutions with Kaiming_uniform_ and set non-linearity to 'relu'\n",
    "# Implement Batchnorm in AuxiliaryConv layers\n",
    "\n",
    "class AuxiliaryConvolutions_BN(nn.Module):\n",
    "    \"\"\"\n",
    "    These layers are put on top of base model to produce more feature maps for object detections.(smaller maps)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions_BN, self).__init__()\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)         # output=(19-1)/1+1 = 19\n",
    "        self.bn_8_1  = nn.BatchNorm2d(256)\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)# output=(19+2-3)/2+1 = 10\n",
    "        self.bn_8_2  = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)          # output=(10-1)/1+1 = 10\n",
    "        self.bn_9_1  = nn.BatchNorm2d(128)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)# output=(10+2-3)/2+1 = 5 because by defaul we use \"floor\"\n",
    "        self.bn_9_2  = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(5-1)/1+1 = 5\n",
    "        self.bn_10_1  = nn.BatchNorm2d(128)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(5-3)/1+1 = 3\n",
    "        self.bn_10_2  = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(3-1)/1+1 = 3\n",
    "        self.bn_11_1  = nn.BatchNorm2d(128)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(3-3)/1+1 = 1\n",
    "        self.bn_11_2  = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        print('Using Batchnorm version')\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        for i, c in enumerate(self.children()):\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "            elif isinstance(c, nn.BatchNorm2d):\n",
    "                c.reset_parameters()\n",
    "                print(f'layer {i} is initialized as BatchNorm2d')\n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        conv7_feats: (N, 1024, 19, 19)\n",
    "        return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn_8_1(self.conv8_1(conv7_feats)))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.bn_8_2(self.conv8_2(out)))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.bn_9_1(self.conv9_1(out)))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.bn_9_2(self.conv9_2(out)))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.bn_10_1(self.conv10_1(out)))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.bn_10_2(self.conv10_2(out)))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.bn_11_1(self.conv11_1(out)))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.bn_11_2(self.conv11_2(out)))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, we replace xavier_uniform with kaiming_uniform_\n",
    "\n",
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict class scores and bounding boxes using lower and higher level feature maps\n",
    "\n",
    "    The bounding boxes (offsets (g_{c_x}, g_{c_y}, g_w, g_h) of the 8732 default priors)\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for encoding definition\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 8732 hounding boxes\n",
    "    A high score for 'background' = no object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of proior_boxes we are considering per position in each feature map\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                    'conv7': 6,\n",
    "                    'conv8_2': 6,\n",
    "                    'conv9_2': 6,\n",
    "                    'conv10_2': 4,\n",
    "                    'conv11_2': 4}\n",
    "        # 4 prior-boxes prediction convoluitions (predict offsets w.r.t prior-boxes)\n",
    "\n",
    "        # This is the part to compute LOCALIZATION prediction\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3']*4, kernel_size=3, padding=1) # output = (38-3+2)/1+1 = 38, same padding\n",
    "        self.loc_conv7   = nn.Conv2d(1024, n_boxes['conv7']*4, kernel_size=3, padding=1)  # output = (19-3+2)/1+1 = 19\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2']*4, kernel_size=3, padding=1) # output = (10-3+2)/1+1 = 10\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2']*4, kernel_size=3, padding=1) # output = (5-3+2)/1 +1 = 5\n",
    "        self.loc_conv10_2= nn.Conv2d(256, n_boxes['conv10_2']*4,kernel_size=3, padding=1) # output = (3-3+2)/1 +1 = 3\n",
    "        self.loc_conv11_2= nn.Conv2d(256, n_boxes['conv11_2']*4,kernel_size=3, padding=1) # output = (1-3+2)/1 +1 = 1\n",
    "\n",
    "        # This is the part to comput CLASS prediction\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7   = nn.Conv2d(1024,n_boxes['conv7']   * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,n_boxes['conv10_2'] * n_classes,kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,n_boxes['conv11_2'] * n_classes,kernel_size=3, padding=1)\n",
    "\n",
    "        self.init_conv2d()\n",
    "    def init_conv2d(self):\n",
    "        # Use Kaiming_uniform_ here instead of xavier_uniform_\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds w.r.t prior boxes\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)            # (N, 16, 38, 38)  16 is from 4 priors 4*4=16\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)  to match prior-box order (after .view())\n",
    "        # .contiguous() ensures it is stores in a contiguous chunk of memory, needed for .view() below\n",
    "\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)          # This give us (N, 5776, 4) the (g_{c_x}, g_{c_y}, g_w, g_h) for all 5776 priors\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                    self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                                self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # A total of 8732 boxes\n",
    "        # Concatenate in this specific order    \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "\n",
    "        \"\"\"\n",
    "        This class works as a wrapper that encapsulates the base VGG network, auxiliary, and prediciton convolutions.\n",
    "        \"\"\"\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase_BN()\n",
    "        self.aux_convs = AuxiliaryConvolutions_BN()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factor = nn.Parameter(torch.FloatTensor(1, 512, 1, 1)) # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factor, 20)\n",
    "\n",
    "        # The above two lines demonstrate a simple example of how do add a leanable-parameter into our computation\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()  # with shape of (8732, 4)\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        image: (N, 3, 300, 300)\n",
    "        return:  8732 locations and class scores (i.e.  w.r.t each prior box) for the given image(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run VGG base network convolutions (lower level feature map generators, up to conv7)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)   # (N, 512, 38, 38),  (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm using our learnable parameter\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm                         # (N, 512, 38, 38) this step was done by broadcasting\n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factor            # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliaury convolution (higher level feature map extraction)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats) \n",
    "        # (N, 512, 10, 10), (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offset w.r.t. priors and classes in each resulting location)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of our forward pass) to detect objects.\n",
    "\n",
    "        For each class. perform Non-Maximum Suppression (NMS) on boxes that are above a minimum score\n",
    "\n",
    "        predicted_locs: predicted locations w.r.t the 8732 prior boxes, a tensor of (N, 8732, 4)\n",
    "        predicted_scores: predicted class score for each of prediced locations, a tensor of (N, 8732, n_classes)\n",
    "        min_score: the minimun score for a box to be consifered a match for a CERTAIN CLASS\n",
    "        max_overlap: the maximum overlap that we allow. For any pair of boxes with higher overlap, the lower class score one will be suppressed\n",
    "        top_k: if there are a lot of resulting detection across all classes, keep only the top_k \n",
    "        \n",
    "        return: detections (boxes, labels, and scores), lists of length batch_size N\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0) # N\n",
    "        n_priors = self.priors_cxcy.size(0) # 8732\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)\n",
    "\n",
    "        # list to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_scores = list()\n",
    "        all_images_labels = list()\n",
    "\n",
    "        assert n_priors == predicted_scores.size(1) == predicted_locs.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Convert diviation from prior boxes to (c_x, c_y, w, h)\n",
    "            # Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max)\n",
    "            \n",
    "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_boxes = list()\n",
    "            image_scores = list()\n",
    "            image_labels = list()\n",
    "\n",
    "            max_score,  best_label = predicted_scores[i].max(dim=1) # (8732), (8732)\n",
    "\n",
    "            # operations for each class. Class 0 is not included here because it denotes background(negative)\n",
    "            for c in range(1, self.n_classes):\n",
    "                # Keep only predicted boxes and scores where scores for this class are above minimum_score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_scores > min_score # torch.uint8 (byte) tensor, for infexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                # here, we will retain the score & locs of the boxes with score higher than the threshold\n",
    "                class_scores = class_scores[score_above_min_score] # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualfied, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs) # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "                \n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the version with learnable alpha implemented\n",
    "    The Multibox loss function for SSD300 architecture, which is a combination of:\n",
    "\n",
    "    1) a localization loss for the predicted locations of the boxes, and\n",
    "    2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha_C=0., alpha_L=1.5):\n",
    "        \"\"\"\n",
    "        priors_cxcy: priors' (c_x, c_y, w, h)\n",
    "        threshold: overlapping less than 'threshold' with priors are set to class-background\n",
    "        neg_pos_ratio: a parameter used when calculating hard negative mining. Detail in forward() section\n",
    "        alpha: the ratio between localization loss and confidence loss\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha_C = nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        nn.init.constant_(self.alpha_C, alpha_C)\n",
    "        self.alpha_L = nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
    "        nn.init.constant_(self.alpha_L, alpha_L)\n",
    "        \n",
    "        # the two loss functions for localization and classification\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "        \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        predicted_locs:   predicted locations/box w.r.t 8732 priors, (N, 8732, 4)\n",
    "        predicted_scores: preidted class scores for each of the encoded locations, (N, 8732, n_classes)\n",
    "        boxes:            ground truth boxes,  a list of N tensors\n",
    "        label:            ground truth labels, a list of N tensors\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors   = self.priors_cxcy.size(0)\n",
    "        n_classes  = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)   # (N, 8732)\n",
    "\n",
    "        # for each image in the minibatch\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0) # the number of objects exist in the given image\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # for each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0) # (8732),  (8732)\n",
    "\n",
    "            # we dont want a situation where an object is not represented in our positive (non-background) priors for reasons like:\n",
    "            # 1. An objext might not be the best object for all priors, and is theresore not in the object_for_each_prior\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5 by defaul)\n",
    "\n",
    "            # to remedy this\n",
    "            # first, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)   # (n_object)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (this fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "        total_loss = loc_loss*torch.exp(self.alpha_L)-self.alpha_L+conf_loss*torch.exp(self.alpha_C)-self.alpha_C\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used as DataLoader later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        \"\"\"\n",
    "        data_folder: folder where data files are stored\n",
    "        split: this must be either 'TRAIN' or 'TEST'\n",
    "        keep_difficult: keep or discard objects that are considered as difficult(a property come with the dataset)\n",
    "        \"\"\"\n",
    "        self.split = split.upper()\n",
    "\n",
    "        assert self.split in {'TRAIN','TEST'}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        with open(os.path.join(data_folder,self.split+'_images.json'),   'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split+'_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Read Image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
    "        objects = self.objects[i]\n",
    "        boxes  = torch.FloatTensor(objects['boxes']) # (n_objects, 4)\n",
    "        labels = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
    "\n",
    "        # Discard difficult objects, if specified\n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1-difficulties]\n",
    "            labels = labels[1-difficulties]\n",
    "            difficulties = difficulties[1-difficulties]\n",
    "\n",
    "        # Apply transformations\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader)\n",
    "\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "\n",
    "        @Params\n",
    "        batch: an iterable of N sets from __getitem__()\n",
    "        return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes  = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate annealing\n",
    "In the first phase, we used a constant learning rate in our training loop. \n",
    "Decay the learning rate by a ratio of 0.9, when our model cease to make an improvemnet. \n",
    "This was recommentded in the [original paper](https://arxiv.org/abs/1512.02325). <br>\n",
    "What we are going to do differently this time is to implement a learning rate scheduler that anneal our learning rate in a way that it has a period of \"warming up & catch up speed\", followed by a period of \"high learning rate\" then gradually slow down to find good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "@annealer\n",
    "def sched_no(start, end, pos): return start\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
    "\n",
    "#This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will build different annealing options\n",
    "Cosine scheduling will be implemented later same as it's done in FastAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd1zV1f/A8dfhspENLhAx9144c+DempmKe+Aqs/xlWWnDplaOtDQVV1rq18xypCKas3Ln3igqLmTI3pzfHxcNFQThDric5+NxH3A/67y52ZsP53Pe5wgpJYqiKIrpMjN2AIqiKIp+qUSvKIpi4lSiVxRFMXEq0SuKopg4legVRVFMnLmxA8iOm5ub9Pb2NnYYiqIoRcaxY8fCpZTu2e0rlIne29ubo0ePGjsMRVGUIkMIcT2nfarrRlEUxcSpRK8oimLiVKJXFEUxcSrRK4qimDiV6BVFUUxcroleCFFOCLFbCHFeCHFWCPFmNscIIcQ8IcQVIcQpIUSDLPuGCSEuZ76G6foHUBRFUZ4tL8Mr04BJUsrjQgh74JgQIkhKeS7LMV2AypmvJsAPQBMhhAvwMeADyMxzN0kpo3T6UyiKoig5yjXRSynvAHcyv48VQpwHPICsib4XsFJq5zw+KIRwEkKUAXyBICllJIAQIgjoDKzR6U+RadAvXxMWnYoZVphJK8ywxVzao5ElMMceUTjLBhRFUQCoUdaBj3vU1Pl1nyvzCSG8gfrAoSd2eQA3s7wPzdyW0/bsrj0GGAPg5eX1PGE9ciZhLRkWqdnvlAJz7DGXzlhIVyxlSSxlSawySmMly6LBNl9tKoqiFHZ5TvRCiBLAr8BEKWXMk7uzOUU+Y/vTG6VcDCwG8PHxyddqKIc9XyKhfHMSSlcnPjWe2JRYopKiiEyKJCIxgrsJd7kbf5fbcbcJjT1Fmkx7dG4p21JUdalKTdea2pdbTdxs3PIThqIoSqGSp0QvhLBAm+R/llJuyOaQUKBclveewO3M7b5PbN+Tn0BzlRiF1al1WO2fjXPj0dDuI3CukuPhaRlp3Im7w7WYa1yOusyVB1e4EHmBA7cOkCEzACjvUJ76JevTsFRDmpZpSmm70noJXVEURZ9EbksJCiEE8CMQKaWcmMMx3YDXga5oH8bOk1I2znwYewx4OArnONDwYZ99Tnx8fGS+5rpJjoU/P4dDi8DBA7rPgSodn+sSCakJXIi8wMn7Jzkedpx/w/4lOjkaAG8Hb5qWaUorz1Y0Kt0Ia3Pr549RURRFD4QQx6SUPtnuy0OibwHsB04DGZmbpwBeAFLKhZm/DL5H+6A1ARghpTyaef7IzOMBvpBSLs8t4Hwn+oduHoZNE+D+BajdFzrPALv8dcNkyAwuR13m0J1DHLxzkKP3jpKYloi1xpqmZZrSvnx7fMv54mjlmP94FUVRCqhAid4YCpzoAdKS4cAc2DcTrOy1yb5OPxDZPTbIu+T0ZI7cPcLem3vZE7qHu/F3MRfmNCnbhK4VutLOqx12FnYFi11RFOU5Fc9E/1DYBe3dfehhqNRe253jlL9RPU+SUnIm/AxBN4LYEbKDW3G3sNZY41vOl16VetGsTDM0ZhqdtKUoivIsxTvRA2Skw5GlsOsTkBLafQiNx4AOk7CUkpP3T7Ll6hYCQwJ5kPyA0nal6VWxF30q96FMiTI6a0tRFOVJKtE/9OAmbPk/uBIEHj7Q63soWV3nzaSkp7D75m5+u/wbf9/+GyEErT1b41fVj6Zlm2Im1BRDiqLoVrFI9FJKks6cRWNfAstnLUMoJZxeD9vfhaQYaPkWtJwE5lYFCzoHt+Jusf7SejZc3kBkUiQVHCswpMYQerzQQ43aURRFZ4pFos9ITORSs+Y4vdyb0h99lPsJ8eGw/X04vQ7cqkLP78CrST4jzl1KegqBIYGsOreK85HncbZyxq+aHwOrDcTJ2klv7SqKUjwUi0QPEDphAoknT1Fpz26EWR67Ry4HweaJEHMLHhZaWdk/d9t5JaXk2L1j/HjuR/bc3IONuQ19KvdheM3hlLIrpbd2FUUxbc9K9CbVWWzfvj1pYWEknT6d95Mqd4DxB7UPZw8HwPymcGmH3mIUQuBT2ofv2n7Hhp4baO/VnjUX1tBlQxe+PPQl9+Lv6a1tRVGKJ5NK9CV8fcHcnNidO5/vRCt76Po1+O8AqxKwui+s99d27+hRZefKfNnyS7b03kLPij355eIvdNnQhemHphOeqN+2FUUpPkyq6wbgxsiRpN6+wwvbtiLyUxyVlgz7Z8P+WTottMqL0NhQAk4HsPHKRiw1lgyuPpjhtYbjYOmg97YVRSnaik3XDUCJ9u1JCQkh5erV/F3A3AravA/j9oNrRfhtDPz8Cjy4odtAs+Fp78knzT/h916/09qzNQGnA+i6oSs/n/+Z1PQcpl9WFEXJhcklevt27QCIDXrO7psnlawOIwOhy9dw46C27/7gQm3xlZ55O3rzTetvWNd9HdVcqjHj8Axe2vgSu67vojD+BaYoSuFmconeolQprOvUef5++uyYaaDJWHjtIJRvrh17v6wThJ0v+LXzoLprdQI6BLCg3QIszCyYuGcio3eM5lLUJYO0ryiKaTC5RA/a0TdJZ86QeueObi7oVA4G/QIvB0DkVVjYEnZP1/bn65kQgpaeLVnfcz1TmkzhfOR5+m7uyxcHv3g0fbKiKMqzmGyiB4jduUt3FxVC+1B2/GGo2Rv2zoBFrbRTIhuAuZk5A6oN4I/ef9CvSj/WXVpHz997sil4k+rOURTlmUwy0Vu9UAHLShWJDQzU/cXt3KBPAAz8BZLjYGlH2PqOdtETA3CydmJq06ms7bYWT3tPph6YyvDtw7n6IJ8PnxVFMXkmmegBHDp1JuHYMVLDwvTTQJWOBi20elJ11+qs6rKKT5p/QnB0MH0292H+ifkkp+u/O0lRlKIl10QvhFgmhAgTQpzJYf87QogTma8zQoj0zCUEEUKECCFOZ+7Tw3SUOXPo3AmkJDYoSH+NZFdo9esovRdaPWQmzHi58sts7LWRTt6dWHhyIa9seoXj944bpH1FUYqGvNzRr0C7RGC2pJTfSCnrSSnrAe8De59YE7ZN5v5sB/Lri1Xlytrum23b9d9YucYwdh/4vg9nf4fvG8HJ/2lnyjQAVxtXZrScwaL2i0jNSGX49uFMPzSdhNQEg7SvKErhlmuil1LuA565mHcWA4A1BYpIh/TefZOVuRX4vvdEoVVfgxRaPdTcozkbem7Ar5ofqy+s5uVNL3Pk7hGDta8oSuGksz56IYQt2jv/X7NslsAOIcQxIcSYXM4fI4Q4KoQ4ev/+fZ3EZJDumydlLbS6/rdBC60AbC1smdJkCis6r8BMmDEycCRfHf6KpLQkg7SvKErho8uHsT2Av57otnlRStkA6AKMF0K0yulkKeViKaWPlNLH3d1dJwEZtPsmq4eFVuMPQvlmBi+0AmhYqiHre6zHr6ofP53/ib6b+3ImPNvHLIqimDhdJno/nui2kVLezvwaBvwGNNZhe3li0O6bJzl5waD12kKriGCDFlqB9u5+atOpBHQMICk9iSFbh7Do5CLSMtIM0r6iKIWDThK9EMIRaA1szLLNTghh//B7oCNg8FtKo3TfZPWw0Or1I0YptAJoWqYpv/b8lQ7eHfj+xPeM2D6C0NhQg7WvKIpx5WV45RrgH6CqECJUCOEvhBgnhBiX5bDewA4pZXyWbaWAA0KIk8Bh4A8ppYH7UP7rvonZus3QTT/uYaHVoPVZCq0ma783AAdLB75u9TUzWs7gyoMr9N3cl23XjPyZKIpiECY3H312wn/4gftz51Hpz11YlC2rs+vmW3Is7PpUW2jl6And52hXujKQW3G3mLxvMqfun6JXxV5MaTIFWwtbg7WvKIruFav56LPj0K0bADFbtxo5kkxW9tD1G+3oHAtb7Xz3v442WKGVRwkPVnRewejao9kUvAm/P/zUjJiKYsKKRaK39PLCum4dorf8YexQHufVRDvuvvV7cPY3mN8YTq0zSKGVhZkFbzR4g4COAcQkxzDwj4FsuLxBTZCmKCaoWCR6AMdu3Um+cIHky5eNHcrjHq5oNXYfOFeADaMNWmjVpEwT1vdcT72S9fj474+ZemCqqqhVFBNTbBK9Q5fOYGZG9B+F7K7+oVI1tHPmdJ5h8EIrNxs3FrVfxGv1XmPL1S0M2jqIa9HX9N6uoiiGUWwSvbm7O3ZNmxKz5Y/C2z1hpoGmr8Jr/4BXU4MWWmnMNLxa91UWdlhIRGIEflv82BFiuNk4FUXRn2KT6AEcevQgNTSUpJMnjR3KszmXh8G/Qu/FBi+0al62Oet6rKOScyUm7Z3E7GOzVYGVohRxxSrR23doj7C0JHrzFmOHkjshoG7/zEKrlwxaaFXarjTLOy2nf9X+LD+znHFB44hMyuu8doqiFDbFKtFrSpSgRJs2xGzbhkxNNXY4eWPnBn2WPLGi1WS9r2hlqbHkg6Yf8GnzT/k37F/8tvhxPsJwc/UoiqI7xSrRAzi+1Iv0yEji9u83dijP59GKVqPh8GJY0Awu639ah96Ve7Oyy0oyZAZDtw1l69VCUougKEqeFbtEX6JFCzQuLkT/9ruxQ3l+Riq0qulWk7Xd11LDtQbv7n+XOcfmkG6gaZcVRSm4YpfohYUFjj26E7tnD2lRUcYOJ3+MUGjlZuPGko5L6FelH8vOLOPN3W8Sl2KYeXoURSmYYpfoARxfeglSUwvPlAj58bDQatx+gxVaWWgs+LDZh0xtMpUDtw4wZNsQbsbe1Ft7iqLoRrFM9NbVq2NVtSrRv2/M/eDCrmT1zEKrr/4rtDq0SK+FVn7V/FjYYSFhCWEM/GMgx+4d01tbiqIUXLFM9KC9q086fZrk4GBjh1JwZhpoOu6/Qqttk2FZZwi7oLcmm5Zpyppua3CycmLUjlFsCt6kt7YURSmY4pvoe3QHjcY07uofeqzQ6gosbAF7ZkBail6a83Lw4qeuP9GwZEOmHpjK3ONzyZAZemlLUZT8K7aJ3tzNjRItWhC9cSMyzYQqPx8WWo0/DDV6wZ7pmYVWR/TSnKOVIz90+IFXqrzCktNLeHffuySnG2apREVR8iYvK0wtE0KECSGyXQZQCOErhIgWQpzIfH2UZV9nIcRFIcQVIcR7ugxcFxxf6UNaWBhxBw4YOxTdK+EOryyFgeu0xVVLO8C2d/WyopWFmQUfNf2Itxq+xfaQ7YwKHKUqaRWlEMnLHf0KoHMux+yXUtbLfH0KIITQAPOBLkANYIAQokZBgtU1e19fNK6uPFi/3tih6E+VTv8VWh1aBAuawuWdOm9GCMGIWiOY1XoW5yPPM3jrYK7HXNd5O4qiPL9cE72Uch+Qn9uzxsAVKeVVKWUKsBbolY/r6I2wsMCp90vE7d5D2v37xg5Hf54qtOqTWWgVofOmOnp3ZGmnpcSlxDF462BOhJ3QeRuKojwfXfXRNxNCnBRCbBNC1Mzc5gFkHWQdmrktW0KIMUKIo0KIo/cNmHQd+/SB9HQe/F4EK2Wf16NCq3czC60awalfdF5oVde9Lj91/QkHSwf8A/0Juq7/qRoURcmZLhL9caC8lLIu8B3wMGOKbI7NMaNIKRdLKX2klD7u7u46CCtvrCpUwNbHhwfr1xfeeep1ydwK2kzJsqLVKL0UWnk5eLGq6yqquVZj0p5J/Hz+Z51eX1GUvCtwopdSxkgp4zK/3wpYCCHc0N7Bl8tyqCdwu6Dt6YNT31dIvX6DhCP6GZlSKGW3opWOC61crF1Y2nEpvuV8mXF4BnOOzVHDLxXFCAqc6IUQpYUQIvP7xpnXjACOAJWFEBWEEJaAH1Aoq2rsO3bEzN7etB/KZufJFa30UGhlbW7NHN85j+bImXpgKqnpRWSKaEUxEXkZXrkG+AeoKoQIFUL4CyHGCSHGZR7yCnBGCHESmAf4Sa004HUgEDgPrJNSntXPj1EwZjY22onOtgcW3YnOCkLPhVYaMw0fNP2ACfUnsOXqFib8OUEtQK4oBiQKY7+0j4+PPHr0qEHbTLp4iWu9elFy8mRcR44waNuFSnw4bH8PTv8C7tWh53dQrpHOLr/h8gY++ecTarrWZH67+ThbO+vs2opSnAkhjkkpfbLbV2wrY59kXbUKNg0bErV2LTKjGPcjP1rRSj+FVi9Xfplvfb/lUtQlhm4byp24Ozq5rqIoOVOJPgtnPz9Sb9wg/u9/jB2K8T0stGo0SueFVm282rC4w2IiEiMYvG0wwQ9MYGI5RSnEVKLPwr5TRzQuLkStWWPsUAoHK3voNhNGbgcLG50WWjUo1YDlnZeTITMYtn0Yp+6f0kHAiqJkRyX6LMwsLXHq04e43btJvaO6FB7xagrjDui80KqqS1VWdl6JvYU9o3aM4u/bf+soYEVRslKJ/glO/fuDlEStW2fsUAqX7AqtVveDBwVbYaqcQzlWdlmJp70nr+96nZ3XdT8Pj6IUdyrRP8HS04MSrVrx4Jf1yBT9zONepGUttAo5oO27P7QYCvAA293WneWdllPDtQaT9k7it8u/6TBgRVFUos+G8+BBpIeHE7N9u7FDKZweFVodhHKNYds7sKxTgQqtHK0cWdxhMU3LNOWjvz9i1blVOgxYUYo3leizYffii1hWqEDkylXFY/6b/HIuD4M3QO9FEHEZFrWEPV/lu9DK1sKW79p+R3uv9nx95GsWnlyoPn9F0QGV6LMhzMxwHjKYpDNnSPxXTbP7TEJAXT8YfwSq94Q9XxZoRStLjSXftP6GnhV7Mv/EfGYfm62SvaIUkEr0OXDq1QszBwciV640dihFgw5XtDI3M+ezFz/Dr6ofK86u4LODn6nJ0BSlAFSiz4GZnR1Or7xCbFCQGmr5PHS0opWZMGNKkyn41/Lnl0u/8MGBD0jLMKG1fRXFgFSifwaXQQO1Qy1XrzZ2KEVLditabRjz3IVWQggmNpzIhPoT2Hx1M5P3TVYzXypKPqhE/wwWHh7Yt29P1LpfyEhQsy0+t6wrWp3ZkO9CqzF1xjC50WSCrgfx5u43SU5P1lPAimKaVKLPhcvwYWRER/NggxrbnS86KrQaUmMIHzX7iAO3DvD6rtfVNMeK8hxUos+FbYMG2NSrR+Ty5cg01UecbzootOpbpS+ft/icw3cP8+rOV4lPjddjwIpiOlSizwPXUf6k3rpF7I4dxg6laHus0KpJvgqtelbsyVctv+Lk/ZOMCRpDTEqMHgNWFNOQlxWmlgkhwoQQZ3LYP0gIcSrz9bcQom6WfSFCiNNCiBNCCMOuJKJDJdq2xdLbm4ily9SYbl14ckWr5yy06lyhM7N8Z3Eu4hyjd4wmOjlazwErStGWlzv6FUDnZ+y/BrSWUtYBPgMWP7G/jZSyXk4rnxQFwswMlxEjSDp7loRDh40djmkQAur2h/GH81Vo1c6rHXPbzOVK1BX8A/2JTIrUc8CKUnTlmuillPuAHP8vklL+LaV8uNDqQcBTR7EVKo4v9ULj6krE0qXGDsW0ZFto9V6eCq1aebbiu7bfERITgn+gP+GJ4QYIWFGKHl330fsD27K8l8AOIcQxIcSYZ50ohBgjhDgqhDh6//59HYdVcGZWVrgMGUz8/v0kXcj/5F1KDh4rtFoIC5rBldwLrZp7NGd+u/ncirvFyMCR3E8ofP92FMXYdJbohRBt0Cb6d7NsflFK2QDoAowXQrTK6Xwp5WIppY+U0sfd3V1XYemU84ABmNnZEb5okbFDMU2PCq22g4U1/NQHNoyFhGd3yzQp04QF7RZwN/4uIwNHci/+noECVpSiQSeJXghRB1gC9JJSPip/lFLezvwaBvwGNNZFe8aicXTEedAgYrcHknz1mrHDMV0PV7RqNRnOrIfvG8Hp9c8stPIp7cOiDosISwhjROAI7sbfNWDAilK4FTjRCyG8gA3AECnlpSzb7YQQ9g+/BzoC2Y7cKUpchg9DWFkRsfjJZ86KTplbQdupmYVW5eFXf1jdH6JDczylfsn6LO64mKikKEZsH8GdODVHkaJA3oZXrgH+AaoKIUKFEP5CiHFCiHGZh3wEuAILnhhGWQo4IIQ4CRwG/pBSFvmVPMxdXHDu34/ozZtJCc056Sg6Uqom+AdBp+kQsh/mN4HDATkWWtV1r8uiDot4kPyAEYEjuB1328ABK0rhIwrjuHAfHx959GjhHXafeu8ewe074NjnZcpMm2bscIqPqBDYPBGu7tYWXPX8DtyrZnvomfAzjAkag72FPcs6L8OjhIdhY1UUAxNCHMtpGLuqjM0Hi1KlcHz5ZaJ/3UDqPfXgz2CcvWHIb/DSQgi/BAtb5FhoVcutFgEdA4hLjWPk9pHcirtl+HgVpZBQiT6fXEePRkpJhBqBY1hCQL0BmSta9fiv0Cr06b8Aa7rWfJTsR2wfQWis6mpTiieV6PPJ0tMDp5dfJuqX9aTeVv3ABlfCHV5ZBgP+B8kxsKR9toVWNVxrENAxgPjUeEYGjuRm7PPNmqkopkAl+gJwGzcWAYT/sNDYoRRfVTtrJ0lr5A+Hfsi20KqGaw2WdFxCQloC/oH+KtkrxY5K9AVgUbYsTn378uC330i5qZKH0Vg7QLdZMCLnQqvqrtUJ6BCgkr1SLKlEX0CuY8cizMwIX/CDsUNRyjeDsfuh1TvZFlo9mexVn71SXKhEX0AWpUriPMCP6I0bSb6mqmWNzsIa2n6QY6HVw2T/sM9eJXulOFCJXgdcR49GWFtzf+48Y4eiPPSMQqvqrtVZ0nEJ8anx+Af6q6GXislTiV4HzN3ccB0+jNjt20k8fdrY4SgPmWmg2Wvw2j/g2Qi2vg3LO8P9i9o7ezXOXikmVKLXEZeRI9E4OxM2a7Zahaqwya7Qau/X1HCsREDHAGJTY/EP9Fdz4ygmSyV6HdGUKIHbq+NIOHiQ+L/+NnY4ypMeFVodhmrdYfcXsLg1NRITCOgQQExKjJr1UjFZKtHrkJOfHxYeHoTNmoXMYdItxchKlIS+y2HAWkh8AEvaU/PIKha3/paY5BhGbFfJXjE9KtHrkJmlJe5vvkHy+fPE/PGHscNRnqVqFxh/CHxGwqEfqPU/fxZV9+dB8gO1eIliclSi1zGH7t2xrlGDsFmzyUhMNHY4yrNYO0D32dpCK3Mram98i0UWFYhMjMB/h79K9orJUIlex4SZGaXef4+0u3eJWL7c2OEoeVG+WeaKVu9Q53wgC8MiCY+7w6gd/moNWsUk5CnRCyGWCSHChBDZrhAltOYJIa4IIU4JIRpk2TdMCHE58zVMV4EXZraNGmHfoQMRAUtIvRdm7HCUvHhYaDVmL/XsPFkYeoOwmJuM3DZUJXulyMvrHf0KoPMz9ncBKme+xgA/AAghXICPgSZo14v9WAjhnN9gi5KS77wNaWncnzvX2KEoz6N0LRi1k3q+0/ghLJJ7MTfx39iH8Hj1C1spuvKU6KWU+4DIZxzSC1gptQ4CTkKIMkAnIEhKGSmljAKCePYvDJNh6eWF85AhRP/2G4lnzxo7HOV5mGmg2XgajNrPAjMP7iZF4P9LJ8JDDxk7MkXJF1310XsAWacDDM3cltP2YsHt1XFonJ259/kXarhlUeTsjc/Q7cyvPIQ7MpXR24YR8ecn2a5opSiFma4Svchmm3zG9qcvIMQYIcRRIcTR+/dNo09UY29PyUmTSPz3X6I3bjJ2OEp+CEGjFu/xfetZhFpaMSp4NZGLs1/RSlEKK10l+lCgXJb3nsDtZ2x/ipRysZTSR0rp4+7urqOwjM+x90vY1K1L2MyZpMfEGDscJZ8av9CJ7zou4qaVLaOs4oha1hG2vw8p8cYOTVFypatEvwkYmjn6pikQLaW8AwQCHYUQzpkPYTtmbis2hJkZpT78kPTISO5//72xw1EKoGmZpnzXfgE3rKwY/UJVHhxeCAuawpVdxg5NUZ4pr8Mr1wD/AFWFEKFCCH8hxDghxLjMQ7YCV4ErQADwGoCUMhL4DDiS+fo0c1uxYlOrJk79+xH182qSLl40djhKATQr24x5bb7jmkxhTM1mRGss4aeX4bdxj61opSiFiSiMMy36+PjIo0dNqw80/cEDgjt3wbJCBcr//BPCTNWqFWUHbh3gjT/foJJjRQJsquH4zwKwdoIuX0GtPtpJ1BTFgIQQx6SUPtntU9nGQDROTpR8710S//2XB+vWGTscpYBaeLRgbpu5XIkOZmzyZWJGbAUnL+2KVmv8Hq1opSiFgUr0BuTYqxe2zZoSNnOWqpg1AS09W/Jtm2+5GHWRsSe/JWboBuj0JVzb99iKVopibCrRG5AQgjIff4xMSeHel18aOxxFB1p5tmKO7xwuRF1g3K7xxDYc+sSKVl3g/iVjh6kUcyrRG5iltzdur71KbGAgsX/uNnY4ig74lvNlduvZnI88z7igccTauWauaPUD3L8AC1+EvV+rQivFaFSiNwLXkSOxqlyZu598osbWm4g2Xm2Y1XoW5yLOMW7nOOJS46HeQHj9SJYVrXwh9JixQ1WKIZXojUBYWlLmyy9JCw/n3ldfGTscRUfaerVlpu9MzoWfY+zOscSlxP23opXfGkiMgiXtVKGVYnAq0RuJTe1auPr7E/3rBuL27zd2OIqOtPNqx8zWTyR7gGpd/1vR6uACVWilGJRK9Ebk9vp4LCtV5M6HH5EeG2vscBQdaVc+h2SfdUUrjZUqtFIMRiV6IzKztKTsl1+SFhbGvRkzjB2OokM5Jnv4b0Wrlm/D6V/g+0Zwej0UwuJFxTSoRG9kNnXq4DpqFNG/biB2505jh6Po0JPJPjYly19tFtbQ7kMYs1cVWil6pxJ9IeD++nisa9TgzocfkWYiUzQrWlmT/bigcY8ne3i0otV/hVZNVaGVonMq0RcCwtKSst98TUZCArenTqUwzj+k5F+78u20o3EizjE2aCwxKU8Mqc1c0UpbaOWjCq0UnVOJvpCwqliRku+8Q/y+/UStWWPscBQda+fVjlm+szgfeZ4xO8YQnRz99EHO3v8VWoVfzCy0+kYVWikFphJ9IeI8aCB2LVsS9tXXJF1Ud3Ompq1XW+b4zuFi1EXGBOWQ7IXQFlqNP5xZaPW5KrRSCkwl+kJECEHZGdMxc7Dn1nleTHMAACAASURBVP/9HxkJCcYOSdEx33K+zG0zl8tRlxm9Y3T2yR7+K7QasFZbaLW0PWyfogqtlHxRib6QMXd1xePrr0m5do27n39h7HAUPWjl2Yq5beYS/CAY/0B/opKicj64ahdtoVXDEXBwviq0UvIlrytMdRZCXBRCXBFCvJfN/jlCiBOZr0tCiAdZ9qVn2adWyM4Du2bNcB03lugNG4jepD4yU9TSsyXftf2OkJgQ/Hf4E5EYkfPBjwqttmUptHpVFVopeZbrClNCCA1wCeiAdrHvI8AAKeW5HI6fANSXUo7MfB8npSzxPEGZ4gpTz0umpXF9+HCSzp2nwrr/YVWpkrFDUvTg4J2DTNg1AY8SHizptAQ3G7dnn5CaBPu+gb++1a5o1fVrqPmyWtFKKfAKU42BK1LKq1LKFGAt0OsZxw8A1LCRAhLm5njMmo2ZrS2hE94gPS4u95OUIqdpmaYsaL+A2/G3GbF9BGEJuSxI82Sh1fqRsGYARN8yTMBKkZSXRO8B3MzyPjRz21OEEOWBCsCfWTZbCyGOCiEOCiFeyqkRIcSYzOOO3ldFQwBYlCqJx+xZpNy4wZ0pany9qWpUuhEL2y8kLCGMEdtHcDf+bu4nPSy06vgFXN2jXdHqyBJVaKVkKy+JPru/CXPKOH7AeillepZtXpl/TgwEvhVCVMzuRCnlYimlj5TSx93dPQ9hFQ92jRtT8q23iN2xg8hly40djqInDUo1YHHHxUQlRTF8+3BCY/MwFYKZBpq/nllo1RD+mAQruqpCK+UpeUn0oUC5LO89gds5HOvHE902UsrbmV+vAnuA+s8dZTHnMnIE9h07EjZrFnEH/jJ2OIqe1HWvS0CnAGJTYhm+fTjXY67n7USXCjDkd+i1AMLOawut9n0D6an6DVgpMvKS6I8AlYUQFYQQlmiT+VNDQYQQVQFn4J8s25yFEFaZ37sBLwLZPsRVciaEoOz0L7GqVIlbb71F8rVrxg5J0ZOarjVZ1mkZqRmpDN8+nOAHwXk7UQioPyhzRatu8OfnsKg13FKFVkoeEr2UMg14HQgEzgPrpJRnhRCfCiF6Zjl0ALBWPt6RXB04KoQ4CewGZuQ0Wkd5NjM7OzwXLEBoNIS+Nl7NX2/CqrpUZVmnZQCM2D6Ci5EX835yiZLQd0XmilaRsEQVWil5GF5pDGp4Zc7iDx/mxkh/7Jo1o9wPCxDm5sYOSdGT6zHX8Q/0JzEtkYXtF1LbvfbzXSApGnZOg6PLwKk89PgWKrbVS6yK8RV0eKVSiNg1bkzpDz8kfv9+7n35pRqJY8LKO5Tnxy4/4mDpwOig0Ry9+5w3P9aO0H0ODN8KGgtY1VsVWhVTKtEXQc79++HiP5Ko1WuIXPGjscNR9MijhAcrOq+gpG1JXt35Kn/f+vv5L+L9Ioz7C1pOgtPrYH5jOPOrWtGqGFGJvogqOWkS9p06Efb118QEBRk7HEWPStmVYnmn5ZR3KM/rf77Oruv5mOvGwhrafQRj9oCjpyq0KmZUoi+ihJkZZb+agU2dOtx++x0SjqnRFabM1caVpZ2WUt21OpP2TmJz8Ob8Xah0bfDfCR0/V4VWxYhK9EWYmbU1nj8swKJMGW6++pqaw97EOVo5EtAhAJ9SPkw5MIX/Xfhf/i6kMYfmE1ShVTGiEn0RZ+7igtfSJZhZW3Nz9GhSQtWf4qbM1sKW+e3n4+vpy+eHPmfJ6SX5fyD/qNBqviq0MnEq0ZsACw8Pyi0JICMpiZv+/qSFhxs7JEWPrDRWzG4zm64VujL3+FzmHJuT/2QvBNQfrF3RqmpXVWhlolSiNxHWVapQbuFCUsPCuDHSn7SoZyxmoRR5FmYWTG85nf5V+7P87HI++ecT0jPScz8xJ/aloN+P4LdaFVqZIJXoTYhtg/qUWzCflJAQbvqPIj0mxtghKXpkJsyY2mQqo2uP5tfLv/LOvndISS/gQuLVumWuaDU8c0WrZhD8Z66nKYWbSvQmxq5ZMzy/m0fS5cvcHD1GzWNv4oQQvNHgDd72eZug60G8tus14lMLeBeuCq1MTpGZAiE1NZXQ0FCSkpKMFFXhZG1tjaenJxYWFo9tj9mxg1v/9xY2tWtTLmAxGnt7I0WoGMqm4E189NdHVHepzvz283Gxdin4RVOTYN/X8NdcsHGGLl9Dzd5qRatC6FlTIBSZRH/t2jXs7e1xdXVFqH9kAEgpiYiIIDY2lgoVKjy1P2bHDm69NQnrmjXwCghA4+BghCgVQ9pzcw9v732bMnZlWNRhEWVLlNXNhe+ehk0T4Pa/UKULdJsFjtmuP6QYiUnMdZOUlKSS/BOEELi6uub4V45Dx454zv2WpHPnuTHSn/QHD7I9TjEdvuV8WdxhMRFJEQzZOoRLUToaG68KrYq0IpPoAZXks5HbZ2Lfrh2e8+aSfPEi14cMJTUslzVJlSKvQakG/NhZOwfS8G3Dn38ytJxkLbTyaKAKrYqQIpXolfyxb9OGcosXkXLrFtcHD1FFVcVAZefKrOq6CjdbN8YGjWVHyA7dXdylAgzdqAqtihCV6J+DEIJJkyY9ej9z5kymTZv26P3ixYupVq0a1apVo3Hjxhw4cMAIUWbPrlkzyi9fRnp0NNcHDiT58mVjh6ToWdkSZVnVZRU1XGvw9t63+fn8z7q7uCq0KlLylOiFEJ2FEBeFEFeEEO9ls3+4EOK+EOJE5mtUln3DhBCXM1/DdBm8oVlZWbFhwwbCs6k83bJlC4sWLeLAgQNcuHCBhQsXMnDgQO7evWuESLNnU7cu5VeuBCkJGTiI+MOHjR2SomeOVo4EdAygTbk2zDg8g9lHZ5Mhddivnl2hVeBUVWhVyOS6PJEQQgPMBzqgXSj8iBBiUzZLAv5PSvn6E+e6AB8DPoAEjmWeW6CyzU82n+Xcbd0WA9Uo68DHPWo+8xhzc3PGjBnDnDlz+OKLLx7b99VXX/HNN9/g5uYGQIMGDRg2bBjz58/ns88+02msBWFdtQrea9dwY8xYbvqPouxXM3Do2tXYYSl6ZG1uzWzf2Uw/PJ3lZ5dzJ/4On7f4HCuNle4aqdYNvFtA0Mfwz/dwfrNa0aoQycsdfWPgipTyqpQyBVgL9Mrj9TsBQVLKyMzkHgR0zl+ohcP48eP5+eefiY6Ofmz72bNnadiw4WPbfHx8OHv2rCHDyxMLDw+8f/4J67p1uPXWJMIDAtRKVSZOY6ZhapOpvNXwLbaHbGfMjjFEJ0fnfuLzsHbUJveshVa/v6YKrQqBvCw46gHczPI+FGiSzXF9hBCtgEvA/0kpb+ZwbraDb4UQY4AxAF5eXs8MKLc7b31ycHBg6NChzJs3Dxsbm2ceK6UstCOFNE5OeC1dyp333+f+rNmkXL1GmU+mISwtjR2aoidCCEbUGkEZuzJMOTCFwVsHs6DdAso5lNNtQw9XtNr7lbbQ6vIOVWhlZHm5o8/uv8yTt3+bAW8pZR1gJ/Bwfbu8nKvdKOViKaWPlNLH3d09D2EZz8SJE1m6dCnx8f/1Q9aoUYNjTyz+cfz4cWrUqGHo8PLMzMqKsrNm4TZ+PNG//cb1kSPVZGjFQOcKnQnoGEBUchSDtg7iRNgJ3TdiYQ3tP4axe8HBA9aPUCtaGVFeEn0okPVXvidwO+sBUsoIKWVy5tsAoGFezy2KXFxc6NevH0uXLn20bfLkybz77rtEREQAcOLECVasWMFrr71mrDDzRAiB+4TXKTtzJkmnThPS5xWSzj35+EUxNQ1LNeSnLj9hb2mPf6A/20O266eh0rVh1K4nCq2WqkIrA8tLoj8CVBZCVBBCWAJ+wKasBwghymR52xM4n/l9INBRCOEshHAGOmZuK/ImTZr02Oibnj17MnLkSJo3b061atUYPXo0P/30E2XKlHnGVQoPx+7dKP/zT8iMDEIGDCR6cz6XqlOKDG9Hb37q+hO13Grxzt53WHhyoX6e1TwqtPo7s9DqLVjRDcLVEF+DkVLm+gK6ou17DwamZm77FOiZ+f104CxwEtgNVMty7kjgSuZrRF7aa9iwoXzSuXPnntqmaOnys0kND5chgwbLc1WryTuffS4zkpN1dm2lcEpOS5bv73tf1lpRS767712ZlJakv8YyMqQ8vkrK6eWk/NRdyr1fS5mWor/2ihHgqMwhpxaZSc3Onz9P9erVjRRR4abrz0amphI2cyaRP67EunZtPObMwdJTTWBlyqSULDm9hHn/zqOOex3mtpmLm42b/hqMvQfbJsO536FULeg5Dzwa5n6ekiOTmNRMMRxhYUGp99/HY95cUkJCuPbyy8Tu2mXssBQ9EkIwus5oZvvO5nLUZfy2+HE+4nzuJ+ZX1kKrhAhVaKVnKtErOXLo2JEKG37Fslw5Qse/zp1p08hITDR2WIoedSjfgZVdViKEYOi2oQSG6PmR2sMVrRoM0xZaLWgGwbv122YxpBK98kyW5cpRfs1qXEaO5MHa/3Htlb4kXbhg7LAUParmUo013dZQzaUab+99m3nH5xVsPdrcPFVo9ZIqtNIxleiVXJlZWlJq8juUW7qE9JhorvXtR/jCRci0NGOHpuiJm40bSzstpU/lPgScDuCN3W8QmxKr30YfFlq1eAtOroX5jeHMBiiEzxGLGpXolTwr8eKLvLBpE/bt23H/228JGTiI5KtXjR2WoieWGks+bvYxHzT5gL9v/c2APwZwJeqKfhvNrtBq7UBVaFVAKtE/hxIlSjy1bdq0acycOROA4cOH4+HhQXKytnYsPDwcb29vAEJCQrCxsaFevXqPXitXrnx0nX///RchBIGBj/eJajQa6tWrR61atejRowcPjLxKlLmzM55z5uAxexap169z7aXe2rv7VDUXuSkSQtC/Wn+WdFpCXEocA7cO1H+/PTxeaBW8GxY0VYVWBaASvY5pNBqWLVuW7b6KFSty4sSJR6+hQ4c+2rdmzRpatGjBmjVrHjvHxsaGEydOcObMGVxcXJg/f75e488rh65deWHLZkq0acP9b7/lWt9+JJ4+Y+ywFD1pWKoh63qso4pzFd7e+zazjs4iLUPPXXdZC63K1leFVgWQl0nNCp9t72kXK9al0rWhy4wCX2bixInMmTOH0aNH5/kcKSXr168nKCiIli1bkpSUhLW19VPHNWvWjFOnThU4Rl0xd3fHc+63xAQFce/Tzwjp3x/nAQNwn/gmGnt7Y4en6FhJ25Is77Scr458xYqzKzh1/xQzW8/E3VbPc1O5vKBd0erEagicAj+8CK0nw4tvah/eKrlSd/Q65uXlRYsWLVi1atVT+4KDgx/rutm/fz8Af/31FxUqVKBixYr4+vqydevWp85NT09n165d9OzZU+8/w/Ny6NCBF/7YgvOAAUStXk1wl65Eb96spj42QRYaCz5o+gHTW07nfOR5+m7uy5G7R/TfsBBQf1DmilZd4M/PYLEv3Dqu/7ZNQNG8o9fBnbc+TZkyhZ49e9KtW7fHtj/sunnSmjVr8PPzA8DPz49Vq1bx8ssvA5CYmEi9evUICQmhYcOGdOjQQf8/QD5oHBwo/eEHOPbuzd1p07j9zmSi1qyl1NQp2NQ03rTSin50f6E71Zyr8X97/o9RO0bxat1XGV17NBozjX4bflhodeEP7eLkS9pB09egzRSwtNNv20WYuqPXg0qVKlGvXj3WrVuX67Hp6en8+uuvfPrpp3h7ezNhwgS2bdtGbKx2KNvDPvrr16+TkpJSaProc2JTqybe/1tL6c8+JSUkhJBX+nL7gw9IDQszdmiKjlVyrsTa7mvp7N2Z+SfmM3bnWMITn15mUy9UodVzUYleT6ZOnfpoNM6z7Ny5k7p163Lz5k1CQkK4fv06ffr04ffff3/sOEdHR+bNm8fMmTNJLeQjXIRGg3PfvlQM3I7LsGFEb9xEcOcu3P/uezLiVYm7KbGzsGNGyxlMazaNE2EneGXTK/x962/DNJ5todV4VWiVDZXon0NCQgKenp6PXrNnz87x2Jo1a9KgQYPHtj3ZRz9v3jzWrFlD7969HzuuT58+rF69+qlr1q9fn7p167J27Vrd/EB6prG3p9R771Lxjy2UaNWK8PnzudKpM5E//UxGSoqxw1N0RAhBnyp9WN1tNU5WTozdOZbZR2eTmm6gG5LHCq3WqEKrbKjZK01AUflsEk+cIGzWbBKOHMG8bBncx4/HsVcvhHnRfFSkPC0xLZGZR2ay7tI6arrWZEbLGXg7ehsugLunYePrcOcEVO0K3WaBQ1nDtW9EavZKpVCwqVcPr5U/Um7pEsxdXLkz9QOCu3Tlwa+/qoIrE2FjbsOHzT5kju8cQuNC6belH79c+sVwI7CeLLRSK1oBKtErBiaEoMSLL+L9yzo8F8xH4+CgTfiduxC1Zg0ZSUnGDlHRgfbl2/Nrj1+p516PT//5lDf+fMNwD2qzK7T6sTuE63n6hkIsT4leCNFZCHFRCHFFCPFeNvvfEkKcE0KcEkLsEkKUz7IvXQhxIvO16clzleJJCIF927Z4r/8Fz4U/oHFz5e4nn3KlfQfCFweQHh1t7BCVAiplV4qFHRbybqN3+fv23/Te2Nsw0yc89LDQqtd8uHcGfmgO+2eBoZ4dFCK59tELITRolxHsgHax7yPAACnluSzHtAEOSSkThBCvAr5Syv6Z++KklE9PEvMMqo/++ZjCZyOlJOHwESICAog/cABha4vTyy/jMnQIll5exg5PKaCrD64y9cBUzkScobN3Z6Y0mYKztbPhAoi9B9vegXMboVTtzBWtGuR+XhFS0D76xsAVKeVVKWUKsBbolfUAKeVuKWVC5tuDgGdBAlaKHyEEdk0a47UkgAq//4ZDx45E/e9/BHfqzM1XXyPur7+QxbyftSh7wekFVnVdxYT6E9h5YycvbXyJwJBAw/Xd25eCfiuh/88Qf19baLXjA0hJyP1cE5CXRO8B3MzyPjRzW078gW1Z3lsLIY4KIQ4KIV7K6SQhxJjM447ev38/D2Eppsq6WjXKzphOpV07cR03lsRTp7jpP4qr3boT+eOPpBt5Bk8lf8zNzBlTZwxru62ltF1p3t77Nm/teYv7CQb8/71698xCq6Hw93fwQzO4usdw7RtJXhK9yGZbtr+GhRCDAR/gmyybvTL/nBgIfCuEqJjduVLKxVJKHymlj7u7nidJyqe7d+/i5+dHxYoVqVGjBl27duXSpUucPXuWtm3bUqVKFSpXrsxnn3326E7l3r17dO/enbp16z46R8kbi5IlKfnmm1Ta/Sdlv/kajYMD96bP4HKr1tyaPJn4w4fVfDpFUFWXqvzc9WcmNpjIvtB99Pq9F+suriNDGugvNhsn6DEXhv8BQgMre2kLrRKjDNO+MUgpn/kCmgGBWd6/D7yfzXHtgfNAyWdcawXwSm5tNmzYUD7p3LlzT20zpIyMDNm0aVP5ww8/PNr277//yn379skXXnhBBgYGSimljI+Pl507d5bff/+9lFLKMWPGyG+//fbROSdPntR5bMb+bAwp8cIFeeeTT+WFhj7yXNVq8nL7DjJs/nyZEhpq7NCUfLj24JocuX2krLWilhyydYi8FHnJsAGkJEgZ9LGU05yl/LqSlGc2SJmRYdgYdAQ4KnPIqXl5GGuO9mFsO+AW2oexA6WUZ7McUx9YD3SWUl7Ost0ZSJBSJgsh3IB/gF4yy4Pc7OT2MParw19xIVK365ZWc6nGu43fzXH/n3/+ybRp09i3b99j25cuXcrevXsfW0QkODgYX19fbt68Sc+ePRk2bBh9+vTRabxZmcLD2OeVkZhIbFAQDzb8RsLBgwDY+vjg0LMHDp06oXF0NHKESl5JKdkUvImZR2cSmxLL4OqDebXeq9hZGHCSsjsnYdME7deq3aDbzCJXaFWgh7FSyjTgdSAQ7R37OinlWSHEp0KIh3PmfgOUAH55YhhldeCoEOIksBuYkVuSL6zOnDlDw4YNn9p+9uzZp7ZXrFiRuLg4YmJiGD9+PP7+/rRp04YvvviC27dvGypkk2ZmY4Njz56UX7Gcijt34j7xTdIiIrj70cdcatGSm+NeJXrzZtLj1Nw6hZ0Qgl6VerH5pc28VOklfjz3Iz1/68nWq1sN1zVXpi6M+hM6fAbBu7SFVkeXmUyhlZoCIY/mzZvHtWvXmDNnzmPb/+///o8KFSrwxhtvPLbd2dmZGzduYG9vT2RkJNu3b2fbtm3s2LGDM2fOoMvnEMb+bAoLKSVJZ84Ss3UrMdu3k3bnDsLSErsXX8S+Qwfs27ZB4+Rk7DCVXJy6f4rPD37O+cjz1C9Zn/cav0cN1xqGCyDyKmx+E67tg/IvQo954FbJcO3nk5oCQQdq1qzJsWPHst3+5C+lq1evUqJECewzV1lycXFh4MCBrFq1ikaNGj3V/aPohhACm9q1KPXuZCrt2kn51T/jPMCPpIsXuDNlCpdebMH1ocOI/PFHUm7ezP2CilHUca/Dmm5rmNZsGtdjruO3xY8P//qQe/H3DBOAywswdBP0/N5kCq1Uos+jtm3bkpycTEBAwKNtR44coXLlyhw4cICdO3cC2oVC3njjDSZPngxo+/YTErRjdWNjYwkODsZLFQDpnTAzw7ZBA0q9/z6Vdu3C+5d1uI4aRXpUJPemzyC4Q0eCu3bj3oyviP/nHzWbZiGjMdPQp0oftvTewrCaw/jj6h/0+L0H80/MJyHVAGPfhYAGQ7QrWlXpBLs+hcVtiuyKVqrr5jncvn2biRMncuzYMaytrfH29ubbb78lKSmJCRMmcOfOHdLT0xkyZAgfffQRQgi++eYbli9fjrm5ORkZGYwYMYJJkybpNK7C8NkUJSk3bhC3Zw9xe/eRcPgwMjUVYWODbeNGlHjxReyaNcOyUiWEyG5ksWIMN2NvMu/4PLaHbMfF2oXRtUfTr2o/LDWWhgng/Gb4422ID4Nm48F3CljaGqbtPHpW141K9CZAfTb5lxEfT/yhw8T/9RfxBw6Qcv06ABp3N+yaNMW2cSPsGjfGonx5lfgLgVP3TzH3+FwO3z1MGbsyjKs7jh4Ve2BhZoBFwhMfQNBHcPxHcPbWjsV/wVf/7eaRSvQmTn02upN66xbxBw8S/89B4g8dJP2+dsZFc3d3bHwaYtugIbYNG2BVpYqaR99IpJQcvHOQecfncSbiDB4lPBhbZyzdK3Y3TMK/tl/7sDYyGOoNhk6fg40B5+3JgUr0Jk59NvohpSTlWggJR46QcPgwCcePk3bnDgDC1habOnWwqVcXmzp1salTG3M3NyNHXLxIKdkXuo8FJxdwLuIcZe3KMrzWcHpX6o21ubV+G09NhL1fwV/zwNYVun4NNV7S9u0biUr0Jk59NoaTevs2CceOk3jiBIn//kvSxYuQng6Aedky2NSqjXXNmljXqol1jRqYOxv/Ts/UPUz4AacDOHn/JC7WLgyqPoh+VfrhZK3n4bR3TsGm1wtFoZVK9CZOfTbGk5GQQNL58ySeOk3iqZMknT1H6o0bj/ably6NdfXqWFevhlWVqlhXq4pFuXIIjcaIUZsmKSVH7x1l6Zml/HXrL6w11vSq1ItB1QdRwbGC/hpOT4OD82H3l6CxhA6fQIPhYGbYQY0q0Zs49dkULunR0SSdO0fSufMknde+Uq5de1RlKaytsapYEavKlbGqXAnLF17AqmJFLDw81C8AHbkcdZmfzv/E5uDNpGak0qxMM/pX609rz9aYm+np2UpEsLbvPmQ/lG+hfVhrwEIrlehNnPpsCr+MpCSSrwSTfPECyZevkHz5MsmXLpGWZUpuYWmJpbc3li+8gGUFbyzLl8fK2xuL8uXRODmpUT/5EJ4YzobLG1h3cR33Eu5R0rYkvSr2onfl3pSzL6f7BqWEf1dB4AeQlgS+70LzN0Cj/4fEKtHriEajoXbt2o/e+/n58c4779C4cWPmzJlDq1atAOjYsSOjR4+mb9++eHt7Y29vj5mZGaVKlWLlypWULl1ap3EVhs9GyZ/0mBiSg4NJvnKFlKvXSLl2jeRrV0kNvfWo7x/AzN4eSy8vLLzKYelZDgtPTyw8PbD09MS8TBnMLA00nryISstIY+/Nvay/vJ6/bv2FRNK4dGO6v9Cd9uXbY29pr9sGY+/C1nfg/CaDrWilEr2OlChRgri4uKe2Hzp0iFGjRnH8+HHWr1/PihUrCAzUro3p7e3N0aNHcXNzY8qUKcTFxTFv3jydxlUYPhtFt2RKCim3bpESEkLK9euk3rhJyo0bpNy8QertO5CapRxfCMzd3bEoWxaLsmUwL1MGi9JlsChTGvPSZbAoVRKNqyvCwH3GhdXd+LtsvLKRTcGbuBF7A0szS1qXa01H74608miFrYUOC6EMWGj1rERfJAcC3/3yS5LP63aaYqvq1Sg9ZUq+zm3SpAnNmzdn2rRprF69mqCgoGyPa9Wqlc6TvGKahKUlVhUqYFXh6YeIMj2dtHv3SLkZSurt26TeuqV93blD4tmzpAXtRKY+MS+Lubn2l0HJkpg/fLm7Y+7uhrm7OxpXV8zd3DB3cUFYGGAsuhGVtivN2LpjGVNnDGfCz7Dl6hYCQwIJuh6ElcaK5mWb06ZcG1p6tsTNpoBDZqv3AO+W2kKrv7/TJn4jFFoVyURvLImJidSrV+/R+/fff5/+/fsDMH36dMqVK8fEiROpVCn7BzBbtmx5rOtHUfJDaDSZd+/ZD+OTGRmkR0WReucuaXfvkHrvHmn3wki7d5e0+/dJvnaV+IMHyYiNzfZ8jaOjNvG7uqJxcUHj4oy5szMaZxc0zs5onJ3QODlh7qT9Kmxti+TzAyEEtd1rU9u9NpMbTebfsH/ZeWMnO6/vZPfN3QDUdK1J87LNaVa2GXXd6+ZvygUbJ23XTe2+sPkN7YpW9QdDR8MVWqmum+eQU9cNwO+//85rr71Go0aN2Lhx46PtD/voNRoNderUYd68eTjpeKrcwvDZKEVPRlISaeERpN0PIz0igrTwcNLCalHVggAACORJREFUI0iPjCAtIpK0iHDSI6NIj4wkPTpa+6AxG8LCAjMnR+0vCAdHNA4OaBwdMHNwRGNvj5mDvfarfebXEvZo7EtgVkL7ElZWheoXhZSSS1GX2Bu6l32h+zgTfoZ0mY61xpo67nVoUKoB9UvWp7Zb7efv209NhD0ztHf3tq7Q9Ruo0UsnhVaqj15Hckr08fHx1K9fn02bNjFy5Eg++OCDR2vDZu2j15fC8Nkopk2mpZEeE0P6gwekR0VpXw8e/PeKjiE9Olr7io0hIzqG9JiYHP9qeIy5ORo7O8xyetnaal92tpjZ2CBsbDCzscXM1ibL+8zvrW0ws7FGWFsjLCx08gskLiWOI3ePcOjuIY7fO87FqIuP1rf1dvCmhmsNqrpUpZJTJao4V6GUbanc29XDilYF7qMXQnQG5gIaYImUcsYT+62AlUBDIALoL6UMydz3PuAPpANvSCkD8/lzFFqffvop/fr1o1q1aixYsID+/fvTtm1brK31XIatKAYizM0xd3HB3MXluc6T6elkxMc/SvoZcXGkx8aRERdLelwcGbFxZMTFkREfT0Z8HOnx8drjY2NJvXeXjPgE7b6EBEhLe76gzcwQ1taYWVlpv1prfwGYWVpqfxFYWWr3WVpp/6qwstTus7RCWFpmef1/e/cXI1dZxnH8+9uhswu1pVuarWxXpcSqIOrSNFr/xBgUu6BxvTCxxqS9wHCDEY2JwXhR5cZsQvwXCAkBFIkBtVLcoNFAi/GmFIpaLBbbxS66WO26/SPdnb87jxfvu+l0naHT7kxn+87zSSZ7ztkzO+/TZ/rMzHvOnGcJg9ks67PXoCXvJt9XYXxmgvHcq7w8+Qpjh3fzl9KvKGeglIEl2R76Lu+nf8WbWb2sn1XL30jf8itZtbSP3ktX0tvdy7LV15H5wi7YfTf87tuho9WNd8L6rS35otVZC72kDHAPcCMwATwnaXReS8BbgONm9lZJm4ER4LOSrgU2A+8E+oGnJL3NzGa5CM2fox8aGmLLli3s2LGDffv2ATA4OMimTZsYGRlh27Zt7Rqqc4uCMpkwlbN8+YL/lhWLVGZmqORy4TaTw/JVy4U8lVz+9LZ8HssXqORzWKEYfp8vYPk8lWKByrFpysUiVihQKRSwueVi8cyzmmpYBrwr3v7fNHAo3s50qgtOZKDSBbNdUMl0UcmsYlaGjY6QX3oXH//1s5Btbr/cRt7RvxcYM7O/AUh6FBgGqgv9MPDNuLwduFvhs8sw8KiZFYDDksbi39vdnOFfWLOztV+fDh48eMZ69Zk14+PjrRyScx1D2SyZbPaCtIO0SgUrlULxr76VSqe3Vy+Xy1Xby6d/Vy6Ry53iVO44M7lT5AvTFArTlEsFZksFZkvh7zBbQbnXsEy56UUeGiv0a4DqvmsTwPvq7WNmZUkngSvi9mfm3XdNrQeRdCtwK+AdmJxzbaWuLtTdDd3d7R5KUzQyGVTrqML8I7j19mnkvmGj2X1mtsHMNjSzcbZzznW6Rgr9BFB9UYgB4J/19pF0CXA5cKzB+zZsMZ4h1G7+b+KcO5tGCv1zwDpJayVlCQdXR+ftMwpsjcufAXZZqECjwGZJ3ZLWAuuAZ89noD09PUxNTXlhq2JmTE1N+dk9zrnXddY5+jjn/kXgt4TTKx80sxcl3QnsNbNR4AHg4Xiw9RjhxYC4388IB27LwG3ne8bNwMAAExMTTFZd7c+FF8CBgYF2D8M5t4hdNF+Ycs45V9/rfWHKL2fnnHOJ80LvnHOJ80LvnHOJW5Rz9JImgVfO8+6rgP80cTgXg06MGToz7k6MGToz7nON+S1mVvNLSIuy0C+EpL31DkikqhNjhs6MuxNjhs6Mu5kx+9SNc84lzgu9c84lLsVCf1+7B9AGnRgzdGbcnRgzdGbcTYs5uTl655xzZ0rxHb1zzrkqXuidcy5xyRR6SUOS/ippTNId7R5Pq0h6k6SnJR2Q9KKk2+P2lZKelHQo/uxt91ibTVJG0h8lPRHX10raE2P+aby6alIkrZC0XdJLMefvTz3Xkr4Sn9v7JT0iqSfFXEt6UNJRSfurttXMrYIfxPr2gqT15/JYSRT6qr62NwHXAp+L/WpTVAa+ambXABuB22KsdwA7zWwdsDOup+Z24EDV+gjw3RjzcULv4tR8H/iNmb0DeA8h/mRzLWkN8CVgg5ldR7hi7lwf6tRy/SNgaN62erm9iXCZ93WETnz3nssDJVHoqepra2ZFYK6vbXLM7IiZ/SEuv0b4j7+GEO9DcbeHgE+3Z4StIWkA+ARwf1wXcAOhRzGkGfNy4MOEy4BjZkUzO0HiuSZcPv3S2MToMuAICebazH5PuKx7tXq5HQZ+bMEzwApJVzb6WKkU+lp9bWv2pk2JpKuA64E9wGozOwLhxQDoa9/IWuJ7wNeASly/AjhhZuW4nmLOrwYmgR/GKav7JS0l4Vyb2avAXcDfCQX+JPA86ed6Tr3cLqjGpVLoG+5NmwpJbwB+AXzZzP7b7vG0kqRPAkfN7PnqzTV2TS3nlwDrgXvN7HpgmoSmaWqJc9LDwFqgH1hKmLaYL7Vcn82Cnu+pFPqm9qZd7CQtIRT5n5jZY3Hzv+c+ysWfR9s1vhb4IPApSeOEabkbCO/wV8SP95BmzieACTPbE9e3Ewp/yrn+GHDYzCbNrAQ8BnyA9HM9p15uF1TjUin0jfS1TUKcm34AOGBm36n6VXXf3q3ALy/02FrFzL5uZgNmdhUht7vM7PPA04QexZBYzABm9i/gH5LeHjd9lNCWM9lcE6ZsNkq6LD7X52JOOtdV6uV2FNgSz77ZCJycm+JpiJklcQNuBg4CLwPfaPd4Whjnhwgf2V4A/hRvNxPmrHcCh+LPle0ea4vi/wjwRFy+mtBsfgz4OdDd7vG1IN5BYG/M9+NAb+q5Br4FvATsBx4GulPMNfAI4ThEifCO/ZZ6uSVM3dwT69ufCWclNfxYfgkE55xLXCpTN8455+rwQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4nzQu+cc4n7HzOFPfMjdA0aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "annealings = \"NO LINEAR COS EXP\".split()\n",
    "\n",
    "a = torch.arange(0, 100)\n",
    "p = torch.linspace(0.01,1,100)\n",
    "\n",
    "fns = [sched_no, sched_lin, sched_cos, sched_exp]\n",
    "for fn, t in zip(fns, annealings):\n",
    "    f = fn(2, 1e-2)\n",
    "    plt.plot(a, [f(o) for o in p], label=t)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + list(pcts))\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04873f5470>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 1.1), sched_cos(1.1, 0.1)])\n",
    "plt.plot(a, [sched(o) for o in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the learning rate scheduler used in training loop\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4, 1e-2), sched_cos(1e-2, 1e-6)])\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, total_iters):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    :param total_iters: total_number of iterations\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "        \n",
    "        # learning rate annealing\n",
    "        current_iter = epoch*n_batches+i\n",
    "        \n",
    "        # learning rate anealing and visualization for later\n",
    "    \n",
    "        optimizer.param_groups[0]['lr'] = sched(current_iter/total_iters)*2    # this is the bias group\n",
    "\n",
    "        optimizer.param_groups[1]['lr'] = sched(current_iter/total_iters)      # this is the non-bias group\n",
    "\n",
    "        learning_rates.append(optimizer.param_groups[1]['lr'])\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))            \n",
    "            print(f'alpha_loc: {criterion.alpha_L}, alpha_conf: {criterion.alpha_C}')\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(val_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i % print_freq == 0:\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      loss=losses))\n",
    "\n",
    "    print('\\n * LOSS - {loss.avg:.3f}\\n'.format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-95801a5efb07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiBoxLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriors_cxcy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors_cxcy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Loss function to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# initialize model or load checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Training SSD300 with Kaiming_uniform_ and learning rate annealing\n",
    "global epochs_since_improvement, start_epoch, label_map, best_loss, epoch, checkpoint\n",
    "\n",
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "n_classes = len(label_map)\n",
    "\n",
    "# Training parameters\n",
    "checkpoint = None  # path to model checkpoint if consider resume training from there\n",
    "batch_size = 8\n",
    "start_epoch = 0    # start at this epoch\n",
    "epochs = 200       # total training epochs to run without early-stopping\n",
    "epochs_since_improvement = 0 # record the no. of epochs since last improvement\n",
    "best_loss = 100.   # assume a hight loss at first\n",
    "workers = 4        # number of workers for loading data in the DataLoader\n",
    "print_freq = 200   # print training or validation status every __ batches\n",
    "lr = 1e-3/2          # learning rate\n",
    "momentum = 0.9     \n",
    "weight_decay = 5e-4\n",
    "grad_clip = None   # consider clipping the gradient when using high learning_rate\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device) # Loss function to GPU\n",
    "\n",
    "# initialize model or load checkpoint\n",
    "if checkpoint is None:\n",
    "    model = SSD300(n_classes)\n",
    "    # Initialize the optimizer, with twice the default learning rate for biases\n",
    "    biases = list()\n",
    "    not_biases = list()\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param_name.endswith('.bias'):\n",
    "                biases.append(param)\n",
    "            else:\n",
    "                not_biases.append(param)\n",
    "    for param_name, param in criterion.named_parameters():\n",
    "        not_biases.append(param)\n",
    "        \n",
    "    optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    \n",
    "# move to  default device\n",
    "model = model.to(device)      # model to GPU\n",
    "\n",
    "\n",
    "# Custom dataloaders\n",
    "train_dataset = PascalVOCDataset(data_folder,'train',keep_difficult)\n",
    "val_dataset   = PascalVOCDataset(data_folder,'test' ,keep_difficult)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                            collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True) # pass in our collate function here\n",
    "val_loader    = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                             collate_fn=val_dataset.collate_fn, num_workers=workers,\n",
    "                                             pin_memory=True)\n",
    "# used to schedule learning rate later\n",
    "n_batches = len(train_loader)\n",
    "\n",
    "learning_rates = list()\n",
    "total_iters = epochs*n_batches\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "            \n",
    "    train(train_loader=train_loader,\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          epoch=epoch,\n",
    "          total_iters=total_iters\n",
    "          )\n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss = validate(val_loader=val_loader,\n",
    "                        model=model,\n",
    "                        criterion=criterion)\n",
    "\n",
    "    # Did validation loss improve?\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, epochs_since_improvement, model, optimizer, val_loss, best_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000000000003757e-05,\n",
       " 1.0000000315555054e-05,\n",
       " 1.0000001266605545e-05,\n",
       " 1.0000002853155115e-05,\n",
       " 1.0000005099192599e-05,\n",
       " 1.0000007962735937e-05,\n",
       " 1.0000011461778355e-05,\n",
       " 1.0000015596320077e-05,\n",
       " 1.0000020366360876e-05,\n",
       " 1.0000025771900978e-05,\n",
       " 1.0000031812940161e-05,\n",
       " 1.0000038555490636e-05,\n",
       " 1.0000045873523476e-05,\n",
       " 1.0000053827055396e-05,\n",
       " 1.000006241608651e-05,\n",
       " 1.0000071640616812e-05,\n",
       " 1.0000081500646305e-05,\n",
       " 1.0000092098215192e-05,\n",
       " 1.0000103235238123e-05,\n",
       " 1.0000115007760244e-05,\n",
       " 1.0000127415781447e-05,\n",
       " 1.0000140459301729e-05,\n",
       " 1.0000154138321202e-05,\n",
       " 1.0000168452839757e-05,\n",
       " 1.0000183546921081e-05,\n",
       " 1.0000199138433071e-05,\n",
       " 1.0000215365444033e-05,\n",
       " 1.0000232227954185e-05,\n",
       " 1.0000249725963306e-05,\n",
       " 1.0000267859471399e-05,\n",
       " 1.0000286628478572e-05,\n",
       " 1.0000306219072e-05,\n",
       " 1.0000326265072499e-05,\n",
       " 1.0000346946571859e-05,\n",
       " 1.0000368263570298e-05,\n",
       " 1.0000390216067596e-05,\n",
       " 1.0000412804063865e-05,\n",
       " 1.0000436027559105e-05,\n",
       " 1.0000460114663976e-05,\n",
       " 1.000048461515232e-05,\n",
       " 1.0000509751139524e-05,\n",
       " 1.0000535522625477e-05,\n",
       " 1.00005619296104e-05,\n",
       " 1.0000588972094183e-05,\n",
       " 1.0000616650076715e-05,\n",
       " 1.0000645233692255e-05,\n",
       " 1.0000674188667671e-05,\n",
       " 1.0000703779141837e-05,\n",
       " 1.0000734005114641e-05,\n",
       " 1.0000764866586305e-05,\n",
       " 1.0000796363556607e-05,\n",
       " 1.000082880218802e-05,\n",
       " 1.0000861576150985e-05,\n",
       " 1.0000894985612479e-05,\n",
       " 1.0000929030572723e-05,\n",
       " 1.0000963711031494e-05,\n",
       " 1.0000999026988794e-05,\n",
       " 1.0001034978444731e-05,\n",
       " 1.0001071913585046e-05,\n",
       " 1.0001109142033317e-05,\n",
       " 1.0001147005980004e-05,\n",
       " 1.000118550542511e-05,\n",
       " 1.0001224640368744e-05,\n",
       " 1.0001264410810797e-05,\n",
       " 1.0001304816751155e-05,\n",
       " 1.0001346248399155e-05,\n",
       " 1.0001387931331516e-05,\n",
       " 1.0001430249762182e-05,\n",
       " 1.0001473203691046e-05,\n",
       " 1.0001516793118327e-05,\n",
       " 1.0001561018043695e-05,\n",
       " 1.0001605878467261e-05,\n",
       " 1.0001651806621733e-05,\n",
       " 1.0001697944036856e-05,\n",
       " 1.0001744716950176e-05,\n",
       " 1.0001792125361472e-05,\n",
       " 1.0001840169270855e-05,\n",
       " 1.0001888848678324e-05,\n",
       " 1.000193863184458e-05,\n",
       " 1.0001988588243166e-05,\n",
       " 1.0002039180139727e-05,\n",
       " 1.0002090407534154e-05,\n",
       " 1.0002142270426336e-05,\n",
       " 1.0002194768816494e-05,\n",
       " 1.0002247902704297e-05,\n",
       " 1.0002302182374264e-05,\n",
       " 1.0002356593252851e-05,\n",
       " 1.0002411639629194e-05,\n",
       " 1.000246732150318e-05,\n",
       " 1.0002523638874703e-05,\n",
       " 1.0002580591743868e-05,\n",
       " 1.0002638180110567e-05,\n",
       " 1.0002696956282366e-05,\n",
       " 1.00027558216393e-05,\n",
       " 1.0002815322493656e-05,\n",
       " 1.0002875458845436e-05,\n",
       " 1.0002936230694528e-05,\n",
       " 1.0002997638040934e-05,\n",
       " 1.0003059680884653e-05,\n",
       " 1.0003122953556627e-05,\n",
       " 1.0003186273390027e-05,\n",
       " 1.0003250228720518e-05,\n",
       " 1.0003314819548213e-05,\n",
       " 1.000338004587289e-05,\n",
       " 1.0003445907694658e-05,\n",
       " 1.000351240501341e-05,\n",
       " 1.000358017418357e-05,\n",
       " 1.0003647948491337e-05,\n",
       " 1.0003716358296088e-05,\n",
       " 1.0003785403597597e-05,\n",
       " 1.000385508439598e-05,\n",
       " 1.0003925400691012e-05,\n",
       " 1.0003997024865113e-05,\n",
       " 1.0004068618148721e-05,\n",
       " 1.000414084692898e-05,\n",
       " 1.0004213711205778e-05,\n",
       " 1.0004287210979005e-05,\n",
       " 1.0004361346248663e-05,\n",
       " 1.000443611701486e-05,\n",
       " 1.000451223768306e-05,\n",
       " 1.000458828543706e-05,\n",
       " 1.0004664968687377e-05,\n",
       " 1.0004742287433794e-05,\n",
       " 1.000482024167653e-05,\n",
       " 1.0004898831415253e-05,\n",
       " 1.0004978056650073e-05,\n",
       " 1.0005058673809943e-05,\n",
       " 1.0005139176032014e-05,\n",
       " 1.0005220313749962e-05,\n",
       " 1.0005302086963788e-05,\n",
       " 1.000538449567349e-05,\n",
       " 1.0005467539878849e-05,\n",
       " 1.0005551219579863e-05,\n",
       " 1.0005636333228861e-05,\n",
       " 1.0005721289916463e-05,\n",
       " 1.00058068820995e-05,\n",
       " 1.0005893109778083e-05,\n",
       " 1.00059799729521e-05,\n",
       " 1.0006067471621444e-05,\n",
       " 1.0006155605786112e-05,\n",
       " 1.0006245215921586e-05,\n",
       " 1.0006334627072068e-05,\n",
       " 1.0006424673717543e-05,\n",
       " 1.0006515355858122e-05,\n",
       " 1.0006606673493804e-05,\n",
       " 1.0006698626624481e-05,\n",
       " 1.000679121524993e-05,\n",
       " 1.0006885321869118e-05,\n",
       " 1.0006979187479498e-05,\n",
       " 1.0007073688584649e-05,\n",
       " 1.0007168825184461e-05,\n",
       " 1.0007264597278937e-05,\n",
       " 1.0007361004867853e-05,\n",
       " 1.000745804795132e-05,\n",
       " 1.0007555726529228e-05,\n",
       " 1.0007654971118646e-05,\n",
       " 1.0007753926680491e-05,\n",
       " 1.0007853517736445e-05,\n",
       " 1.000795374428673e-05,\n",
       " 1.0008054606331014e-05,\n",
       " 1.0008156103869407e-05,\n",
       " 1.00082582369018e-05,\n",
       " 1.0008361977968305e-05,\n",
       " 1.000846538798364e-05,\n",
       " 1.0008569433492753e-05,\n",
       " 1.0008674114495644e-05,\n",
       " 1.0008779430992092e-05,\n",
       " 1.0008885382982207e-05,\n",
       " 1.000899197046577e-05,\n",
       " 1.000909919344278e-05,\n",
       " 1.000920807247189e-05,\n",
       " 1.0009316572430849e-05,\n",
       " 1.0009425707883031e-05,\n",
       " 1.000953547882844e-05,\n",
       " 1.0009645885266855e-05,\n",
       " 1.0009756927198385e-05,\n",
       " 1.0009868604622807e-05,\n",
       " 1.0009981980121826e-05,\n",
       " 1.0010094934527203e-05,\n",
       " 1.0010208524425365e-05,\n",
       " 1.0010322749816089e-05,\n",
       " 1.0010437610699487e-05,\n",
       " 1.0010553107075448e-05,\n",
       " 1.0010669238943752e-05,\n",
       " 1.0010786006304506e-05,\n",
       " 1.0010904519757514e-05,\n",
       " 1.0011022564098008e-05,\n",
       " 1.0011141243930624e-05,\n",
       " 1.0011260559255361e-05,\n",
       " 1.0011380510072108e-05,\n",
       " 1.0011501096380643e-05,\n",
       " 1.0011622318181192e-05,\n",
       " 1.0011745328096373e-05,\n",
       " 1.0011867826875445e-05,\n",
       " 1.0011990961146196e-05,\n",
       " 1.0012114730908515e-05,\n",
       " 1.0012239136162292e-05,\n",
       " 1.0012364176907306e-05,\n",
       " 1.001248985314378e-05,\n",
       " 1.001261616487138e-05,\n",
       " 1.0012744312731273e-05,\n",
       " 1.001287190143629e-05,\n",
       " 1.0013000125632217e-05,\n",
       " 1.0013128985319047e-05,\n",
       " 1.0013258480496674e-05,\n",
       " 1.0013388611165095e-05,\n",
       " 1.0013519377324093e-05,\n",
       " 1.0013652021637764e-05,\n",
       " 1.0013784064772964e-05,\n",
       " 1.0013916743398628e-05,\n",
       " 1.0014050057514536e-05,\n",
       " 1.0014184007120687e-05,\n",
       " 1.001431859221697e-05,\n",
       " 1.0014453812803277e-05,\n",
       " 1.0014590953566639e-05,\n",
       " 1.0014727451127933e-05,\n",
       " 1.001486458417903e-05,\n",
       " 1.0015002352719927e-05,\n",
       " 1.0015140756750293e-05,\n",
       " 1.001527979627024e-05,\n",
       " 1.0015419471279658e-05,\n",
       " 1.0015561108488183e-05,\n",
       " 1.0015702060471375e-05,\n",
       " 1.0015843647943703e-05,\n",
       " 1.0015985870905171e-05,\n",
       " 1.0016128729355557e-05,\n",
       " 1.0016272223294749e-05,\n",
       " 1.0016416352722859e-05,\n",
       " 1.0016562486372238e-05,\n",
       " 1.0016707892772797e-05,\n",
       " 1.001685393466194e-05,\n",
       " 1.0017000612039449e-05,\n",
       " 1.0017147924905214e-05,\n",
       " 1.0017295873259343e-05,\n",
       " 1.0017445861861738e-05,\n",
       " 1.001759508718721e-05,\n",
       " 1.0017744948000607e-05,\n",
       " 1.0017895444301926e-05,\n",
       " 1.0018046576090948e-05,\n",
       " 1.0018198343367561e-05,\n",
       " 1.0018350746131877e-05,\n",
       " 1.001850523116651e-05,\n",
       " 1.0018658910900734e-05,\n",
       " 1.0018813226122329e-05,\n",
       " 1.0018968176830963e-05,\n",
       " 1.0019123763026747e-05,\n",
       " 1.001927998470957e-05,\n",
       " 1.0019436841879212e-05,\n",
       " 1.0019595823341222e-05,\n",
       " 1.0019753957479447e-05,\n",
       " 1.0019912727104268e-05,\n",
       " 1.0020072132215578e-05,\n",
       " 1.0020232172813263e-05,\n",
       " 1.0020392848897216e-05,\n",
       " 1.0020554160467213e-05,\n",
       " 1.002071763835174e-05,\n",
       " 1.0020880226888993e-05,\n",
       " 1.002104345091207e-05,\n",
       " 1.0021207310420974e-05,\n",
       " 1.002137180541548e-05,\n",
       " 1.002153693589559e-05,\n",
       " 1.0021704268716781e-05,\n",
       " 1.0021870676162711e-05,\n",
       " 1.0022037719093914e-05,\n",
       " 1.0022205397510278e-05,\n",
       " 1.0022373711411582e-05,\n",
       " 1.0022542660797827e-05,\n",
       " 1.002271224566879e-05,\n",
       " 1.002288407490278e-05,\n",
       " 1.0023054936738237e-05,\n",
       " 1.0023226434058085e-05,\n",
       " 1.0023398566862322e-05,\n",
       " 1.0023571335150834e-05,\n",
       " 1.0023744738923405e-05,\n",
       " 1.0023918778180034e-05,\n",
       " 1.0024095103821405e-05,\n",
       " 1.002427042004087e-05,\n",
       " 1.002444637174406e-05,\n",
       " 1.0024622958930756e-05,\n",
       " 1.0024800181600956e-05,\n",
       " 1.002497803975455e-05,\n",
       " 1.0025156533391429e-05,\n",
       " 1.0025337355434549e-05,\n",
       " 1.0025517126032606e-05,\n",
       " 1.0025697532113507e-05,\n",
       " 1.0025878573677248e-05,\n",
       " 1.0026060250723722e-05,\n",
       " 1.0026242563252596e-05,\n",
       " 1.0026425511263979e-05,\n",
       " 1.0026610829703325e-05,\n",
       " 1.0026795054674232e-05,\n",
       " 1.0026979915127318e-05,\n",
       " 1.0027165411062362e-05,\n",
       " 1.0027351542479255e-05,\n",
       " 1.0027538309377883e-05,\n",
       " 1.0027727482730918e-05,\n",
       " 1.0027915526587634e-05,\n",
       " 1.0028104205925867e-05,\n",
       " 1.0028293520745284e-05,\n",
       " 1.0028483471045777e-05,\n",
       " 1.0028674056827232e-05,\n",
       " 1.0028865278089654e-05,\n",
       " 1.0029058947827866e-05,\n",
       " 1.0029251446046827e-05,\n",
       " 1.0029444579746198e-05,\n",
       " 1.0029638348925982e-05,\n",
       " 1.0029832753585956e-05,\n",
       " 1.003002779372612e-05,\n",
       " 1.0030223469346367e-05,\n",
       " 1.0030421635463682e-05,\n",
       " 1.0030618588038701e-05,\n",
       " 1.0030816176093246e-05,\n",
       " 1.0031014399627321e-05,\n",
       " 1.0031213258640703e-05,\n",
       " 1.003141275313339e-05,\n",
       " 1.0031612883105275e-05,\n",
       " 1.0031815545595618e-05,\n",
       " 1.0032016952520398e-05,\n",
       " 1.0032218994923932e-05,\n",
       " 1.003242167280611e-05,\n",
       " 1.0032624986166711e-05,\n",
       " 1.0032828935005626e-05,\n",
       " 1.0033035452389443e-05,\n",
       " 1.0033240678179816e-05,\n",
       " 1.0033446539448171e-05,\n",
       " 1.0033653036194287e-05,\n",
       " 1.0033860168418162e-05,\n",
       " 1.0034067936119577e-05,\n",
       " 1.0034276339298421e-05,\n",
       " 1.0034487353043224e-05,\n",
       " 1.0034697033171539e-05,\n",
       " 1.0034907348776951e-05,\n",
       " 1.003511829985935e-05,\n",
       " 1.0035329886418405e-05,\n",
       " 1.0035542108454116e-05,\n",
       " 1.0035754965966262e-05,\n",
       " 1.0035970476065423e-05,\n",
       " 1.0036184610525272e-05,\n",
       " 1.0036399380461115e-05,\n",
       " 1.0036614785872948e-05,\n",
       " 1.0036830826760666e-05,\n",
       " 1.0037047503123934e-05,\n",
       " 1.0037264814962755e-05,\n",
       " 1.0037484821409534e-05,\n",
       " 1.0037703410194069e-05,\n",
       " 1.0037922634453713e-05,\n",
       " 1.0038142494188354e-05,\n",
       " 1.0038362989397774e-05,\n",
       " 1.0038584120081972e-05,\n",
       " 1.0038805886240617e-05,\n",
       " 1.0039030389028167e-05,\n",
       " 1.0039253432130538e-05,\n",
       " 1.0039477110707023e-05,\n",
       " 1.0039701424757513e-05,\n",
       " 1.0039926374281897e-05,\n",
       " 1.0040151959279844e-05,\n",
       " 1.004038031693259e-05,\n",
       " 1.0040607178872605e-05,\n",
       " 1.0040834676285852e-05,\n",
       " 1.004106280917222e-05,\n",
       " 1.0041291577531599e-05,\n",
       " 1.0041520981363657e-05,\n",
       " 1.0041751020668502e-05,\n",
       " 1.0041983874648764e-05,\n",
       " 1.004221519089347e-05,\n",
       " 1.0042447142610523e-05,\n",
       " 1.0042679729799483e-05,\n",
       " 1.004291295246046e-05,\n",
       " 1.004314681059323e-05,\n",
       " 1.0043381304197686e-05,\n",
       " 1.0043618654498171e-05,\n",
       " 1.0043854425040387e-05,\n",
       " 1.0044090831053737e-05,\n",
       " 1.0044327872538219e-05,\n",
       " 1.0044565549493502e-05,\n",
       " 1.0044803861919584e-05,\n",
       " 1.0045042809816249e-05,\n",
       " 1.0045284656429446e-05,\n",
       " 1.0045524881261772e-05,\n",
       " 1.0045765741564235e-05,\n",
       " 1.0046007237336617e-05,\n",
       " 1.0046249368578804e-05,\n",
       " 1.0046492135290578e-05,\n",
       " 1.0046737836744778e-05,\n",
       " 1.0046981880390445e-05,\n",
       " 1.0047226559505259e-05,\n",
       " 1.0047471874089215e-05,\n",
       " 1.0047717824141984e-05,\n",
       " 1.0047964409663456e-05,\n",
       " 1.0048211630653407e-05,\n",
       " 1.0048461828406068e-05,\n",
       " 1.0048710326327817e-05,\n",
       " 1.0048959459717494e-05,\n",
       " 1.0049209228575098e-05,\n",
       " 1.0049459632900523e-05,\n",
       " 1.0049710672693545e-05,\n",
       " 1.0049962347954053e-05,\n",
       " 1.0050217041997444e-05,\n",
       " 1.0050469994187207e-05,\n",
       " 1.0050723581844016e-05,\n",
       " 1.0050977804967649e-05,\n",
       " 1.0051232663557775e-05,\n",
       " 1.0051488157614504e-05,\n",
       " 1.0051744287137504e-05,\n",
       " 1.005200347746356e-05,\n",
       " 1.0052260883913716e-05,\n",
       " 1.0052518925829593e-05,\n",
       " 1.005277760321108e-05,\n",
       " 1.0053036916058064e-05,\n",
       " 1.0053296864370436e-05,\n",
       " 1.0053557448147864e-05,\n",
       " 1.0053821134748631e-05,\n",
       " 1.0054082995450785e-05,\n",
       " 1.0054345491617666e-05,\n",
       " 1.005460862324905e-05,\n",
       " 1.0054872390344718e-05,\n",
       " 1.0055136792904558e-05,\n",
       " 1.0055404334312745e-05,\n",
       " 1.0055670013795325e-05,\n",
       " 1.0055936328741523e-05,\n",
       " 1.0056203279151121e-05,\n",
       " 1.0056470865024119e-05,\n",
       " 1.0056739086360186e-05,\n",
       " 1.0057007943159101e-05,\n",
       " 1.0057279980826533e-05,\n",
       " 1.0057550114545865e-05,\n",
       " 1.0057820883727602e-05,\n",
       " 1.0058092288371634e-05,\n",
       " 1.005836432847774e-05,\n",
       " 1.00586370040457e-05,\n",
       " 1.0058910315075513e-05,\n",
       " 1.0059186848993465e-05,\n",
       " 1.0059461436941044e-05,\n",
       " 1.0059736660349923e-05,\n",
       " 1.0060012519219772e-05,\n",
       " 1.0060289013550592e-05,\n",
       " 1.0060566143342048e-05,\n",
       " 1.006084390859392e-05,\n",
       " 1.0061124938753776e-05,\n",
       " 1.0061403980921095e-05,\n",
       " 1.0061683658548392e-05,\n",
       " 1.0061963971635553e-05,\n",
       " 1.0062244920182245e-05,\n",
       " 1.0062526504188363e-05,\n",
       " 1.0062811389127357e-05,\n",
       " 1.0063094250046712e-05,\n",
       " 1.0063377746424935e-05,\n",
       " 1.006366187826192e-05,\n",
       " 1.0063946645557444e-05,\n",
       " 1.0064232048311285e-05,\n",
       " 1.0064518086523336e-05,\n",
       " 1.0064807467687663e-05,\n",
       " 1.0065094782810408e-05,\n",
       " 1.00653827333907e-05,\n",
       " 1.0065671319428425e-05,\n",
       " 1.0065960540923364e-05,\n",
       " 1.0066250397875516e-05,\n",
       " 1.006654089028444e-05,\n",
       " 1.0066834767665043e-05,\n",
       " 1.0067126536982012e-05,\n",
       " 1.006741894175531e-05,\n",
       " 1.006771198198483e-05,\n",
       " 1.0068005657670237e-05,\n",
       " 1.006829996881131e-05,\n",
       " 1.0068594915408051e-05,\n",
       " 1.0068893288995758e-05,\n",
       " 1.0069189512498003e-05,\n",
       " 1.0069486371455143e-05,\n",
       " 1.0069783865867064e-05,\n",
       " 1.0070081995733659e-05,\n",
       " 1.0070380761054707e-05,\n",
       " 1.0070680161829873e-05,\n",
       " 1.0070983031615407e-05,\n",
       " 1.0071283709293316e-05,\n",
       " 1.0071585022425017e-05,\n",
       " 1.0071886971010172e-05,\n",
       " 1.0072189555048566e-05,\n",
       " 1.0072492774540087e-05,\n",
       " 1.0072799499066207e-05,\n",
       " 1.007310399545815e-05,\n",
       " 1.0073409127302556e-05,\n",
       " 1.0073714894599315e-05,\n",
       " 1.0074021297348097e-05,\n",
       " 1.00743283355489e-05,\n",
       " 1.0074636009201392e-05,\n",
       " 1.0074944318305354e-05,\n",
       " 1.0075256180457597e-05,\n",
       " 1.0075565766458889e-05,\n",
       " 1.0075875987911097e-05,\n",
       " 1.0076186844814002e-05,\n",
       " 1.0076498337167382e-05,\n",
       " 1.0076810464971237e-05,\n",
       " 1.0077123228225125e-05,\n",
       " 1.0077439586546143e-05,\n",
       " 1.0077753626694599e-05,\n",
       " 1.0078068302292535e-05,\n",
       " 1.0078383613339734e-05,\n",
       " 1.0078699559836079e-05,\n",
       " 1.0079016141781465e-05,\n",
       " 1.0079333359175447e-05,\n",
       " 1.0079651212018027e-05,\n",
       " 1.0079972707941084e-05,\n",
       " 1.0080291837674808e-05,\n",
       " 1.0080611602856466e-05,\n",
       " 1.0080932003485837e-05,\n",
       " 1.0081253039562811e-05,\n",
       " 1.0081574711086947e-05,\n",
       " 1.0081897018058242e-05,\n",
       " 1.0082223010128643e-05,\n",
       " 1.008254659398821e-05,\n",
       " 1.0082870813294276e-05,\n",
       " 1.008319566804662e-05,\n",
       " 1.008352115824502e-05,\n",
       " 1.0083847283889366e-05,\n",
       " 1.0084174044979214e-05,\n",
       " 1.0084501441514458e-05,\n",
       " 1.0084832571162044e-05,\n",
       " 1.0085161244582356e-05,\n",
       " 1.0085490553447397e-05,\n",
       " 1.0085820497757169e-05,\n",
       " 1.008615107751123e-05,\n",
       " 1.008648229270947e-05,\n",
       " 1.0086814143351668e-05,\n",
       " 1.0087149769124394e-05,\n",
       " 1.0087482896648677e-05,\n",
       " 1.0087816659616144e-05,\n",
       " 1.0088151058026795e-05,\n",
       " 1.008848609188041e-05,\n",
       " 1.0088821761176546e-05,\n",
       " 1.0089158065915203e-05,\n",
       " 1.0089498187802683e-05,\n",
       " 1.0089835769420443e-05,\n",
       " 1.0090173986479952e-05,\n",
       " 1.0090512838980988e-05,\n",
       " 1.0090852326923439e-05,\n",
       " 1.0091192450307088e-05,\n",
       " 1.0091533209131711e-05,\n",
       " 1.0091874603396977e-05,\n",
       " 1.0092219862823754e-05,\n",
       " 1.0092562533964588e-05,\n",
       " 1.0092905840545513e-05,\n",
       " 1.0093249782566309e-05,\n",
       " 1.0093594360026643e-05,\n",
       " 1.0093939572926516e-05,\n",
       " 1.0094285421265487e-05,\n",
       " 1.009463517678393e-05,\n",
       " 1.0094982301995375e-05,\n",
       " 1.0095330062645364e-05,\n",
       " 1.0095678458733568e-05,\n",
       " 1.0096027490259983e-05,\n",
       " 1.0096377157224171e-05,\n",
       " 1.0096727459626018e-05,\n",
       " 1.0097078397465196e-05,\n",
       " 1.0097433290496313e-05,\n",
       " 1.009778550520432e-05,\n",
       " 1.0098138355349215e-05,\n",
       " 1.0098491840930555e-05,\n",
       " 1.009884596194812e-05,\n",
       " 1.0099200718401799e-05,\n",
       " 1.009955611029126e-05,\n",
       " 1.0099915499390406e-05,\n",
       " 1.0100272168145494e-05,\n",
       " 1.0100629472335813e-05,\n",
       " 1.010098741196103e-05,\n",
       " 1.0101345987021036e-05,\n",
       " 1.010170519751539e-05,\n",
       " 1.0102068441242556e-05,\n",
       " 1.0102428928599772e-05,\n",
       " 1.0102790051390785e-05,\n",
       " 1.0103151809615371e-05,\n",
       " 1.0103514203273309e-05,\n",
       " 1.0103877232364269e-05,\n",
       " 1.010424089688814e-05,\n",
       " 1.0104608636662122e-05,\n",
       " 1.0104973578045653e-05,\n",
       " 1.010533915486121e-05,\n",
       " 1.0105705367108685e-05,\n",
       " 1.0106072214787965e-05,\n",
       " 1.0106439697898611e-05,\n",
       " 1.01068078164404e-05,\n",
       " 1.0107180052249821e-05,\n",
       " 1.0107549447648066e-05,\n",
       " 1.0107919478476681e-05,\n",
       " 1.0108290144735666e-05,\n",
       " 1.010866144642469e-05,\n",
       " 1.0109033383543421e-05,\n",
       " 1.010940595609175e-05,\n",
       " 1.0109782687924901e-05,\n",
       " 1.0110156537326261e-05,\n",
       " 1.0110531022156336e-05,\n",
       " 1.0110906142415122e-05,\n",
       " 1.011128189810229e-05,\n",
       " 1.0111658289217508e-05,\n",
       " 1.0112035315760555e-05,\n",
       " 1.0112416543605619e-05,\n",
       " 1.0112794846998384e-05,\n",
       " 1.0113173785818206e-05,\n",
       " 1.0113553360064974e-05,\n",
       " 1.0113933569738467e-05,\n",
       " 1.0114314414838352e-05,\n",
       " 1.011469949726283e-05,\n",
       " 1.011508161920934e-05,\n",
       " 1.0115464376581582e-05,\n",
       " 1.0115847769379334e-05,\n",
       " 1.0116231797602262e-05,\n",
       " 1.0116616461250038e-05,\n",
       " 1.011700176032266e-05,\n",
       " 1.0117391338736734e-05,\n",
       " 1.0117777914652557e-05,\n",
       " 1.0118165125992343e-05,\n",
       " 1.0118552972755761e-05,\n",
       " 1.011894145494281e-05,\n",
       " 1.0119330572552939e-05,\n",
       " 1.0119720325586147e-05,\n",
       " 1.0120114399977553e-05,\n",
       " 1.0120505429850426e-05,\n",
       " 1.0120897095145607e-05,\n",
       " 1.012128939586265e-05,\n",
       " 1.0121682332001558e-05,\n",
       " 1.0122075903561887e-05,\n",
       " 1.0122470110543418e-05,\n",
       " 1.0122868680899788e-05,\n",
       " 1.012326416471756e-05,\n",
       " 1.012366028395576e-05,\n",
       " 1.0124057038614168e-05,\n",
       " 1.0124454428692562e-05,\n",
       " 1.012485245419072e-05,\n",
       " 1.0125254879085853e-05,\n",
       " 1.012565418141705e-05,\n",
       " 1.012605411916724e-05,\n",
       " 1.0126454692336199e-05,\n",
       " 1.0126855900923489e-05,\n",
       " 1.0127257744928885e-05,\n",
       " 1.012766022435228e-05,\n",
       " 1.0128067145189066e-05,\n",
       " 1.0128470901441964e-05,\n",
       " 1.0128875293112086e-05,\n",
       " 1.0129280320198991e-05,\n",
       " 1.0129685982702568e-05,\n",
       " 1.0130092280622596e-05,\n",
       " 1.0130499213958633e-05,\n",
       " 1.0130910630724257e-05,\n",
       " 1.0131318840886153e-05,\n",
       " 1.0131727686463394e-05,\n",
       " 1.0132137167455761e-05,\n",
       " 1.0132547283862923e-05,\n",
       " 1.0132958035684547e-05,\n",
       " 1.0133369422920412e-05,\n",
       " 1.013378533560206e-05,\n",
       " 1.0134197999660138e-05,\n",
       " 1.0134611299131796e-05,\n",
       " 1.0135025234016701e-05,\n",
       " 1.013543980431441e-05,\n",
       " 1.0135855010024927e-05,\n",
       " 1.0136270851147807e-05,\n",
       " 1.0136691259732334e-05,\n",
       " 1.013710837767367e-05,\n",
       " 1.0137526131026709e-05,\n",
       " 1.0137944519791115e-05,\n",
       " 1.0138363543966561e-05,\n",
       " 1.0138783203552824e-05,\n",
       " 1.0139207466622316e-05,\n",
       " 1.0139628403023833e-05,\n",
       " 1.0140049974835393e-05,\n",
       " 1.0140472182056775e-05,\n",
       " 1.0140895024687539e-05,\n",
       " 1.0141318502727463e-05,\n",
       " 1.0141742616176436e-05,\n",
       " 1.0142171375124175e-05,\n",
       " 1.0142596765384535e-05,\n",
       " 1.0143022791053063e-05,\n",
       " 1.0143449452129314e-05,\n",
       " 1.014387674861329e-05,\n",
       " 1.0144304680504437e-05,\n",
       " 1.0144733247802755e-05,\n",
       " 1.014516650261526e-05,\n",
       " 1.01455963467211e-05,\n",
       " 1.0146026826233227e-05,\n",
       " 1.01464579411512e-05,\n",
       " 1.01468896914748e-05,\n",
       " 1.0147322077203803e-05,\n",
       " 1.0147755098337879e-05,\n",
       " 1.0148192849001786e-05,\n",
       " 1.0148627146939627e-05,\n",
       " 1.0149062080281658e-05,\n",
       " 1.0149497649027766e-05,\n",
       " 1.0149933853177622e-05,\n",
       " 1.0150370692730895e-05,\n",
       " 1.0150812297834918e-05,\n",
       " 1.015125041418853e-05,\n",
       " 1.0151689165944786e-05,\n",
       " 1.0152128553103242e-05,\n",
       " 1.0152568575663787e-05,\n",
       " 1.0153009233626092e-05,\n",
       " 1.0153450526989936e-05,\n",
       " 1.015389662791962e-05,\n",
       " 1.0154339198079717e-05,\n",
       " 1.0154782403640577e-05,\n",
       " 1.0155226244601762e-05,\n",
       " 1.0155670720962936e-05,\n",
       " 1.0156115832723881e-05,\n",
       " 1.0156561579884265e-05,\n",
       " 1.0157012176625584e-05,\n",
       " 1.0157459200578466e-05,\n",
       " 1.0157906859929903e-05,\n",
       " 1.0158355154679784e-05,\n",
       " 1.0158804084827669e-05,\n",
       " 1.0159253650373336e-05,\n",
       " 1.0159703851316454e-05,\n",
       " 1.0160158943855156e-05,\n",
       " 1.0160610421586683e-05,\n",
       " 1.0161062534714889e-05,\n",
       " 1.0161515283239329e-05,\n",
       " 1.0161968667159785e-05,\n",
       " 1.0162422686476032e-05,\n",
       " 1.0162877341187744e-05,\n",
       " 1.0163336929509689e-05,\n",
       " 1.0163792861005612e-05,\n",
       " 1.0164249427896223e-05,\n",
       " 1.0164706630181082e-05,\n",
       " 1.0165164467859968e-05,\n",
       " 1.0165622940932548e-05,\n",
       " 1.016608638363584e-05,\n",
       " 1.0166546133489208e-05,\n",
       " 1.0167006518735387e-05,\n",
       " 1.0167467539374155e-05,\n",
       " 1.0167929195405074e-05,\n",
       " 1.0168391486827919e-05,\n",
       " 1.016885441364247e-05,\n",
       " 1.0169322352102051e-05,\n",
       " 1.016978655569319e-05,\n",
       " 1.0170251394675155e-05,\n",
       " 1.0170716869047499e-05,\n",
       " 1.0171182978810002e-05,\n",
       " 1.0171649723962336e-05,\n",
       " 1.0172117104504276e-05,\n",
       " 1.0172589538705565e-05,\n",
       " 1.0173058196019896e-05,\n",
       " 1.0173527488722953e-05,\n",
       " 1.0173997416814401e-05,\n",
       " 1.017446798029391e-05,\n",
       " 1.017493917916115e-05,\n",
       " 1.0175411013415787e-05,\n",
       " 1.017588794334387e-05,\n",
       " 1.0176361054366701e-05,\n",
       " 1.0176834800776266e-05,\n",
       " 1.0177309182571907e-05,\n",
       " 1.0177784199753617e-05,\n",
       " 1.017825985232085e-05,\n",
       " 1.0178740636581454e-05,\n",
       " 1.0179217565913346e-05,\n",
       " 1.0179695130629983e-05,\n",
       " 1.0180173330730927e-05,\n",
       " 1.0180652166216066e-05,\n",
       " 1.0181131637085067e-05,\n",
       " 1.018161174333749e-05,\n",
       " 1.018209702329694e-05,\n",
       " 1.0182578406309714e-05,\n",
       " 1.0183060424705025e-05,\n",
       " 1.0183543078482543e-05,\n",
       " 1.0184026367642046e-05,\n",
       " 1.0184510292183203e-05,\n",
       " 1.0184994852105792e-05,\n",
       " 1.0185484627748845e-05,\n",
       " 1.0185970464427367e-05,\n",
       " 1.0186456936486218e-05,\n",
       " 1.0186944043925175e-05,\n",
       " 1.0187431786743908e-05,\n",
       " 1.0187920164942088e-05,\n",
       " 1.018840917851949e-05,\n",
       " 1.0188903449830899e-05,\n",
       " 1.0189393740159814e-05,\n",
       " 1.0189884665866959e-05,\n",
       " 1.0190376226952111e-05,\n",
       " 1.0190868423414721e-05,\n",
       " 1.0191361255254678e-05,\n",
       " 1.0191854722471647e-05,\n",
       " 1.0192353489435727e-05,\n",
       " 1.0192848233399791e-05,\n",
       " 1.0193343612739875e-05,\n",
       " 1.019383962745565e-05,\n",
       " 1.019433627754678e-05,\n",
       " 1.0194833563013047e-05,\n",
       " 1.0195336184245687e-05,\n",
       " 1.0195834746455184e-05,\n",
       " 1.019633394403882e-05,\n",
       " 1.019683377699627e-05,\n",
       " 1.0197334245327087e-05,\n",
       " 1.0197835349031163e-05,\n",
       " 1.0198337088108055e-05,\n",
       " 1.0198844204964313e-05,\n",
       " 1.0199347220780016e-05,\n",
       " 1.019985087196754e-05,\n",
       " 1.0200355158526555e-05,\n",
       " 1.020086008045684e-05,\n",
       " 1.0201365637758065e-05,\n",
       " 1.0201871830429896e-05,\n",
       " 1.0202383442893864e-05,\n",
       " 1.0202890912299866e-05,\n",
       " 1.0203399017075481e-05,\n",
       " 1.0203907757220377e-05,\n",
       " 1.0204417132734333e-05,\n",
       " 1.02049271436168e-05,\n",
       " 1.0205437789867663e-05,\n",
       " 1.0205953897923214e-05,\n",
       " 1.0206465820903606e-05,\n",
       " 1.0206978379251403e-05,\n",
       " 1.0207491572966273e-05,\n",
       " 1.0208005402047663e-05,\n",
       " 1.0208519866495461e-05,\n",
       " 1.020903982876677e-05,\n",
       " 1.0209555569940343e-05,\n",
       " 1.0210071946479221e-05,\n",
       " 1.0210588958383074e-05,\n",
       " 1.0211106605651567e-05,\n",
       " 1.0211624888284481e-05,\n",
       " 1.0212143806281487e-05,\n",
       " 1.021266826411422e-05,\n",
       " 1.0213188458832248e-05,\n",
       " 1.0213709288913263e-05,\n",
       " 1.021423075435693e-05,\n",
       " 1.021475285516303e-05,\n",
       " 1.0215275591331233e-05,\n",
       " 1.0215798962860982e-05,\n",
       " 1.0216327916238681e-05,\n",
       " 1.0216852564484815e-05,\n",
       " 1.0217377848091727e-05,\n",
       " 1.0217903767058972e-05,\n",
       " 1.0218430321386219e-05,\n",
       " 1.0218957511073137e-05,\n",
       " 1.0219485336119394e-05,\n",
       " 1.022001379652466e-05,\n",
       " 1.022054788678327e-05,\n",
       " 1.0221077623899507e-05,\n",
       " 1.022160799637376e-05,\n",
       " 1.0222139004205583e-05,\n",
       " 1.0222670647394758e-05,\n",
       " 1.0223202925940952e-05,\n",
       " 1.0223735839843725e-05,\n",
       " 1.022427442561162e-05,\n",
       " 1.0224808616220504e-05,\n",
       " 1.0225343442185081e-05,\n",
       " 1.0225878903504913e-05,\n",
       " 1.0226415000179664e-05,\n",
       " 1.0226951732209005e-05,\n",
       " 1.0227489099592492e-05,\n",
       " 1.022803218085277e-05,\n",
       " 1.022857082493762e-05,\n",
       " 1.0229110104375734e-05,\n",
       " 1.022965001916678e-05,\n",
       " 1.0230190569310205e-05,\n",
       " 1.0230731754805903e-05,\n",
       " 1.0231273575653426e-05,\n",
       " 1.0231816031852336e-05,\n",
       " 1.0232364249932877e-05,\n",
       " 1.0232907982827625e-05,\n",
       " 1.0233452351072764e-05,\n",
       " 1.0233997354668074e-05,\n",
       " 1.0234542993613003e-05,\n",
       " 1.023508926790744e-05,\n",
       " 1.0235636177550943e-05,\n",
       " 1.0236188891087083e-05,\n",
       " 1.0236737077421342e-05,\n",
       " 1.0237285899103673e-05,\n",
       " 1.0237835356133633e-05,\n",
       " 1.0238385448510894e-05,\n",
       " 1.0238936176235011e-05,\n",
       " 1.0239487539305764e-05,\n",
       " 1.024003953772282e-05,\n",
       " 1.0240597388036915e-05,\n",
       " 1.0241150663139205e-05,\n",
       " 1.0241704573586585e-05,\n",
       " 1.0242259119378723e-05,\n",
       " 1.0242814300515397e-05,\n",
       " 1.0243370116996167e-05,\n",
       " 1.02439265688207e-05,\n",
       " 1.0244488914553164e-05,\n",
       " 1.0245046643057959e-05,\n",
       " 1.0245605006905413e-05,\n",
       " 1.0246164006095195e-05,\n",
       " 1.0246723640626974e-05,\n",
       " 1.0247283910500306e-05,\n",
       " 1.0247844815714862e-05,\n",
       " 1.024840635627042e-05,\n",
       " 1.0248973838737758e-05,\n",
       " 1.0249536655967723e-05,\n",
       " 1.0250100108537584e-05,\n",
       " 1.0250664196446791e-05,\n",
       " 1.0251228919695234e-05,\n",
       " 1.0251794278282247e-05,\n",
       " 1.0252360272207721e-05,\n",
       " 1.025293225005543e-05,\n",
       " 1.025349952065023e-05,\n",
       " 1.0254067426582388e-05,\n",
       " 1.025463596785146e-05,\n",
       " 1.0255205144457114e-05,\n",
       " 1.025577495639891e-05,\n",
       " 1.025634540367663e-05,\n",
       " 1.0256916486289715e-05,\n",
       " 1.0257493600828549e-05,\n",
       " 1.0258065960105217e-05,\n",
       " 1.0258638954716259e-05,\n",
       " 1.0259212584661342e-05,\n",
       " 1.0259786849940027e-05,\n",
       " 1.026036175055198e-05,\n",
       " 1.0260942719106297e-05,\n",
       " 1.0261518896377301e-05,\n",
       " 1.0262095708980583e-05,\n",
       " 1.0262673156915588e-05,\n",
       " 1.0263251240182094e-05,\n",
       " 1.0263829958779548e-05,\n",
       " 1.0264409312707733e-05,\n",
       " 1.026499477658818e-05,\n",
       " 1.0265575407170222e-05,\n",
       " 1.0266156673081892e-05,\n",
       " 1.0266738574322852e-05,\n",
       " 1.0267321110892554e-05,\n",
       " 1.0267904282790663e-05,\n",
       " 1.026848809001696e-05,\n",
       " 1.0269078049205089e-05,\n",
       " 1.0269663133079943e-05,\n",
       " 1.0270248852281882e-05,\n",
       " 1.0270835206810352e-05,\n",
       " 1.0271422196665131e-05,\n",
       " 1.0272009821845668e-05,\n",
       " 1.0272598082351631e-05,\n",
       " 1.0273192536828882e-05,\n",
       " 1.0273782073978322e-05,\n",
       " 1.0274372246452082e-05,\n",
       " 1.0274963054249724e-05,\n",
       " 1.0275554497371022e-05,\n",
       " 1.027614657581554e-05,\n",
       " 1.0276744884247182e-05,\n",
       " 1.0277338239330536e-05,\n",
       " 1.027793222973589e-05,\n",
       " 1.0278526855463025e-05,\n",
       " 1.0279122116511389e-05,\n",
       " 1.027971801288065e-05,\n",
       " 1.0280314544570477e-05,\n",
       " 1.0280917348256556e-05,\n",
       " 1.0281515156579918e-05,\n",
       " 1.0282113600222629e-05,\n",
       " 1.028271267918436e-05,\n",
       " 1.0283312393464778e-05,\n",
       " 1.0283912743063332e-05,\n",
       " 1.0284513727979798e-05,\n",
       " 1.0285121026901314e-05,\n",
       " 1.0285723288445905e-05,\n",
       " 1.0286326185307193e-05,\n",
       " 1.0286929717484849e-05,\n",
       " 1.0287533884978322e-05,\n",
       " 1.0288138687787388e-05,\n",
       " 1.0288744125911607e-05,\n",
       " 1.028935592004967e-05,\n",
       " 1.0289962634796597e-05,\n",
       " 1.0290569984857462e-05,\n",
       " 1.0291177970232044e-05,\n",
       " 1.0291786590919792e-05,\n",
       " 1.029239584692026e-05,\n",
       " 1.029300573823334e-05,\n",
       " 1.0293622027568506e-05,\n",
       " 1.0294233195498773e-05,\n",
       " 1.0294844998740215e-05,\n",
       " 1.0295457437292611e-05,\n",
       " 1.0296070511155411e-05,\n",
       " 1.0296684220328393e-05,\n",
       " 1.0297304363538746e-05,\n",
       " 1.0297919349324162e-05,\n",
       " 1.0298534970418545e-05,\n",
       " 1.0299151226821343e-05,\n",
       " 1.0299768118532334e-05,\n",
       " 1.0300385645551075e-05,\n",
       " 1.0301003807877126e-05,\n",
       " 1.0301628446248683e-05,\n",
       " 1.0302247885181646e-05,\n",
       " 1.0302867959420813e-05,\n",
       " 1.0303488668965745e-05,\n",
       " 1.0304110013815998e-05,\n",
       " 1.0304731993971239e-05,\n",
       " 1.0305354609430919e-05,\n",
       " 1.0305983742944124e-05,\n",
       " 1.0306607635005191e-05,\n",
       " 1.0307232162369702e-05,\n",
       " 1.0307857325037215e-05,\n",
       " 1.0308483123007176e-05,\n",
       " 1.0309109556279366e-05,\n",
       " 1.0309736624853341e-05,\n",
       " 1.0310370253488311e-05,\n",
       " 1.0310998598658042e-05,\n",
       " 1.0311627579128232e-05,\n",
       " 1.0312257194898663e-05,\n",
       " 1.0312887445968781e-05,\n",
       " 1.0313518332338366e-05,\n",
       " 1.0314155814783347e-05,\n",
       " 1.0314787977743715e-05,\n",
       " 1.0315420776002224e-05,\n",
       " 1.0316054209558542e-05,\n",
       " 1.0316688278412116e-05,\n",
       " 1.0317322982562617e-05,\n",
       " 1.03179583220096e-05,\n",
       " ...]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb634ea8cf8>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Parameters\n",
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = './BEST_checkpoint_ssd300.pth.tar'\n",
    "\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=workers,\n",
    "                                          pin_memory=True)\n",
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate\n",
    "\n",
    "    test_loader: test data loader\n",
    "    model:  trained model\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes  = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels= list()\n",
    "    true_scores= list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult' when we calculate 'mAP' score\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # for each batch\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating model')):\n",
    "            images = images.to(device) # (N, 3, 300, 300)\n",
    "\n",
    "            # Forward prop\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                        min_score=0.01, max_overlap=0.45,\n",
    "                                                                                        top_k=200)\n",
    "            \n",
    "            # Store this batch's results for mAP calculation\n",
    "            # True boxes & labels\n",
    "            boxes  = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Cakcukate mAP\n",
    "        # Calculate mAP\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "        # Print AP for each class\n",
    "        pp.pprint(APs)\n",
    "\n",
    "        print('\\nMean Average Precision (mAP): %.3f' % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|██████████| 78/78 [07:42<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aeroplane': 0.8396266102790833,\n",
      " 'bicycle': 0.8717368245124817,\n",
      " 'bird': 0.7948161363601685,\n",
      " 'boat': 0.7175026535987854,\n",
      " 'bottle': 0.5233060121536255,\n",
      " 'bus': 0.8634150624275208,\n",
      " 'car': 0.8757845759391785,\n",
      " 'cat': 0.8758581876754761,\n",
      " 'chair': 0.613714873790741,\n",
      " 'cow': 0.8218069076538086,\n",
      " 'diningtable': 0.7582772374153137,\n",
      " 'dog': 0.8664650917053223,\n",
      " 'horse': 0.8733634948730469,\n",
      " 'motorbike': 0.852861762046814,\n",
      " 'person': 0.8127259612083435,\n",
      " 'pottedplant': 0.5444532036781311,\n",
      " 'sheep': 0.7859118580818176,\n",
      " 'sofa': 0.7876580953598022,\n",
      " 'train': 0.8783954381942749,\n",
      " 'tvmonitor': 0.7690606117248535}\n",
      "\n",
      "Mean Average Precision (mAP): 0.786\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note\n",
    "We started with the vanilla model followed by original paper's instruction and achieved 74.6mAP. With `Kai_ming` initialization and **learning rate annealing**, we have already achieved better results than the orginal paper(which is 77.2 mAP)<br>\n",
    "Benchmark from last notebook: 77.3mAP\n",
    "This notebook implemented batchnorm for base VGG architecture which pushed the results further to 78.8mAP which is quite a good leap forward.<br>\n",
    "In the second half, we implememted batchnorm for auxilirary layers as well and got 79.1mAP.<br>\n",
    "Conclusion, batchnorm is quite beneficial to implement for SSD no matter we are training from scratch or doing tranfer learning.\n",
    "\n",
    "We are not done yet.<br>\n",
    "There are a few tricks that we may try to make the result even better. See next notebook, Phase IV--Gradual unfreezing and learning the optimal weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
