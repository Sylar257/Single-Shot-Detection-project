{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-shot Detection with sgrvinod and FastAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a interactive notebook where you can change things around, experiment and singling out techniques anywhere in the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide assume solid understand of convolutional concepts and computation. If this assumption does not hold, please feel free to check my other [repos](https://github.com/Sylar257/Image-Captioning-Project) (and [this](https://github.com/Sylar257/Skin-cancer-detection-with-stacking)) that contains more contents on these subjects.\n",
    "Hence, in this notebook, we are going to jump right into our model and explain along the way why we need all these components.<br>\n",
    "This might also be the place where you can experiment most of your modification on the archietectures. Using more powerful base-models, add in regularization layers, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU for cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    # We implement VGG-16 here for low-level feature extraction\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "        \n",
    "        # Stabdard convolutional layers in VGG16\n",
    "        # We have an input size of 300 by 300\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # stride = 1, output = (300+2-3)/1+1 = 300\n",
    "        self.conv1_2 = nn.Conv2d(64,64, kernel_size=3, padding=1)   # output = 300 as before\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (300-2)/2+1 = 150\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output = (150+2-3)/1+1 = 150\n",
    "        self.conv2_2 = nn.Conv2d(128,128, kernel_size=3, padding=1) # output = (150+2-3)/1+1 = 150\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (150-2)/2 +1  = 75\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # ceiling (not floor) here for even dims\n",
    "        # output = ceil((75-2)/2)-1 = 38   if floor we would be getting 37 here which is an odd number\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)# output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)# output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)# output = (38+2-3)/1+1 = 38\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (38-2)/2 +1  = 19\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)# output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)# output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)# output = (19+2-3)/1+1 = 19\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) # We retain the size at this step with padding and stride of 1\n",
    "        # output = (19+2-3)/1+1 = 19\n",
    "        \n",
    "        # Here we replace the FC6 and FC7 with the technique introduce by sgrvinod(same with the original paper)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # output = (19+12-3-2*(6-1))/1+1 = 19\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(1024,1024, kernel_size=1)                        # output = (19-1)/1+1 = 19\n",
    "        \n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "        \n",
    "    def forawrd(self, image):\n",
    "        # forward run with an image input of size 300 by 300\n",
    "        \"\"\"\n",
    "        image: tensor of shape (N, 3, 300, 300)\n",
    "        return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))  # (N,64,300,300)\n",
    "        out = F.relu(self.conv1_2(out))    # (N,64,300,300)\n",
    "        out = self.pool1(out)              # (N,64,150,150)\n",
    "        \n",
    "        out = F.relu(self.conv2_1(out))    # (N,128,150,150)\n",
    "        out = F.relu(self.conv2_2(out))    # (N,128,150,150)\n",
    "        out = self.pool2(out)              # (N,128, 75, 75)\n",
    "        \n",
    "        out = F.relu(self.conv3_1(out))    # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_2(out))    # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_3(out))    # (N,256, 75, 75)\n",
    "        out = self.pool3(out)              # (N,256, 38, 38) because we haveceil_mode=True\n",
    "        \n",
    "        out = F.relu(self.conv4_1(out))    # (N,512, 38, 38)\n",
    "        out = F.relu(self.conv4_2(out))    # (N,512, 38, 38)\n",
    "        out = F.relu(self.conv4_3(out))    # (N,512, 38, 38)\n",
    "        # here we extract the feature from conv4_3\n",
    "        conv4_3_feats = out\n",
    "        out = self.pool4(out)              # (N,512, 19, 19)\n",
    "        \n",
    "        out = F.relu(self.conv5_1(out))    # (N,512, 19, 19)\n",
    "        out = F.relu(self.conv5_2(out))    # (N,512, 19, 19)\n",
    "        out = F.relu(self.conv5_3(out))    # (N,512, 19, 19)\n",
    "        out = self.pool5(out)              # (N,512, 19, 19) k=3,s=1,p=1, we are retaining the size here\n",
    "        \n",
    "        out = F.relu(self.conv6(out))      # (N,1024,19, 19) This is the diated convolutional layer with dilation=6, padding=6\n",
    "        \n",
    "        out = F.relu(self.conv7(out))      # (N,1024,19, 19)\n",
    "        # also, extract feature maps of conv7 here\n",
    "        conv7_feats = out\n",
    "        \n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):        \n",
    "        # Use pre-trained wieght from Torch Vsion. Conver fc6 and fc7 weights into conv6 and conv7\n",
    "        # current state of base architecture\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "        \n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "        \n",
    "        # Transfer conv, parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "            \n",
    "        # Conver fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "        \n",
    "        self.load_state_dict(state_dict)\n",
    "        \n",
    "        print(\"\\nLoaded base model with pre-trained weights\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    These layers are put on top of base model to produce more feature maps for object detections.(smaller maps)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)         # output=(19-1)/1+1 = 19\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)# output=(19+2-3)/2+1 = 10\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)          # output=(10-1)/1+1 = 10\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)# output=(10+2-3)/2+1 = 5 because by defaul we use \"floor\"\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(5-1)/1+1 = 5\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(5-3)/1+1 = 3\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(3-1)/1+1 = 3\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(3-3)/1+1 = 1\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(c.bias, 0.) \n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        conv7_feats: (N, 1024, 19, 19)\n",
    "        return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "      \"\"\"\n",
    "      Convolutions to predict class scores and bounding boxes using lower and higher level feature maps\n",
    "\n",
    "      The bounding boxes (offsets (g_{c_x}, g_{c_y}, g_w, g_h) of the 8732 default priors)\n",
    "      See 'cxcy_to_gcxgcy' in utils.py for encoding definition\n",
    "\n",
    "      The class scores represent the scores of each object class in each of the 8732 hounding boxes\n",
    "      A high score for 'background' = no object\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of proior_boxes we are considering per position in each feature map\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                    'conv7': 6,\n",
    "                    'conv8_2': 6,\n",
    "                    'conv9_2': 6,\n",
    "                    'conv10_2': 4,\n",
    "                    'conv11_2': 4}\n",
    "        # 4 prior-boxes prediction convoluitions (predict offsets w.r.t prior-boxes)\n",
    "\n",
    "        # This is the part to compute LOCALIZATION prediction\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3']*4, kernel_size=3, padding=1) # output = (38-3+2)/1+1 = 38, same padding\n",
    "        self.loc_conv7   = nn.Conv2d(1024, n_boxes['conv7']*4, kernel_size=3, padding=1)  # output = (19-3+2)/1+1 = 19\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2']*4, kernel_size=3, padding=1) # output = (10-3+2)/1+1 = 10\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2']*4, kernel_size=3, padding=1) # output = (5-3+2)/1 +1 = 5\n",
    "        self.loc_conv10_2= nn.Conv2d(256, n_boxes['conv10_2']*4,kernel_size=3, padding=1) # output = (3-3+2)/1 +1 = 3\n",
    "        self.loc_conv11_2= nn.Conv2d(256, n_boxes['conv11_2']*4,kernel_size=3, padding=1) # output = (1-3+2)/1 +1 = 1\n",
    "\n",
    "        # This is the part to comput CLASS prediction\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7   = nn.Conv2d(1024,n_boxes['conv7']   * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,n_boxes['conv10_2'] * n_classes,kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,n_boxes['conv11_2'] * n_classes,kernel_size=3, padding=1)\n",
    "\n",
    "        self.init_conv2d()\n",
    "    def init_conv2d(self):\n",
    "        # Use Kaiming_uniform_ here instead of xavier_uniform_\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds w.r.t prior boxes\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)            # (N, 16, 38, 38)  16 is from 4 priors 4*4=16\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)  to match prior-box order (after .view())\n",
    "        # .contiguous() ensures it is stores in a contiguous chunk of memory, needed for .view() below\n",
    "\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)          # This give us (N, 5776, 4) the (g_{c_x}, g_{c_y}, g_w, g_h) for all 5776 priors\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                    self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                                self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # A total of 8732 boxes\n",
    "        # Concatenate in this specific order    \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "\n",
    "        \"\"\"\n",
    "        This class works as a wrapper that encapsulates the base VGG network, auxiliary, and prediciton convolutions.\n",
    "        \"\"\"\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factor = nn.Parameter(torch.FloatTensor(1, 512, 1, 1)) # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factor, 20)\n",
    "\n",
    "        # The above two lines demonstrate a simple example of how do add a leanable-parameter into our computation\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()  # with shape of (8732, 4)\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        image: (N, 3, 300, 300)\n",
    "        return:  8732 locations and class scores (i.e.  w.r.t each prior box) for the given image(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run VGG base network convolutions (lower level feature map generators, up to conv7)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)   # (N, 512, 38, 38),  (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm using our learnable parameter\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm                         # (N, 512, 38, 38) this step was done by broadcasting\n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factor            # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliaury convolution (higher level feature map extraction)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats) \n",
    "        # (N, 512, 10, 10), (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offset w.r.t. priors and classes in each resulting location)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of our forward pass) to detect objects.\n",
    "\n",
    "        For each class. perform Non-Maximum Suppression (NMS) on boxes that are above a minimum score\n",
    "\n",
    "        predicted_locs: predicted locations w.r.t the 8732 prior boxes, a tensor of (N, 8732, 4)\n",
    "        predicted_scores: predicted class score for each of prediced locations, a tensor of (N, 8732, n_classes)\n",
    "        min_score: the minimun score for a box to be consifered a match for a CERTAIN CLASS\n",
    "        max_overlap: the maximum overlap that we allow. For any pair of boxes with higher overlap, the lower class score one will be suppressed\n",
    "        top_k: if there are a lot of resulting detection across all classes, keep only the top_k \n",
    "        \n",
    "        return: detections (boxes, labels, and scores), lists of length batch_size N\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0) # N\n",
    "        n_priors = self.priors_cxcy.size(0) # 8732\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)\n",
    "\n",
    "        # list to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_scores = list()\n",
    "        all_images_labels = list()\n",
    "\n",
    "        assert n_priors == predicted_scores.size(1) == predicted_locs.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Convert diviation from prior boxes to (c_x, c_y, w, h)\n",
    "            # Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max)\n",
    "            \n",
    "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_box = list()\n",
    "            image_scores = list()\n",
    "            image_labels = list()\n",
    "\n",
    "            max_score,  best_label = predicted_scores[i].max(dim=1) # (8732), (8732)\n",
    "\n",
    "            # operations for each class. Class 0 is not included here because it denotes background(negative)\n",
    "            for c in range(1, self.n_classes)::\n",
    "                # Keep only predicted boxes and scores where scores for this class are above minimum_score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_score > min_score # torch.uint8 (byte) tensor, for infexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                # here, we will retain the score & locs of the boxes with score higher than the threshold\n",
    "                class_scores = class_scores[score_above_min_score] # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualfied, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs) # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "                \n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multibox loss function for SSD300 architecture, which is a combination of:\n",
    "\n",
    "    1) a localization loss for the predicted locations of the boxes, and\n",
    "    2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        \"\"\"\n",
    "        priors_cxcy: priors' (c_x, c_y, w, h)\n",
    "        threshold: overlapping less than 'threshold' with priors are set to class-background\n",
    "        neg_pos_ratio: a parameter used when calculating hard negative mining. Detail in forward() section\n",
    "        alpha: the ratio between localization loss and confidence loss\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.prior_xy = cxcy_to_xy(self.priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # the two loss functions for localization and classification\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "        \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        predicted_locs:   predicted locations/box w.r.t 8732 priors, (N, 8732, 4)\n",
    "        predicted_scores: preidted class scores for each of the encoded locations, (N, 8732, n_classes)\n",
    "        boxes:            ground truth boxes,  a list of N tensors\n",
    "        label:            ground truth labels, a list of N tensors\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors   = self.priors_cxcy.size(0)\n",
    "        n_classes  = prediced_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs   = torch.zeros((batch_size, n_priors, 4), dtype=torch.float),to(device)\n",
    "        true_labels = torch.zeros((batch_size, n_priors),    dtype=torch.long).to(device)\n",
    "\n",
    "        # for each image in the minibatch\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0) # the number of objects exist in the given image\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i], self.prior_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # for each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0) # (8732),  (8732)\n",
    "\n",
    "            # we dont want a situation where an object is not represented in our positive (non-background) priors for reasons like:\n",
    "            # 1. An objext might not be the best object for all priors, and is theresore not in the object_for_each_prior\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5 by defaul)\n",
    "\n",
    "            # to remedy this\n",
    "            # first, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)  # (n_object)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (this fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
