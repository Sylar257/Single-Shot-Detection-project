{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-shot Detection with sgrvinod and FastAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a interactive notebook where you can change things around, experiment and singling out techniques anywhere in the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide assume solid understand of convolutional concepts and computation. If this assumption does not hold, please feel free to check my other [repos](https://github.com/Sylar257/Image-Captioning-Project) (and [this](https://github.com/Sylar257/Skin-cancer-detection-with-stacking)) that contains more contents on these subjects.\n",
    "Hence, in this notebook, we are going to jump right into our model and explain along the way why we need all these components.<br>\n",
    "This might also be the place where you can experiment most of your modification on the archietectures. Using more powerful base-models, add in regularization layers, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from utils import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify GPU for cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    We implement VGG-16 here for low-level feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # Stabdard convolutional layers in VGG16\n",
    "        # We have an input size of 300 by 300\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)   # stride = 1, output = (300+2-3)/1+1 = 300\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # output = 300 as before\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (300-2)/2+1 = 150\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64,  128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)# output = (150+2-3)/1+1 = 150\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)          # output = (150-2)/2 +1  = 75\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)# output = (75+2-3)/1+1 = 75\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "        # output = ceil((75-2)/2)-1 = 38   if floor we would be getting 37 here which is an odd number\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (38+2-3)/1+1 = 38\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)           # output = (38-2)/2 +1  = 19\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output = (19+2-3)/1+1 = 19\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # We retain the size at this step with padding and stride of 1\n",
    "        # output = (19+2-3)/1+1 = 19\n",
    "\n",
    "        # Here we replace the FC6 and FC7 with the technique introduce by sgrvinod(same with the original paper)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) # output = (19+12-3-2*(6-1))/1+1 = 19\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)                       # output = (19-1)/1+1 = 19\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward run with an image input of size 300 by 300\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: lower-level feature maps conv4_3 and conv7\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))   # (N,64,300,300)\n",
    "        out = F.relu(self.conv1_2(out))     # (N,64,300,300)\n",
    "        out = self.pool1(out)               # (N,64,150,150)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N,128,150,150)\n",
    "        out = F.relu(self.conv2_2(out))  # (N,128,150,150)\n",
    "        out = self.pool2(out)            # (N,128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_2(out))  # (N,256, 75, 75)\n",
    "        out = F.relu(self.conv3_3(out))  # (N,256, 75, 75)\n",
    "        out = self.pool3(out)            # (N,256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
    "        # here we extract the feature from conv4_3\n",
    "        conv4_3_feats = out              # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)            # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)            # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))    # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        Use pre-trained wieght from Torch Vsion. \n",
    "        Convert fc6 and fc7 weights into conv6 and conv7\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
    "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
    "        # ...operating on the 2D image of size (C, H, W) without padding\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model with pre-trained weights\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    These layers are put on top of base model to produce more feature maps for object detections.(smaller maps)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)         # output=(19-1)/1+1 = 19\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)# output=(19+2-3)/2+1 = 10\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)          # output=(10-1)/1+1 = 10\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)# output=(10+2-3)/2+1 = 5 because by defaul we use \"floor\"\n",
    "        \n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(5-1)/1+1 = 5\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(5-3)/1+1 = 3\n",
    "        \n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)         # output=(3-1)/1+1 = 3\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)         # output=(3-3)/1+1 = 1\n",
    "        \n",
    "        self.init_conv2d()\n",
    "        \n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        Initialize convolution parameters\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.) \n",
    "                \n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        conv7_feats: (N, 1024, 19, 19)\n",
    "        return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutions to predict class scores and bounding boxes using lower and higher level feature maps\n",
    "\n",
    "    The bounding boxes (offsets (g_{c_x}, g_{c_y}, g_w, g_h) of the 8732 default priors)\n",
    "    See 'cxcy_to_gcxgcy' in utils.py for encoding definition\n",
    "\n",
    "    The class scores represent the scores of each object class in each of the 8732 hounding boxes\n",
    "    A high score for 'background' = no object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Number of proior_boxes we are considering per position in each feature map\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                    'conv7': 6,\n",
    "                    'conv8_2': 6,\n",
    "                    'conv9_2': 6,\n",
    "                    'conv10_2': 4,\n",
    "                    'conv11_2': 4}\n",
    "        # 4 prior-boxes prediction convoluitions (predict offsets w.r.t prior-boxes)\n",
    "\n",
    "        # This is the part to compute LOCALIZATION prediction\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3']*4, kernel_size=3, padding=1) # output = (38-3+2)/1+1 = 38, same padding\n",
    "        self.loc_conv7   = nn.Conv2d(1024, n_boxes['conv7']*4, kernel_size=3, padding=1)  # output = (19-3+2)/1+1 = 19\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2']*4, kernel_size=3, padding=1) # output = (10-3+2)/1+1 = 10\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2']*4, kernel_size=3, padding=1) # output = (5-3+2)/1 +1 = 5\n",
    "        self.loc_conv10_2= nn.Conv2d(256, n_boxes['conv10_2']*4,kernel_size=3, padding=1) # output = (3-3+2)/1 +1 = 3\n",
    "        self.loc_conv11_2= nn.Conv2d(256, n_boxes['conv11_2']*4,kernel_size=3, padding=1) # output = (1-3+2)/1 +1 = 1\n",
    "\n",
    "        # This is the part to comput CLASS prediction\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7   = nn.Conv2d(1024,n_boxes['conv7']   * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256,n_boxes['conv10_2'] * n_classes,kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256,n_boxes['conv11_2'] * n_classes,kernel_size=3, padding=1)\n",
    "\n",
    "        self.init_conv2d()\n",
    "    def init_conv2d(self):\n",
    "        # Use Kaiming_uniform_ here instead of xavier_uniform_\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # Predict localization boxes' bounds w.r.t prior boxes\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)            # (N, 16, 38, 38)  16 is from 4 priors 4*4=16\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3, 1).contiguous() # (N, 38, 38, 16)  to match prior-box order (after .view())\n",
    "        # .contiguous() ensures it is stores in a contiguous chunk of memory, needed for .view() below\n",
    "\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)          # This give us (N, 5776, 4) the (g_{c_x}, g_{c_y}, g_w, g_h) for all 5776 priors\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # Predict classes in localization boxes\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                    self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                                self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # A total of 8732 boxes\n",
    "        # Concatenate in this specific order    \n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "\n",
    "        \"\"\"\n",
    "        This class works as a wrapper that encapsulates the base VGG network, auxiliary, and prediciton convolutions.\n",
    "        \"\"\"\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factor = nn.Parameter(torch.FloatTensor(1, 512, 1, 1)) # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factor, 20)\n",
    "\n",
    "        # The above two lines demonstrate a simple example of how do add a leanable-parameter into our computation\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()  # with shape of (8732, 4)\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "    \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        image: (N, 3, 300, 300)\n",
    "        return:  8732 locations and class scores (i.e.  w.r.t each prior box) for the given image(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Run VGG base network convolutions (lower level feature map generators, up to conv7)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)   # (N, 512, 38, 38),  (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm using our learnable parameter\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm                         # (N, 512, 38, 38) this step was done by broadcasting\n",
    "        conv4_3_feats = conv4_3_feats*self.rescale_factor            # (N, 512, 38, 38)\n",
    "\n",
    "        # Run auxiliaury convolution (higher level feature map extraction)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = self.aux_convs(conv7_feats) \n",
    "        # (N, 512, 10, 10), (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offset w.r.t. priors and classes in each resulting location)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats)\n",
    "        # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "    \n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of our forward pass) to detect objects.\n",
    "\n",
    "        For each class. perform Non-Maximum Suppression (NMS) on boxes that are above a minimum score\n",
    "\n",
    "        predicted_locs: predicted locations w.r.t the 8732 prior boxes, a tensor of (N, 8732, 4)\n",
    "        predicted_scores: predicted class score for each of prediced locations, a tensor of (N, 8732, n_classes)\n",
    "        min_score: the minimun score for a box to be consifered a match for a CERTAIN CLASS\n",
    "        max_overlap: the maximum overlap that we allow. For any pair of boxes with higher overlap, the lower class score one will be suppressed\n",
    "        top_k: if there are a lot of resulting detection across all classes, keep only the top_k \n",
    "        \n",
    "        return: detections (boxes, labels, and scores), lists of length batch_size N\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0) # N\n",
    "        n_priors = self.priors_cxcy.size(0) # 8732\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2) # (N, 8732, n_classes)\n",
    "\n",
    "        # list to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_scores = list()\n",
    "        all_images_labels = list()\n",
    "\n",
    "        assert n_priors == predicted_scores.size(1) == predicted_locs.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Convert diviation from prior boxes to (c_x, c_y, w, h)\n",
    "            # Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max)\n",
    "            \n",
    "            decoded_locs = cxcy_to_xy(gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_box = list()\n",
    "            image_scores = list()\n",
    "            image_labels = list()\n",
    "\n",
    "            max_score,  best_label = predicted_scores[i].max(dim=1) # (8732), (8732)\n",
    "\n",
    "            # operations for each class. Class 0 is not included here because it denotes background(negative)\n",
    "            for c in range(1, self.n_classes):\n",
    "                # Keep only predicted boxes and scores where scores for this class are above minimum_score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_scores > min_score # torch.uint8 (byte) tensor, for infexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                # here, we will retain the score & locs of the boxes with score higher than the threshold\n",
    "                class_scores = class_scores[score_above_min_score] # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score] # (n_qualfied, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs) # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "                \n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multibox loss function for SSD300 architecture, which is a combination of:\n",
    "\n",
    "    1) a localization loss for the predicted locations of the boxes, and\n",
    "    2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        \"\"\"\n",
    "        priors_cxcy: priors' (c_x, c_y, w, h)\n",
    "        threshold: overlapping less than 'threshold' with priors are set to class-background\n",
    "        neg_pos_ratio: a parameter used when calculating hard negative mining. Detail in forward() section\n",
    "        alpha: the ratio between localization loss and confidence loss\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # the two loss functions for localization and classification\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "        \n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        predicted_locs:   predicted locations/box w.r.t 8732 priors, (N, 8732, 4)\n",
    "        predicted_scores: preidted class scores for each of the encoded locations, (N, 8732, n_classes)\n",
    "        boxes:            ground truth boxes,  a list of N tensors\n",
    "        label:            ground truth labels, a list of N tensors\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors   = self.priors_cxcy.size(0)\n",
    "        n_classes  = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)   # (N, 8732)\n",
    "\n",
    "        # for each image in the minibatch\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0) # the number of objects exist in the given image\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i], self.priors_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # for each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0) # (8732),  (8732)\n",
    "\n",
    "            # we dont want a situation where an object is not represented in our positive (non-background) priors for reasons like:\n",
    "            # 1. An objext might not be the best object for all priors, and is theresore not in the object_for_each_prior\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5 by defaul)\n",
    "\n",
    "            # to remedy this\n",
    "            # first, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)   # (n_object)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (this fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used as DataLoader later\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        \"\"\"\n",
    "        data_folder: folder where data files are stored\n",
    "        split: this must be either 'TRAIN' or 'TEST'\n",
    "        keep_difficult: keep or discard objects that are considered as difficult(a property come with the dataset)\n",
    "        \"\"\"\n",
    "        self.split = split.upper()\n",
    "\n",
    "        assert self.split in {'TRAIN','TEST'}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "        with open(os.path.join(data_folder,self.split+'_images.json'),   'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split+'_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Read Image\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
    "        objects = self.objects[i]\n",
    "        boxes  = torch.FloatTensor(objects['boxes']) # (n_objects, 4)\n",
    "        labels = torch.LongTensor(objects['labels']) # (n_objects)\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
    "\n",
    "        # Discard difficult objects, if specified\n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1-difficulties]\n",
    "            labels = labels[1-difficulties]\n",
    "            difficulties = difficulties[1-difficulties]\n",
    "\n",
    "        # Apply transformations\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader)\n",
    "\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "\n",
    "        @Params\n",
    "        batch: an iterable of N sets from __getitem__()\n",
    "        return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes  = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    One epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :return: average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()  # eval mode disables dropout\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Prohibit gradient computation explicity because I had some problems with memory\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(val_loader):\n",
    "\n",
    "            # Move to default device\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predicted_locs, predicted_scores, boxes, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            # Print status\n",
    "            if i % print_freq == 0:\n",
    "                print('[{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader),\n",
    "                                                                      batch_time=batch_time,\n",
    "                                                                      loss=losses))\n",
    "\n",
    "    print('\\n * LOSS - {loss.avg:.3f}\\n'.format(loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "n_classes = len(label_map)\n",
    "\n",
    "# Training parameters\n",
    "checkpoint = './BEST_checkpoint_ssd300.pth.tar'  # path to model checkpoint if consider resume training from there\n",
    "batch_size = 8\n",
    "start_epoch = 0    # start at this epoch\n",
    "epochs = 200       # total training epochs to run without early-stopping\n",
    "epochs_since_improvement = 0 # record the no. of epochs since last improvement\n",
    "best_loss = 100.   # assume a hight loss at first\n",
    "workers = 4        # number of workers for loading data in the DataLoader\n",
    "print_freq = 200   # print training or validation status every __ batches\n",
    "lr = 1e-3/2          # learning rate\n",
    "momentum = 0.9     \n",
    "weight_decay = 5e-4\n",
    "grad_clip = None   # consider clipping the gradient when using high learning_rate\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 131. Best loss so far is 2.699.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [131][0/2069]\tBatch Time 6.190 (6.190)\tData Time 0.817 (0.817)\tLoss 2.3689 (2.3689)\t\n",
      "Epoch: [131][200/2069]\tBatch Time 0.222 (0.254)\tData Time 0.000 (0.028)\tLoss 2.8418 (2.3710)\t\n",
      "Epoch: [131][400/2069]\tBatch Time 0.185 (0.237)\tData Time 0.000 (0.024)\tLoss 2.3127 (2.3689)\t\n",
      "Epoch: [131][600/2069]\tBatch Time 0.188 (0.231)\tData Time 0.000 (0.022)\tLoss 2.5890 (2.3734)\t\n",
      "Epoch: [131][800/2069]\tBatch Time 0.188 (0.230)\tData Time 0.000 (0.024)\tLoss 2.4765 (2.3697)\t\n",
      "Epoch: [131][1000/2069]\tBatch Time 0.220 (0.228)\tData Time 0.000 (0.023)\tLoss 2.2138 (2.3714)\t\n",
      "Epoch: [131][1200/2069]\tBatch Time 0.193 (0.227)\tData Time 0.000 (0.022)\tLoss 2.5587 (2.3785)\t\n",
      "Epoch: [131][1400/2069]\tBatch Time 0.209 (0.226)\tData Time 0.000 (0.022)\tLoss 2.1377 (2.3775)\t\n",
      "Epoch: [131][1600/2069]\tBatch Time 0.183 (0.226)\tData Time 0.000 (0.021)\tLoss 2.8465 (2.3803)\t\n",
      "Epoch: [131][1800/2069]\tBatch Time 0.178 (0.226)\tData Time 0.000 (0.021)\tLoss 2.9470 (2.3793)\t\n",
      "Epoch: [131][2000/2069]\tBatch Time 0.185 (0.225)\tData Time 0.000 (0.021)\tLoss 2.0200 (2.3793)\t\n",
      "[0/619]\tBatch Time 0.346 (0.346)\tLoss 2.8228 (2.8228)\t\n",
      "[200/619]\tBatch Time 0.081 (0.097)\tLoss 3.0359 (2.8232)\t\n",
      "[400/619]\tBatch Time 0.099 (0.097)\tLoss 2.5505 (2.8194)\t\n",
      "[600/619]\tBatch Time 0.104 (0.097)\tLoss 2.4897 (2.8211)\t\n",
      "\n",
      " * LOSS - 2.821\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SSD300. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VGGBase. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AuxiliaryConvolutions. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PredictionConvolutions. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [132][0/2069]\tBatch Time 0.737 (0.737)\tData Time 0.523 (0.523)\tLoss 2.1209 (2.1209)\t\n",
      "Epoch: [132][200/2069]\tBatch Time 0.223 (0.221)\tData Time 0.000 (0.010)\tLoss 1.7856 (2.3854)\t\n",
      "Epoch: [132][400/2069]\tBatch Time 0.193 (0.216)\tData Time 0.000 (0.007)\tLoss 2.2328 (2.3519)\t\n",
      "Epoch: [132][600/2069]\tBatch Time 0.202 (0.214)\tData Time 0.000 (0.006)\tLoss 2.9838 (2.3536)\t\n",
      "Epoch: [132][800/2069]\tBatch Time 0.213 (0.214)\tData Time 0.000 (0.007)\tLoss 3.1755 (2.3589)\t\n",
      "Epoch: [132][1000/2069]\tBatch Time 0.208 (0.213)\tData Time 0.000 (0.006)\tLoss 2.7488 (2.3452)\t\n",
      "Epoch: [132][1200/2069]\tBatch Time 0.182 (0.212)\tData Time 0.000 (0.006)\tLoss 2.2783 (2.3450)\t\n",
      "Epoch: [132][1400/2069]\tBatch Time 0.247 (0.212)\tData Time 0.010 (0.006)\tLoss 1.9693 (2.3555)\t\n",
      "Epoch: [132][1600/2069]\tBatch Time 0.260 (0.212)\tData Time 0.037 (0.006)\tLoss 2.6186 (2.3534)\t\n",
      "Epoch: [132][1800/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1096 (2.3598)\t\n",
      "Epoch: [132][2000/2069]\tBatch Time 0.207 (0.211)\tData Time 0.008 (0.006)\tLoss 1.8912 (2.3556)\t\n",
      "[0/619]\tBatch Time 0.280 (0.280)\tLoss 3.0026 (3.0026)\t\n",
      "[200/619]\tBatch Time 0.078 (0.097)\tLoss 3.0318 (2.7800)\t\n",
      "[400/619]\tBatch Time 0.120 (0.097)\tLoss 2.4947 (2.7827)\t\n",
      "[600/619]\tBatch Time 0.102 (0.098)\tLoss 2.8526 (2.7664)\t\n",
      "\n",
      " * LOSS - 2.763\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [133][0/2069]\tBatch Time 0.668 (0.668)\tData Time 0.436 (0.436)\tLoss 2.3530 (2.3530)\t\n",
      "Epoch: [133][200/2069]\tBatch Time 0.212 (0.219)\tData Time 0.000 (0.013)\tLoss 2.1894 (2.3583)\t\n",
      "Epoch: [133][400/2069]\tBatch Time 0.208 (0.215)\tData Time 0.005 (0.008)\tLoss 2.6345 (2.3761)\t\n",
      "Epoch: [133][600/2069]\tBatch Time 0.238 (0.215)\tData Time 0.000 (0.009)\tLoss 1.6705 (2.3734)\t\n",
      "Epoch: [133][800/2069]\tBatch Time 0.236 (0.213)\tData Time 0.000 (0.007)\tLoss 2.7927 (2.3714)\t\n",
      "Epoch: [133][1000/2069]\tBatch Time 0.184 (0.213)\tData Time 0.000 (0.007)\tLoss 2.2707 (2.3674)\t\n",
      "Epoch: [133][1200/2069]\tBatch Time 0.212 (0.212)\tData Time 0.029 (0.007)\tLoss 2.1986 (2.3704)\t\n",
      "Epoch: [133][1400/2069]\tBatch Time 0.192 (0.212)\tData Time 0.002 (0.007)\tLoss 1.3496 (2.3604)\t\n",
      "Epoch: [133][1600/2069]\tBatch Time 0.193 (0.212)\tData Time 0.000 (0.007)\tLoss 1.7393 (2.3546)\t\n",
      "Epoch: [133][1800/2069]\tBatch Time 0.204 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3124 (2.3567)\t\n",
      "Epoch: [133][2000/2069]\tBatch Time 0.185 (0.212)\tData Time 0.001 (0.007)\tLoss 2.1974 (2.3598)\t\n",
      "[0/619]\tBatch Time 0.286 (0.286)\tLoss 3.6483 (3.6483)\t\n",
      "[200/619]\tBatch Time 0.093 (0.096)\tLoss 3.1017 (2.6601)\t\n",
      "[400/619]\tBatch Time 0.102 (0.097)\tLoss 3.2338 (2.6983)\t\n",
      "[600/619]\tBatch Time 0.136 (0.098)\tLoss 2.8223 (2.6977)\t\n",
      "\n",
      " * LOSS - 2.700\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [134][0/2069]\tBatch Time 0.765 (0.765)\tData Time 0.548 (0.548)\tLoss 2.0435 (2.0435)\t\n",
      "Epoch: [134][200/2069]\tBatch Time 0.210 (0.211)\tData Time 0.000 (0.009)\tLoss 2.2326 (2.2858)\t\n",
      "Epoch: [134][400/2069]\tBatch Time 0.186 (0.211)\tData Time 0.000 (0.009)\tLoss 1.4142 (2.3467)\t\n",
      "Epoch: [134][600/2069]\tBatch Time 0.189 (0.210)\tData Time 0.001 (0.007)\tLoss 2.4800 (2.3540)\t\n",
      "Epoch: [134][800/2069]\tBatch Time 0.212 (0.210)\tData Time 0.008 (0.007)\tLoss 2.7507 (2.3474)\t\n",
      "Epoch: [134][1000/2069]\tBatch Time 0.184 (0.210)\tData Time 0.003 (0.007)\tLoss 1.3689 (2.3509)\t\n",
      "Epoch: [134][1200/2069]\tBatch Time 0.206 (0.210)\tData Time 0.000 (0.007)\tLoss 1.6007 (2.3617)\t\n",
      "Epoch: [134][1400/2069]\tBatch Time 0.206 (0.210)\tData Time 0.002 (0.007)\tLoss 2.1965 (2.3680)\t\n",
      "Epoch: [134][1600/2069]\tBatch Time 0.224 (0.209)\tData Time 0.000 (0.006)\tLoss 1.8429 (2.3691)\t\n",
      "Epoch: [134][1800/2069]\tBatch Time 0.185 (0.209)\tData Time 0.000 (0.006)\tLoss 2.7877 (2.3708)\t\n",
      "Epoch: [134][2000/2069]\tBatch Time 0.262 (0.209)\tData Time 0.024 (0.006)\tLoss 2.4467 (2.3709)\t\n",
      "[0/619]\tBatch Time 0.312 (0.312)\tLoss 2.9619 (2.9619)\t\n",
      "[200/619]\tBatch Time 0.091 (0.096)\tLoss 2.7217 (2.7486)\t\n",
      "[400/619]\tBatch Time 0.099 (0.096)\tLoss 1.7147 (2.7543)\t\n",
      "[600/619]\tBatch Time 0.101 (0.096)\tLoss 2.3462 (2.7524)\t\n",
      "\n",
      " * LOSS - 2.753\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [135][0/2069]\tBatch Time 0.661 (0.661)\tData Time 0.424 (0.424)\tLoss 2.2309 (2.2309)\t\n",
      "Epoch: [135][200/2069]\tBatch Time 0.205 (0.214)\tData Time 0.005 (0.011)\tLoss 2.4535 (2.3626)\t\n",
      "Epoch: [135][400/2069]\tBatch Time 0.200 (0.211)\tData Time 0.000 (0.008)\tLoss 2.8495 (2.3501)\t\n",
      "Epoch: [135][600/2069]\tBatch Time 0.195 (0.210)\tData Time 0.000 (0.007)\tLoss 2.4280 (2.3716)\t\n",
      "Epoch: [135][800/2069]\tBatch Time 0.197 (0.210)\tData Time 0.008 (0.007)\tLoss 1.5257 (2.3583)\t\n",
      "Epoch: [135][1000/2069]\tBatch Time 0.183 (0.210)\tData Time 0.000 (0.007)\tLoss 1.7024 (2.3528)\t\n",
      "Epoch: [135][1200/2069]\tBatch Time 0.207 (0.210)\tData Time 0.001 (0.007)\tLoss 2.3839 (2.3490)\t\n",
      "Epoch: [135][1400/2069]\tBatch Time 0.192 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1503 (2.3445)\t\n",
      "Epoch: [135][1600/2069]\tBatch Time 0.233 (0.210)\tData Time 0.000 (0.007)\tLoss 2.0919 (2.3373)\t\n",
      "Epoch: [135][1800/2069]\tBatch Time 0.227 (0.210)\tData Time 0.003 (0.007)\tLoss 1.1586 (2.3375)\t\n",
      "Epoch: [135][2000/2069]\tBatch Time 0.215 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9970 (2.3381)\t\n",
      "[0/619]\tBatch Time 0.272 (0.272)\tLoss 2.3545 (2.3545)\t\n",
      "[200/619]\tBatch Time 0.102 (0.099)\tLoss 2.7839 (2.7386)\t\n",
      "[400/619]\tBatch Time 0.105 (0.097)\tLoss 2.8529 (2.7592)\t\n",
      "[600/619]\tBatch Time 0.097 (0.098)\tLoss 2.3653 (2.7760)\t\n",
      "\n",
      " * LOSS - 2.775\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [136][0/2069]\tBatch Time 0.967 (0.967)\tData Time 0.733 (0.733)\tLoss 2.2759 (2.2759)\t\n",
      "Epoch: [136][200/2069]\tBatch Time 0.181 (0.215)\tData Time 0.000 (0.007)\tLoss 1.9790 (2.3196)\t\n",
      "Epoch: [136][400/2069]\tBatch Time 0.181 (0.212)\tData Time 0.000 (0.006)\tLoss 2.1537 (2.3499)\t\n",
      "Epoch: [136][600/2069]\tBatch Time 0.222 (0.209)\tData Time 0.000 (0.005)\tLoss 2.2537 (2.3440)\t\n",
      "Epoch: [136][800/2069]\tBatch Time 0.276 (0.210)\tData Time 0.016 (0.004)\tLoss 1.9683 (2.3542)\t\n",
      "Epoch: [136][1000/2069]\tBatch Time 0.211 (0.210)\tData Time 0.000 (0.005)\tLoss 1.9042 (2.3636)\t\n",
      "Epoch: [136][1200/2069]\tBatch Time 0.181 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5110 (2.3630)\t\n",
      "Epoch: [136][1400/2069]\tBatch Time 0.254 (0.210)\tData Time 0.005 (0.005)\tLoss 2.8195 (2.3688)\t\n",
      "Epoch: [136][1600/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.005)\tLoss 1.8334 (2.3606)\t\n",
      "Epoch: [136][1800/2069]\tBatch Time 0.260 (0.209)\tData Time 0.036 (0.005)\tLoss 2.6297 (2.3593)\t\n",
      "Epoch: [136][2000/2069]\tBatch Time 0.181 (0.209)\tData Time 0.000 (0.005)\tLoss 2.5329 (2.3589)\t\n",
      "[0/619]\tBatch Time 0.303 (0.303)\tLoss 3.2943 (3.2943)\t\n",
      "[200/619]\tBatch Time 0.110 (0.100)\tLoss 2.9260 (2.7214)\t\n",
      "[400/619]\tBatch Time 0.111 (0.098)\tLoss 2.7069 (2.7545)\t\n",
      "[600/619]\tBatch Time 0.100 (0.098)\tLoss 3.4545 (2.7512)\t\n",
      "\n",
      " * LOSS - 2.754\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [137][0/2069]\tBatch Time 0.759 (0.759)\tData Time 0.544 (0.544)\tLoss 1.8058 (1.8058)\t\n",
      "Epoch: [137][200/2069]\tBatch Time 0.199 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4161 (2.3866)\t\n",
      "Epoch: [137][400/2069]\tBatch Time 0.229 (0.211)\tData Time 0.001 (0.006)\tLoss 2.8656 (2.3732)\t\n",
      "Epoch: [137][600/2069]\tBatch Time 0.201 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9017 (2.3627)\t\n",
      "Epoch: [137][800/2069]\tBatch Time 0.232 (0.209)\tData Time 0.001 (0.005)\tLoss 2.9913 (2.3581)\t\n",
      "Epoch: [137][1000/2069]\tBatch Time 0.190 (0.210)\tData Time 0.000 (0.006)\tLoss 2.1819 (2.3598)\t\n",
      "Epoch: [137][1200/2069]\tBatch Time 0.220 (0.210)\tData Time 0.008 (0.006)\tLoss 2.6919 (2.3569)\t\n",
      "Epoch: [137][1400/2069]\tBatch Time 0.180 (0.210)\tData Time 0.000 (0.006)\tLoss 2.9994 (2.3626)\t\n",
      "Epoch: [137][1600/2069]\tBatch Time 0.212 (0.210)\tData Time 0.010 (0.006)\tLoss 1.9976 (2.3583)\t\n",
      "Epoch: [137][1800/2069]\tBatch Time 0.191 (0.210)\tData Time 0.001 (0.006)\tLoss 2.0525 (2.3568)\t\n",
      "Epoch: [137][2000/2069]\tBatch Time 0.192 (0.210)\tData Time 0.001 (0.006)\tLoss 2.4468 (2.3647)\t\n",
      "[0/619]\tBatch Time 0.353 (0.353)\tLoss 2.8990 (2.8990)\t\n",
      "[200/619]\tBatch Time 0.099 (0.098)\tLoss 2.4298 (2.7461)\t\n",
      "[400/619]\tBatch Time 0.103 (0.096)\tLoss 1.9376 (2.7422)\t\n",
      "[600/619]\tBatch Time 0.114 (0.096)\tLoss 2.1774 (2.7407)\t\n",
      "\n",
      " * LOSS - 2.742\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [138][0/2069]\tBatch Time 0.594 (0.594)\tData Time 0.382 (0.382)\tLoss 2.3573 (2.3573)\t\n",
      "Epoch: [138][200/2069]\tBatch Time 0.190 (0.213)\tData Time 0.000 (0.010)\tLoss 2.0012 (2.3897)\t\n",
      "Epoch: [138][400/2069]\tBatch Time 0.228 (0.211)\tData Time 0.000 (0.007)\tLoss 1.9143 (2.3566)\t\n",
      "Epoch: [138][600/2069]\tBatch Time 0.183 (0.211)\tData Time 0.000 (0.007)\tLoss 2.3590 (2.3514)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [138][800/2069]\tBatch Time 0.210 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4682 (2.3487)\t\n",
      "Epoch: [138][1000/2069]\tBatch Time 0.240 (0.210)\tData Time 0.035 (0.006)\tLoss 2.4743 (2.3373)\t\n",
      "Epoch: [138][1200/2069]\tBatch Time 0.222 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2304 (2.3403)\t\n",
      "Epoch: [138][1400/2069]\tBatch Time 0.226 (0.210)\tData Time 0.000 (0.006)\tLoss 2.5730 (2.3467)\t\n",
      "Epoch: [138][1600/2069]\tBatch Time 0.219 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0072 (2.3417)\t\n",
      "Epoch: [138][1800/2069]\tBatch Time 0.181 (0.209)\tData Time 0.000 (0.006)\tLoss 3.2070 (2.3504)\t\n",
      "Epoch: [138][2000/2069]\tBatch Time 0.292 (0.210)\tData Time 0.031 (0.006)\tLoss 2.0133 (2.3548)\t\n",
      "[0/619]\tBatch Time 0.340 (0.340)\tLoss 2.4736 (2.4736)\t\n",
      "[200/619]\tBatch Time 0.119 (0.096)\tLoss 2.6184 (2.7380)\t\n",
      "[400/619]\tBatch Time 0.118 (0.096)\tLoss 2.6500 (2.7456)\t\n",
      "[600/619]\tBatch Time 0.090 (0.097)\tLoss 2.9692 (2.7435)\t\n",
      "\n",
      " * LOSS - 2.746\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [139][0/2069]\tBatch Time 0.545 (0.545)\tData Time 0.326 (0.326)\tLoss 3.2918 (3.2918)\t\n",
      "Epoch: [139][200/2069]\tBatch Time 0.196 (0.213)\tData Time 0.000 (0.009)\tLoss 1.9694 (2.3631)\t\n",
      "Epoch: [139][400/2069]\tBatch Time 0.226 (0.211)\tData Time 0.004 (0.007)\tLoss 2.3682 (2.3508)\t\n",
      "Epoch: [139][600/2069]\tBatch Time 0.182 (0.210)\tData Time 0.000 (0.007)\tLoss 3.0474 (2.3428)\t\n",
      "Epoch: [139][800/2069]\tBatch Time 0.190 (0.210)\tData Time 0.000 (0.007)\tLoss 2.3225 (2.3450)\t\n",
      "Epoch: [139][1000/2069]\tBatch Time 0.230 (0.210)\tData Time 0.025 (0.007)\tLoss 3.1047 (2.3578)\t\n",
      "Epoch: [139][1200/2069]\tBatch Time 0.207 (0.210)\tData Time 0.000 (0.007)\tLoss 2.9226 (2.3549)\t\n",
      "Epoch: [139][1400/2069]\tBatch Time 0.204 (0.210)\tData Time 0.000 (0.007)\tLoss 2.1314 (2.3583)\t\n",
      "Epoch: [139][1600/2069]\tBatch Time 0.186 (0.209)\tData Time 0.000 (0.006)\tLoss 1.8422 (2.3560)\t\n",
      "Epoch: [139][1800/2069]\tBatch Time 0.198 (0.209)\tData Time 0.000 (0.006)\tLoss 2.8232 (2.3592)\t\n",
      "Epoch: [139][2000/2069]\tBatch Time 0.184 (0.209)\tData Time 0.000 (0.006)\tLoss 2.3419 (2.3647)\t\n",
      "[0/619]\tBatch Time 0.279 (0.279)\tLoss 2.3842 (2.3842)\t\n",
      "[200/619]\tBatch Time 0.076 (0.098)\tLoss 2.9863 (2.8358)\t\n",
      "[400/619]\tBatch Time 0.099 (0.098)\tLoss 2.4850 (2.8421)\t\n",
      "[600/619]\tBatch Time 0.097 (0.098)\tLoss 2.5135 (2.8191)\t\n",
      "\n",
      " * LOSS - 2.812\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [140][0/2069]\tBatch Time 0.816 (0.816)\tData Time 0.591 (0.591)\tLoss 2.2291 (2.2291)\t\n",
      "Epoch: [140][200/2069]\tBatch Time 0.272 (0.213)\tData Time 0.048 (0.010)\tLoss 2.2680 (2.4052)\t\n",
      "Epoch: [140][400/2069]\tBatch Time 0.190 (0.210)\tData Time 0.000 (0.006)\tLoss 2.1175 (2.3676)\t\n",
      "Epoch: [140][600/2069]\tBatch Time 0.230 (0.209)\tData Time 0.001 (0.005)\tLoss 2.1753 (2.3498)\t\n",
      "Epoch: [140][800/2069]\tBatch Time 0.196 (0.209)\tData Time 0.000 (0.005)\tLoss 1.9373 (2.3469)\t\n",
      "Epoch: [140][1000/2069]\tBatch Time 0.220 (0.209)\tData Time 0.000 (0.006)\tLoss 2.5516 (2.3455)\t\n",
      "Epoch: [140][1200/2069]\tBatch Time 0.217 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4505 (2.3452)\t\n",
      "Epoch: [140][1400/2069]\tBatch Time 0.193 (0.210)\tData Time 0.000 (0.005)\tLoss 1.8324 (2.3563)\t\n",
      "Epoch: [140][1600/2069]\tBatch Time 0.195 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5881 (2.3526)\t\n",
      "Epoch: [140][1800/2069]\tBatch Time 0.196 (0.209)\tData Time 0.001 (0.005)\tLoss 1.7121 (2.3501)\t\n",
      "Epoch: [140][2000/2069]\tBatch Time 0.205 (0.209)\tData Time 0.000 (0.005)\tLoss 2.1125 (2.3542)\t\n",
      "[0/619]\tBatch Time 0.311 (0.311)\tLoss 2.6117 (2.6117)\t\n",
      "[200/619]\tBatch Time 0.096 (0.098)\tLoss 3.3322 (2.7149)\t\n",
      "[400/619]\tBatch Time 0.113 (0.098)\tLoss 2.4446 (2.7458)\t\n",
      "[600/619]\tBatch Time 0.075 (0.097)\tLoss 2.8961 (2.7353)\t\n",
      "\n",
      " * LOSS - 2.733\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [141][0/2069]\tBatch Time 0.671 (0.671)\tData Time 0.411 (0.411)\tLoss 2.3299 (2.3299)\t\n",
      "Epoch: [141][200/2069]\tBatch Time 0.185 (0.220)\tData Time 0.000 (0.016)\tLoss 1.8501 (2.3552)\t\n",
      "Epoch: [141][400/2069]\tBatch Time 0.230 (0.216)\tData Time 0.001 (0.012)\tLoss 2.0248 (2.3373)\t\n",
      "Epoch: [141][600/2069]\tBatch Time 0.227 (0.214)\tData Time 0.001 (0.010)\tLoss 1.8040 (2.3452)\t\n",
      "Epoch: [141][800/2069]\tBatch Time 0.221 (0.213)\tData Time 0.000 (0.009)\tLoss 2.1140 (2.3460)\t\n",
      "Epoch: [141][1000/2069]\tBatch Time 0.202 (0.212)\tData Time 0.000 (0.008)\tLoss 2.0348 (2.3401)\t\n",
      "Epoch: [141][1200/2069]\tBatch Time 0.214 (0.212)\tData Time 0.000 (0.007)\tLoss 2.7509 (2.3428)\t\n",
      "Epoch: [141][1400/2069]\tBatch Time 0.200 (0.211)\tData Time 0.000 (0.007)\tLoss 3.2320 (2.3435)\t\n",
      "Epoch: [141][1600/2069]\tBatch Time 0.187 (0.211)\tData Time 0.000 (0.007)\tLoss 2.4663 (2.3509)\t\n",
      "Epoch: [141][1800/2069]\tBatch Time 0.206 (0.210)\tData Time 0.000 (0.007)\tLoss 1.9987 (2.3477)\t\n",
      "Epoch: [141][2000/2069]\tBatch Time 0.190 (0.210)\tData Time 0.003 (0.006)\tLoss 2.4754 (2.3475)\t\n",
      "[0/619]\tBatch Time 0.367 (0.367)\tLoss 2.2957 (2.2957)\t\n",
      "[200/619]\tBatch Time 0.137 (0.098)\tLoss 3.1581 (2.7679)\t\n",
      "[400/619]\tBatch Time 0.082 (0.096)\tLoss 3.2636 (2.7580)\t\n",
      "[600/619]\tBatch Time 0.117 (0.096)\tLoss 2.6839 (2.7442)\t\n",
      "\n",
      " * LOSS - 2.746\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [142][0/2069]\tBatch Time 0.909 (0.909)\tData Time 0.667 (0.667)\tLoss 2.9528 (2.9528)\t\n",
      "Epoch: [142][200/2069]\tBatch Time 0.182 (0.211)\tData Time 0.000 (0.008)\tLoss 2.0107 (2.3550)\t\n",
      "Epoch: [142][400/2069]\tBatch Time 0.188 (0.212)\tData Time 0.000 (0.008)\tLoss 3.1821 (2.3541)\t\n",
      "Epoch: [142][600/2069]\tBatch Time 0.203 (0.211)\tData Time 0.001 (0.008)\tLoss 1.9734 (2.3345)\t\n",
      "Epoch: [142][800/2069]\tBatch Time 0.194 (0.211)\tData Time 0.008 (0.007)\tLoss 2.3449 (2.3480)\t\n",
      "Epoch: [142][1000/2069]\tBatch Time 0.202 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0050 (2.3433)\t\n",
      "Epoch: [142][1200/2069]\tBatch Time 0.180 (0.211)\tData Time 0.000 (0.007)\tLoss 2.4723 (2.3409)\t\n",
      "Epoch: [142][1400/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9380 (2.3446)\t\n",
      "Epoch: [142][1600/2069]\tBatch Time 0.217 (0.210)\tData Time 0.008 (0.006)\tLoss 2.4799 (2.3372)\t\n",
      "Epoch: [142][1800/2069]\tBatch Time 0.215 (0.210)\tData Time 0.000 (0.006)\tLoss 2.5805 (2.3433)\t\n",
      "Epoch: [142][2000/2069]\tBatch Time 0.218 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6768 (2.3504)\t\n",
      "[0/619]\tBatch Time 0.301 (0.301)\tLoss 2.7227 (2.7227)\t\n",
      "[200/619]\tBatch Time 0.091 (0.100)\tLoss 2.6397 (2.6917)\t\n",
      "[400/619]\tBatch Time 0.083 (0.098)\tLoss 1.8720 (2.6865)\t\n",
      "[600/619]\tBatch Time 0.116 (0.098)\tLoss 3.0916 (2.6885)\t\n",
      "\n",
      " * LOSS - 2.693\n",
      "\n",
      "Epoch: [143][0/2069]\tBatch Time 0.701 (0.701)\tData Time 0.451 (0.451)\tLoss 2.2996 (2.2996)\t\n",
      "Epoch: [143][200/2069]\tBatch Time 0.200 (0.211)\tData Time 0.000 (0.008)\tLoss 2.7674 (2.2945)\t\n",
      "Epoch: [143][400/2069]\tBatch Time 0.186 (0.208)\tData Time 0.000 (0.006)\tLoss 3.2845 (2.3233)\t\n",
      "Epoch: [143][600/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.007)\tLoss 2.6018 (2.3416)\t\n",
      "Epoch: [143][800/2069]\tBatch Time 0.213 (0.210)\tData Time 0.000 (0.006)\tLoss 1.6933 (2.3321)\t\n",
      "Epoch: [143][1000/2069]\tBatch Time 0.207 (0.209)\tData Time 0.000 (0.006)\tLoss 1.8532 (2.3429)\t\n",
      "Epoch: [143][1200/2069]\tBatch Time 0.270 (0.209)\tData Time 0.024 (0.006)\tLoss 2.5080 (2.3450)\t\n",
      "Epoch: [143][1400/2069]\tBatch Time 0.206 (0.209)\tData Time 0.000 (0.006)\tLoss 2.2218 (2.3466)\t\n",
      "Epoch: [143][1600/2069]\tBatch Time 0.243 (0.210)\tData Time 0.008 (0.007)\tLoss 2.5200 (2.3492)\t\n",
      "Epoch: [143][1800/2069]\tBatch Time 0.246 (0.210)\tData Time 0.042 (0.007)\tLoss 1.9613 (2.3452)\t\n",
      "Epoch: [143][2000/2069]\tBatch Time 0.198 (0.210)\tData Time 0.000 (0.007)\tLoss 2.6967 (2.3575)\t\n",
      "[0/619]\tBatch Time 0.287 (0.287)\tLoss 3.0461 (3.0461)\t\n",
      "[200/619]\tBatch Time 0.084 (0.099)\tLoss 2.8882 (2.7403)\t\n",
      "[400/619]\tBatch Time 0.116 (0.098)\tLoss 2.7918 (2.7425)\t\n",
      "[600/619]\tBatch Time 0.080 (0.097)\tLoss 3.3099 (2.7332)\t\n",
      "\n",
      " * LOSS - 2.728\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [144][0/2069]\tBatch Time 0.660 (0.660)\tData Time 0.426 (0.426)\tLoss 1.9145 (1.9145)\t\n",
      "Epoch: [144][200/2069]\tBatch Time 0.294 (0.216)\tData Time 0.082 (0.012)\tLoss 2.4895 (2.3189)\t\n",
      "Epoch: [144][400/2069]\tBatch Time 0.204 (0.212)\tData Time 0.001 (0.008)\tLoss 2.5859 (2.3161)\t\n",
      "Epoch: [144][600/2069]\tBatch Time 0.193 (0.211)\tData Time 0.000 (0.006)\tLoss 1.6755 (2.3051)\t\n",
      "Epoch: [144][800/2069]\tBatch Time 0.223 (0.211)\tData Time 0.001 (0.006)\tLoss 1.8913 (2.3154)\t\n",
      "Epoch: [144][1000/2069]\tBatch Time 0.186 (0.210)\tData Time 0.001 (0.006)\tLoss 2.5841 (2.3259)\t\n",
      "Epoch: [144][1200/2069]\tBatch Time 0.204 (0.210)\tData Time 0.008 (0.006)\tLoss 2.5423 (2.3249)\t\n",
      "Epoch: [144][1400/2069]\tBatch Time 0.231 (0.210)\tData Time 0.001 (0.006)\tLoss 2.3347 (2.3264)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [144][1600/2069]\tBatch Time 0.184 (0.209)\tData Time 0.000 (0.005)\tLoss 2.7944 (2.3321)\t\n",
      "Epoch: [144][1800/2069]\tBatch Time 0.214 (0.210)\tData Time 0.008 (0.006)\tLoss 2.8135 (2.3363)\t\n",
      "Epoch: [144][2000/2069]\tBatch Time 0.219 (0.210)\tData Time 0.000 (0.006)\tLoss 1.8438 (2.3366)\t\n",
      "[0/619]\tBatch Time 0.399 (0.399)\tLoss 2.8746 (2.8746)\t\n",
      "[200/619]\tBatch Time 0.077 (0.099)\tLoss 2.5894 (2.8685)\t\n",
      "[400/619]\tBatch Time 0.121 (0.099)\tLoss 2.9235 (2.8089)\t\n",
      "[600/619]\tBatch Time 0.098 (0.098)\tLoss 2.2737 (2.8151)\t\n",
      "\n",
      " * LOSS - 2.813\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [145][0/2069]\tBatch Time 0.598 (0.598)\tData Time 0.382 (0.382)\tLoss 1.6230 (1.6230)\t\n",
      "Epoch: [145][200/2069]\tBatch Time 0.247 (0.218)\tData Time 0.000 (0.008)\tLoss 2.2412 (2.2849)\t\n",
      "Epoch: [145][400/2069]\tBatch Time 0.224 (0.218)\tData Time 0.000 (0.008)\tLoss 2.6859 (2.3033)\t\n",
      "Epoch: [145][600/2069]\tBatch Time 0.242 (0.219)\tData Time 0.000 (0.010)\tLoss 2.4618 (2.3174)\t\n",
      "Epoch: [145][800/2069]\tBatch Time 0.205 (0.216)\tData Time 0.000 (0.009)\tLoss 2.2765 (2.3052)\t\n",
      "Epoch: [145][1000/2069]\tBatch Time 0.194 (0.216)\tData Time 0.000 (0.008)\tLoss 1.8870 (2.3072)\t\n",
      "Epoch: [145][1200/2069]\tBatch Time 0.211 (0.216)\tData Time 0.000 (0.008)\tLoss 1.6433 (2.3105)\t\n",
      "Epoch: [145][1400/2069]\tBatch Time 0.202 (0.215)\tData Time 0.000 (0.008)\tLoss 1.9785 (2.3158)\t\n",
      "Epoch: [145][1600/2069]\tBatch Time 0.195 (0.215)\tData Time 0.000 (0.008)\tLoss 2.2884 (2.3219)\t\n",
      "Epoch: [145][1800/2069]\tBatch Time 0.243 (0.214)\tData Time 0.000 (0.008)\tLoss 1.8168 (2.3300)\t\n",
      "Epoch: [145][2000/2069]\tBatch Time 0.234 (0.214)\tData Time 0.008 (0.007)\tLoss 1.9560 (2.3367)\t\n",
      "[0/619]\tBatch Time 0.289 (0.289)\tLoss 2.7236 (2.7236)\t\n",
      "[200/619]\tBatch Time 0.088 (0.103)\tLoss 2.6126 (2.7530)\t\n",
      "[400/619]\tBatch Time 0.086 (0.101)\tLoss 2.5998 (2.7089)\t\n",
      "[600/619]\tBatch Time 0.135 (0.099)\tLoss 3.2171 (2.7023)\t\n",
      "\n",
      " * LOSS - 2.708\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [146][0/2069]\tBatch Time 0.659 (0.659)\tData Time 0.415 (0.415)\tLoss 2.1103 (2.1103)\t\n",
      "Epoch: [146][200/2069]\tBatch Time 0.185 (0.211)\tData Time 0.000 (0.007)\tLoss 2.4863 (2.3571)\t\n",
      "Epoch: [146][400/2069]\tBatch Time 0.201 (0.211)\tData Time 0.000 (0.005)\tLoss 2.0197 (2.3713)\t\n",
      "Epoch: [146][600/2069]\tBatch Time 0.180 (0.213)\tData Time 0.000 (0.008)\tLoss 2.4640 (2.3552)\t\n",
      "Epoch: [146][800/2069]\tBatch Time 0.206 (0.214)\tData Time 0.000 (0.009)\tLoss 2.3200 (2.3512)\t\n",
      "Epoch: [146][1000/2069]\tBatch Time 0.189 (0.214)\tData Time 0.000 (0.009)\tLoss 1.9647 (2.3510)\t\n",
      "Epoch: [146][1200/2069]\tBatch Time 0.187 (0.213)\tData Time 0.000 (0.008)\tLoss 2.0098 (2.3510)\t\n",
      "Epoch: [146][1400/2069]\tBatch Time 0.183 (0.213)\tData Time 0.002 (0.008)\tLoss 3.5266 (2.3494)\t\n",
      "Epoch: [146][1600/2069]\tBatch Time 0.800 (0.214)\tData Time 0.617 (0.009)\tLoss 3.1685 (2.3502)\t\n",
      "Epoch: [146][1800/2069]\tBatch Time 0.225 (0.214)\tData Time 0.000 (0.009)\tLoss 2.6872 (2.3493)\t\n",
      "Epoch: [146][2000/2069]\tBatch Time 0.186 (0.213)\tData Time 0.000 (0.008)\tLoss 2.3509 (2.3438)\t\n",
      "[0/619]\tBatch Time 0.275 (0.275)\tLoss 4.0506 (4.0506)\t\n",
      "[200/619]\tBatch Time 0.088 (0.098)\tLoss 2.4234 (2.7535)\t\n",
      "[400/619]\tBatch Time 0.115 (0.097)\tLoss 2.4551 (2.7316)\t\n",
      "[600/619]\tBatch Time 0.147 (0.097)\tLoss 2.6438 (2.7217)\t\n",
      "\n",
      " * LOSS - 2.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [147][0/2069]\tBatch Time 0.591 (0.591)\tData Time 0.358 (0.358)\tLoss 1.5475 (1.5475)\t\n",
      "Epoch: [147][200/2069]\tBatch Time 0.193 (0.219)\tData Time 0.000 (0.013)\tLoss 2.5982 (2.2958)\t\n",
      "Epoch: [147][400/2069]\tBatch Time 0.190 (0.214)\tData Time 0.000 (0.010)\tLoss 2.4689 (2.3221)\t\n",
      "Epoch: [147][600/2069]\tBatch Time 0.224 (0.213)\tData Time 0.025 (0.009)\tLoss 1.3543 (2.3159)\t\n",
      "Epoch: [147][800/2069]\tBatch Time 0.187 (0.213)\tData Time 0.000 (0.008)\tLoss 2.4730 (2.3086)\t\n",
      "Epoch: [147][1000/2069]\tBatch Time 0.215 (0.213)\tData Time 0.000 (0.008)\tLoss 2.1984 (2.3195)\t\n",
      "Epoch: [147][1200/2069]\tBatch Time 0.199 (0.213)\tData Time 0.000 (0.008)\tLoss 2.2647 (2.3163)\t\n",
      "Epoch: [147][1400/2069]\tBatch Time 0.195 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2366 (2.3169)\t\n",
      "Epoch: [147][1600/2069]\tBatch Time 0.217 (0.212)\tData Time 0.008 (0.007)\tLoss 3.1151 (2.3254)\t\n",
      "Epoch: [147][1800/2069]\tBatch Time 0.186 (0.213)\tData Time 0.004 (0.007)\tLoss 3.3121 (2.3291)\t\n",
      "Epoch: [147][2000/2069]\tBatch Time 0.210 (0.213)\tData Time 0.000 (0.007)\tLoss 2.6159 (2.3356)\t\n",
      "[0/619]\tBatch Time 0.305 (0.305)\tLoss 3.0878 (3.0878)\t\n",
      "[200/619]\tBatch Time 0.088 (0.098)\tLoss 2.8094 (2.7452)\t\n",
      "[400/619]\tBatch Time 0.088 (0.097)\tLoss 2.9180 (2.7607)\t\n",
      "[600/619]\tBatch Time 0.089 (0.098)\tLoss 3.4006 (2.7440)\t\n",
      "\n",
      " * LOSS - 2.747\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [148][0/2069]\tBatch Time 0.881 (0.881)\tData Time 0.662 (0.662)\tLoss 2.1910 (2.1910)\t\n",
      "Epoch: [148][200/2069]\tBatch Time 0.195 (0.218)\tData Time 0.003 (0.011)\tLoss 2.2040 (2.4154)\t\n",
      "Epoch: [148][400/2069]\tBatch Time 0.233 (0.212)\tData Time 0.000 (0.007)\tLoss 2.0332 (2.3574)\t\n",
      "Epoch: [148][600/2069]\tBatch Time 0.195 (0.212)\tData Time 0.000 (0.007)\tLoss 1.9125 (2.3649)\t\n",
      "Epoch: [148][800/2069]\tBatch Time 0.204 (0.213)\tData Time 0.000 (0.009)\tLoss 1.8401 (2.3742)\t\n",
      "Epoch: [148][1000/2069]\tBatch Time 0.228 (0.212)\tData Time 0.000 (0.007)\tLoss 2.4968 (2.3630)\t\n",
      "Epoch: [148][1200/2069]\tBatch Time 0.196 (0.212)\tData Time 0.001 (0.007)\tLoss 2.6795 (2.3591)\t\n",
      "Epoch: [148][1400/2069]\tBatch Time 0.225 (0.212)\tData Time 0.000 (0.008)\tLoss 2.0605 (2.3539)\t\n",
      "Epoch: [148][1600/2069]\tBatch Time 0.197 (0.212)\tData Time 0.000 (0.008)\tLoss 2.0595 (2.3500)\t\n",
      "Epoch: [148][1800/2069]\tBatch Time 0.203 (0.212)\tData Time 0.000 (0.007)\tLoss 2.8066 (2.3535)\t\n",
      "Epoch: [148][2000/2069]\tBatch Time 0.187 (0.212)\tData Time 0.004 (0.008)\tLoss 1.3986 (2.3544)\t\n",
      "[0/619]\tBatch Time 0.286 (0.286)\tLoss 2.2641 (2.2641)\t\n",
      "[200/619]\tBatch Time 0.077 (0.099)\tLoss 2.0652 (2.7443)\t\n",
      "[400/619]\tBatch Time 0.094 (0.096)\tLoss 2.3494 (2.7546)\t\n",
      "[600/619]\tBatch Time 0.118 (0.096)\tLoss 2.7674 (2.7636)\t\n",
      "\n",
      " * LOSS - 2.764\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [149][0/2069]\tBatch Time 0.592 (0.592)\tData Time 0.370 (0.370)\tLoss 1.8351 (1.8351)\t\n",
      "Epoch: [149][200/2069]\tBatch Time 0.180 (0.214)\tData Time 0.000 (0.011)\tLoss 2.2358 (2.3623)\t\n",
      "Epoch: [149][400/2069]\tBatch Time 0.206 (0.212)\tData Time 0.000 (0.008)\tLoss 2.5179 (2.3619)\t\n",
      "Epoch: [149][600/2069]\tBatch Time 0.205 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2417 (2.3577)\t\n",
      "Epoch: [149][800/2069]\tBatch Time 0.208 (0.213)\tData Time 0.000 (0.008)\tLoss 1.9690 (2.3575)\t\n",
      "Epoch: [149][1000/2069]\tBatch Time 0.207 (0.212)\tData Time 0.001 (0.007)\tLoss 2.2276 (2.3502)\t\n",
      "Epoch: [149][1200/2069]\tBatch Time 0.206 (0.212)\tData Time 0.000 (0.007)\tLoss 1.8181 (2.3534)\t\n",
      "Epoch: [149][1400/2069]\tBatch Time 0.224 (0.213)\tData Time 0.002 (0.008)\tLoss 2.1777 (2.3527)\t\n",
      "Epoch: [149][1600/2069]\tBatch Time 0.191 (0.213)\tData Time 0.000 (0.007)\tLoss 1.6835 (2.3497)\t\n",
      "Epoch: [149][1800/2069]\tBatch Time 0.189 (0.213)\tData Time 0.000 (0.008)\tLoss 1.3012 (2.3486)\t\n",
      "Epoch: [149][2000/2069]\tBatch Time 0.203 (0.213)\tData Time 0.000 (0.008)\tLoss 2.2454 (2.3455)\t\n",
      "[0/619]\tBatch Time 0.370 (0.370)\tLoss 3.1407 (3.1407)\t\n",
      "[200/619]\tBatch Time 0.087 (0.101)\tLoss 2.8292 (2.6401)\t\n",
      "[400/619]\tBatch Time 0.084 (0.099)\tLoss 3.2807 (2.6892)\t\n",
      "[600/619]\tBatch Time 0.090 (0.098)\tLoss 2.2215 (2.7149)\t\n",
      "\n",
      " * LOSS - 2.715\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [150][0/2069]\tBatch Time 0.961 (0.961)\tData Time 0.741 (0.741)\tLoss 1.7668 (1.7668)\t\n",
      "Epoch: [150][200/2069]\tBatch Time 0.219 (0.211)\tData Time 0.001 (0.009)\tLoss 1.8433 (2.2814)\t\n",
      "Epoch: [150][400/2069]\tBatch Time 0.193 (0.212)\tData Time 0.000 (0.008)\tLoss 2.8056 (2.3248)\t\n",
      "Epoch: [150][600/2069]\tBatch Time 0.195 (0.211)\tData Time 0.000 (0.007)\tLoss 2.0407 (2.3295)\t\n",
      "Epoch: [150][800/2069]\tBatch Time 0.194 (0.211)\tData Time 0.000 (0.007)\tLoss 2.6199 (2.3247)\t\n",
      "Epoch: [150][1000/2069]\tBatch Time 0.243 (0.213)\tData Time 0.000 (0.007)\tLoss 1.8940 (2.3302)\t\n",
      "Epoch: [150][1200/2069]\tBatch Time 0.179 (0.212)\tData Time 0.000 (0.006)\tLoss 2.8451 (2.3245)\t\n",
      "Epoch: [150][1400/2069]\tBatch Time 0.188 (0.213)\tData Time 0.002 (0.006)\tLoss 2.5541 (2.3287)\t\n",
      "Epoch: [150][1600/2069]\tBatch Time 0.208 (0.212)\tData Time 0.001 (0.006)\tLoss 1.6998 (2.3278)\t\n",
      "Epoch: [150][1800/2069]\tBatch Time 0.190 (0.212)\tData Time 0.000 (0.006)\tLoss 2.7442 (2.3303)\t\n",
      "Epoch: [150][2000/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.006)\tLoss 1.9361 (2.3392)\t\n",
      "[0/619]\tBatch Time 0.273 (0.273)\tLoss 3.7660 (3.7660)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200/619]\tBatch Time 0.109 (0.096)\tLoss 2.7426 (2.7283)\t\n",
      "[400/619]\tBatch Time 0.091 (0.095)\tLoss 2.2993 (2.7339)\t\n",
      "[600/619]\tBatch Time 0.077 (0.096)\tLoss 2.3164 (2.7345)\t\n",
      "\n",
      " * LOSS - 2.734\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [151][0/2069]\tBatch Time 1.045 (1.045)\tData Time 0.800 (0.800)\tLoss 2.7054 (2.7054)\t\n",
      "Epoch: [151][200/2069]\tBatch Time 0.181 (0.219)\tData Time 0.000 (0.013)\tLoss 2.8300 (2.3841)\t\n",
      "Epoch: [151][400/2069]\tBatch Time 0.203 (0.213)\tData Time 0.000 (0.008)\tLoss 2.9841 (2.3562)\t\n",
      "Epoch: [151][600/2069]\tBatch Time 0.187 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2796 (2.3503)\t\n",
      "Epoch: [151][800/2069]\tBatch Time 0.220 (0.211)\tData Time 0.000 (0.007)\tLoss 3.1572 (2.3624)\t\n",
      "Epoch: [151][1000/2069]\tBatch Time 0.184 (0.211)\tData Time 0.000 (0.007)\tLoss 2.8594 (2.3456)\t\n",
      "Epoch: [151][1200/2069]\tBatch Time 0.185 (0.211)\tData Time 0.000 (0.007)\tLoss 1.8225 (2.3358)\t\n",
      "Epoch: [151][1400/2069]\tBatch Time 0.193 (0.211)\tData Time 0.001 (0.007)\tLoss 3.0217 (2.3414)\t\n",
      "Epoch: [151][1600/2069]\tBatch Time 0.244 (0.211)\tData Time 0.009 (0.007)\tLoss 1.7660 (2.3368)\t\n",
      "Epoch: [151][1800/2069]\tBatch Time 0.200 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3203 (2.3443)\t\n",
      "Epoch: [151][2000/2069]\tBatch Time 0.440 (0.210)\tData Time 0.248 (0.006)\tLoss 2.4145 (2.3421)\t\n",
      "[0/619]\tBatch Time 0.317 (0.317)\tLoss 2.7659 (2.7659)\t\n",
      "[200/619]\tBatch Time 0.080 (0.096)\tLoss 3.0416 (2.6438)\t\n",
      "[400/619]\tBatch Time 0.097 (0.096)\tLoss 2.1486 (2.6847)\t\n",
      "[600/619]\tBatch Time 0.076 (0.097)\tLoss 2.6229 (2.7116)\t\n",
      "\n",
      " * LOSS - 2.714\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [152][0/2069]\tBatch Time 0.658 (0.658)\tData Time 0.432 (0.432)\tLoss 1.8556 (1.8556)\t\n",
      "Epoch: [152][200/2069]\tBatch Time 0.195 (0.214)\tData Time 0.000 (0.008)\tLoss 1.9361 (2.3936)\t\n",
      "Epoch: [152][400/2069]\tBatch Time 0.219 (0.214)\tData Time 0.011 (0.008)\tLoss 2.6287 (2.3350)\t\n",
      "Epoch: [152][600/2069]\tBatch Time 0.222 (0.214)\tData Time 0.000 (0.009)\tLoss 3.1522 (2.3168)\t\n",
      "Epoch: [152][800/2069]\tBatch Time 0.194 (0.214)\tData Time 0.000 (0.009)\tLoss 2.2686 (2.3220)\t\n",
      "Epoch: [152][1000/2069]\tBatch Time 0.183 (0.213)\tData Time 0.000 (0.008)\tLoss 2.2123 (2.3242)\t\n",
      "Epoch: [152][1200/2069]\tBatch Time 0.190 (0.212)\tData Time 0.000 (0.007)\tLoss 1.7001 (2.3219)\t\n",
      "Epoch: [152][1400/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.006)\tLoss 2.4006 (2.3259)\t\n",
      "Epoch: [152][1600/2069]\tBatch Time 0.240 (0.211)\tData Time 0.000 (0.006)\tLoss 2.8792 (2.3290)\t\n",
      "Epoch: [152][1800/2069]\tBatch Time 0.207 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3111 (2.3309)\t\n",
      "Epoch: [152][2000/2069]\tBatch Time 0.191 (0.211)\tData Time 0.003 (0.007)\tLoss 2.2274 (2.3291)\t\n",
      "[0/619]\tBatch Time 0.374 (0.374)\tLoss 2.6168 (2.6168)\t\n",
      "[200/619]\tBatch Time 0.105 (0.100)\tLoss 2.8820 (2.7245)\t\n",
      "[400/619]\tBatch Time 0.080 (0.097)\tLoss 3.0682 (2.7191)\t\n",
      "[600/619]\tBatch Time 0.078 (0.097)\tLoss 2.3692 (2.7412)\t\n",
      "\n",
      " * LOSS - 2.740\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [153][0/2069]\tBatch Time 0.936 (0.936)\tData Time 0.704 (0.704)\tLoss 2.0616 (2.0616)\t\n",
      "Epoch: [153][200/2069]\tBatch Time 0.183 (0.214)\tData Time 0.003 (0.009)\tLoss 2.5852 (2.3371)\t\n",
      "Epoch: [153][400/2069]\tBatch Time 0.191 (0.213)\tData Time 0.000 (0.008)\tLoss 2.3405 (2.3229)\t\n",
      "Epoch: [153][600/2069]\tBatch Time 0.189 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1053 (2.3147)\t\n",
      "Epoch: [153][800/2069]\tBatch Time 0.192 (0.210)\tData Time 0.002 (0.006)\tLoss 2.1821 (2.3251)\t\n",
      "Epoch: [153][1000/2069]\tBatch Time 0.198 (0.211)\tData Time 0.000 (0.007)\tLoss 1.0210 (2.3166)\t\n",
      "Epoch: [153][1200/2069]\tBatch Time 0.207 (0.211)\tData Time 0.000 (0.007)\tLoss 2.0408 (2.3223)\t\n",
      "Epoch: [153][1400/2069]\tBatch Time 0.188 (0.210)\tData Time 0.000 (0.006)\tLoss 1.3187 (2.3257)\t\n",
      "Epoch: [153][1600/2069]\tBatch Time 0.221 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3672 (2.3285)\t\n",
      "Epoch: [153][1800/2069]\tBatch Time 0.181 (0.210)\tData Time 0.000 (0.006)\tLoss 3.0448 (2.3311)\t\n",
      "Epoch: [153][2000/2069]\tBatch Time 0.192 (0.210)\tData Time 0.004 (0.006)\tLoss 2.7382 (2.3379)\t\n",
      "[0/619]\tBatch Time 0.291 (0.291)\tLoss 2.7345 (2.7345)\t\n",
      "[200/619]\tBatch Time 0.090 (0.095)\tLoss 2.7548 (2.7444)\t\n",
      "[400/619]\tBatch Time 0.073 (0.096)\tLoss 1.9010 (2.7583)\t\n",
      "[600/619]\tBatch Time 0.097 (0.096)\tLoss 2.6496 (2.7639)\t\n",
      "\n",
      " * LOSS - 2.761\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [154][0/2069]\tBatch Time 0.613 (0.613)\tData Time 0.413 (0.413)\tLoss 2.3575 (2.3575)\t\n",
      "Epoch: [154][200/2069]\tBatch Time 0.226 (0.210)\tData Time 0.001 (0.008)\tLoss 2.2780 (2.3441)\t\n",
      "Epoch: [154][400/2069]\tBatch Time 0.210 (0.209)\tData Time 0.000 (0.005)\tLoss 2.3304 (2.3486)\t\n",
      "Epoch: [154][600/2069]\tBatch Time 0.200 (0.208)\tData Time 0.000 (0.005)\tLoss 2.1485 (2.3318)\t\n",
      "Epoch: [154][800/2069]\tBatch Time 0.203 (0.209)\tData Time 0.005 (0.005)\tLoss 2.6427 (2.3429)\t\n",
      "Epoch: [154][1000/2069]\tBatch Time 0.273 (0.210)\tData Time 0.000 (0.005)\tLoss 3.1531 (2.3492)\t\n",
      "Epoch: [154][1200/2069]\tBatch Time 0.193 (0.210)\tData Time 0.000 (0.005)\tLoss 2.1485 (2.3325)\t\n",
      "Epoch: [154][1400/2069]\tBatch Time 0.220 (0.210)\tData Time 0.008 (0.005)\tLoss 2.1757 (2.3386)\t\n",
      "Epoch: [154][1600/2069]\tBatch Time 0.181 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3685 (2.3370)\t\n",
      "Epoch: [154][1800/2069]\tBatch Time 0.195 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0267 (2.3366)\t\n",
      "Epoch: [154][2000/2069]\tBatch Time 0.191 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9294 (2.3412)\t\n",
      "[0/619]\tBatch Time 0.359 (0.359)\tLoss 3.0365 (3.0365)\t\n",
      "[200/619]\tBatch Time 0.090 (0.100)\tLoss 2.7681 (2.8061)\t\n",
      "[400/619]\tBatch Time 0.121 (0.099)\tLoss 2.5397 (2.7678)\t\n",
      "[600/619]\tBatch Time 0.090 (0.098)\tLoss 2.4289 (2.7640)\t\n",
      "\n",
      " * LOSS - 2.762\n",
      "\n",
      "\n",
      "Epochs since last improvement: 12\n",
      "\n",
      "Epoch: [155][0/2069]\tBatch Time 0.627 (0.627)\tData Time 0.384 (0.384)\tLoss 2.4393 (2.4393)\t\n",
      "Epoch: [155][200/2069]\tBatch Time 0.289 (0.214)\tData Time 0.107 (0.010)\tLoss 2.5098 (2.2727)\t\n",
      "Epoch: [155][400/2069]\tBatch Time 0.187 (0.214)\tData Time 0.000 (0.010)\tLoss 2.2124 (2.2741)\t\n",
      "Epoch: [155][600/2069]\tBatch Time 0.227 (0.213)\tData Time 0.001 (0.008)\tLoss 1.8661 (2.2992)\t\n",
      "Epoch: [155][800/2069]\tBatch Time 0.210 (0.213)\tData Time 0.001 (0.008)\tLoss 1.6485 (2.3061)\t\n",
      "Epoch: [155][1000/2069]\tBatch Time 0.182 (0.212)\tData Time 0.000 (0.007)\tLoss 2.8151 (2.3404)\t\n",
      "Epoch: [155][1200/2069]\tBatch Time 0.182 (0.212)\tData Time 0.000 (0.008)\tLoss 1.3823 (2.3437)\t\n",
      "Epoch: [155][1400/2069]\tBatch Time 0.258 (0.213)\tData Time 0.000 (0.008)\tLoss 1.9146 (2.3498)\t\n",
      "Epoch: [155][1600/2069]\tBatch Time 0.182 (0.213)\tData Time 0.000 (0.008)\tLoss 2.2323 (2.3521)\t\n",
      "Epoch: [155][1800/2069]\tBatch Time 0.185 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2809 (2.3503)\t\n",
      "Epoch: [155][2000/2069]\tBatch Time 0.193 (0.212)\tData Time 0.000 (0.007)\tLoss 1.8293 (2.3435)\t\n",
      "[0/619]\tBatch Time 0.283 (0.283)\tLoss 3.6881 (3.6881)\t\n",
      "[200/619]\tBatch Time 0.097 (0.095)\tLoss 2.3405 (2.7243)\t\n",
      "[400/619]\tBatch Time 0.104 (0.095)\tLoss 2.5969 (2.7224)\t\n",
      "[600/619]\tBatch Time 0.098 (0.095)\tLoss 3.2420 (2.7207)\t\n",
      "\n",
      " * LOSS - 2.724\n",
      "\n",
      "\n",
      "Epochs since last improvement: 13\n",
      "\n",
      "Epoch: [156][0/2069]\tBatch Time 0.857 (0.857)\tData Time 0.618 (0.618)\tLoss 2.0608 (2.0608)\t\n",
      "Epoch: [156][200/2069]\tBatch Time 0.196 (0.214)\tData Time 0.000 (0.010)\tLoss 2.5548 (2.3456)\t\n",
      "Epoch: [156][400/2069]\tBatch Time 0.199 (0.212)\tData Time 0.000 (0.008)\tLoss 2.1381 (2.3383)\t\n",
      "Epoch: [156][600/2069]\tBatch Time 0.262 (0.211)\tData Time 0.000 (0.006)\tLoss 2.3057 (2.3261)\t\n",
      "Epoch: [156][800/2069]\tBatch Time 0.197 (0.212)\tData Time 0.008 (0.007)\tLoss 1.9691 (2.3314)\t\n",
      "Epoch: [156][1000/2069]\tBatch Time 0.237 (0.212)\tData Time 0.000 (0.007)\tLoss 2.6924 (2.3267)\t\n",
      "Epoch: [156][1200/2069]\tBatch Time 0.211 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2143 (2.3298)\t\n",
      "Epoch: [156][1400/2069]\tBatch Time 0.250 (0.212)\tData Time 0.000 (0.007)\tLoss 1.5375 (2.3287)\t\n",
      "Epoch: [156][1600/2069]\tBatch Time 0.209 (0.212)\tData Time 0.002 (0.006)\tLoss 1.7359 (2.3289)\t\n",
      "Epoch: [156][1800/2069]\tBatch Time 0.275 (0.212)\tData Time 0.055 (0.006)\tLoss 1.6772 (2.3359)\t\n",
      "Epoch: [156][2000/2069]\tBatch Time 0.222 (0.212)\tData Time 0.000 (0.006)\tLoss 1.7226 (2.3355)\t\n",
      "[0/619]\tBatch Time 0.315 (0.315)\tLoss 2.6955 (2.6955)\t\n",
      "[200/619]\tBatch Time 0.119 (0.098)\tLoss 2.2843 (2.7482)\t\n",
      "[400/619]\tBatch Time 0.083 (0.096)\tLoss 2.3525 (2.7461)\t\n",
      "[600/619]\tBatch Time 0.075 (0.096)\tLoss 2.3456 (2.7569)\t\n",
      "\n",
      " * LOSS - 2.758\n",
      "\n",
      "\n",
      "Epochs since last improvement: 14\n",
      "\n",
      "Epoch: [157][0/2069]\tBatch Time 0.595 (0.595)\tData Time 0.375 (0.375)\tLoss 2.5076 (2.5076)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [157][200/2069]\tBatch Time 0.244 (0.211)\tData Time 0.000 (0.007)\tLoss 2.9683 (2.3299)\t\n",
      "Epoch: [157][400/2069]\tBatch Time 0.186 (0.214)\tData Time 0.002 (0.009)\tLoss 2.4360 (2.3200)\t\n",
      "Epoch: [157][600/2069]\tBatch Time 0.216 (0.213)\tData Time 0.001 (0.008)\tLoss 2.9015 (2.3299)\t\n",
      "Epoch: [157][800/2069]\tBatch Time 0.184 (0.213)\tData Time 0.000 (0.008)\tLoss 2.1663 (2.3255)\t\n",
      "Epoch: [157][1000/2069]\tBatch Time 0.192 (0.214)\tData Time 0.000 (0.009)\tLoss 2.1875 (2.3322)\t\n",
      "Epoch: [157][1200/2069]\tBatch Time 0.181 (0.213)\tData Time 0.000 (0.008)\tLoss 2.9414 (2.3313)\t\n",
      "Epoch: [157][1400/2069]\tBatch Time 0.214 (0.212)\tData Time 0.000 (0.008)\tLoss 1.4998 (2.3335)\t\n",
      "Epoch: [157][1600/2069]\tBatch Time 0.272 (0.212)\tData Time 0.024 (0.008)\tLoss 2.7062 (2.3312)\t\n",
      "Epoch: [157][1800/2069]\tBatch Time 0.207 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3134 (2.3330)\t\n",
      "Epoch: [157][2000/2069]\tBatch Time 0.238 (0.212)\tData Time 0.005 (0.007)\tLoss 2.0531 (2.3347)\t\n",
      "[0/619]\tBatch Time 0.309 (0.309)\tLoss 2.8562 (2.8562)\t\n",
      "[200/619]\tBatch Time 0.095 (0.096)\tLoss 2.9870 (2.6734)\t\n",
      "[400/619]\tBatch Time 0.074 (0.097)\tLoss 2.6402 (2.7084)\t\n",
      "[600/619]\tBatch Time 0.097 (0.097)\tLoss 2.8893 (2.7152)\t\n",
      "\n",
      " * LOSS - 2.715\n",
      "\n",
      "\n",
      "Epochs since last improvement: 15\n",
      "\n",
      "Epoch: [158][0/2069]\tBatch Time 0.862 (0.862)\tData Time 0.623 (0.623)\tLoss 1.8777 (1.8777)\t\n",
      "Epoch: [158][200/2069]\tBatch Time 0.217 (0.213)\tData Time 0.001 (0.007)\tLoss 2.9202 (2.2893)\t\n",
      "Epoch: [158][400/2069]\tBatch Time 0.234 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1913 (2.2890)\t\n",
      "Epoch: [158][600/2069]\tBatch Time 0.183 (0.213)\tData Time 0.000 (0.007)\tLoss 3.0611 (2.3034)\t\n",
      "Epoch: [158][800/2069]\tBatch Time 0.184 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3219 (2.3045)\t\n",
      "Epoch: [158][1000/2069]\tBatch Time 0.191 (0.213)\tData Time 0.000 (0.008)\tLoss 1.9105 (2.3047)\t\n",
      "Epoch: [158][1200/2069]\tBatch Time 0.203 (0.213)\tData Time 0.000 (0.008)\tLoss 2.8789 (2.3121)\t\n",
      "Epoch: [158][1400/2069]\tBatch Time 0.187 (0.213)\tData Time 0.000 (0.008)\tLoss 2.1371 (2.3125)\t\n",
      "Epoch: [158][1600/2069]\tBatch Time 0.221 (0.212)\tData Time 0.000 (0.007)\tLoss 2.0217 (2.3115)\t\n",
      "Epoch: [158][1800/2069]\tBatch Time 0.245 (0.213)\tData Time 0.000 (0.007)\tLoss 2.1685 (2.3142)\t\n",
      "Epoch: [158][2000/2069]\tBatch Time 0.189 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2586 (2.3163)\t\n",
      "[0/619]\tBatch Time 0.343 (0.343)\tLoss 3.3018 (3.3018)\t\n",
      "[200/619]\tBatch Time 0.087 (0.098)\tLoss 2.4132 (2.7167)\t\n",
      "[400/619]\tBatch Time 0.100 (0.097)\tLoss 3.2320 (2.7264)\t\n",
      "[600/619]\tBatch Time 0.097 (0.098)\tLoss 2.3516 (2.7203)\t\n",
      "\n",
      " * LOSS - 2.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 16\n",
      "\n",
      "Epoch: [159][0/2069]\tBatch Time 0.518 (0.518)\tData Time 0.301 (0.301)\tLoss 1.8570 (1.8570)\t\n",
      "Epoch: [159][200/2069]\tBatch Time 0.195 (0.220)\tData Time 0.000 (0.012)\tLoss 1.9838 (2.3199)\t\n",
      "Epoch: [159][400/2069]\tBatch Time 0.227 (0.217)\tData Time 0.000 (0.010)\tLoss 2.0498 (2.3229)\t\n",
      "Epoch: [159][600/2069]\tBatch Time 0.194 (0.216)\tData Time 0.000 (0.010)\tLoss 2.3260 (2.3132)\t\n",
      "Epoch: [159][800/2069]\tBatch Time 0.233 (0.215)\tData Time 0.008 (0.009)\tLoss 1.9133 (2.3198)\t\n",
      "Epoch: [159][1000/2069]\tBatch Time 0.234 (0.216)\tData Time 0.000 (0.009)\tLoss 2.8874 (2.3066)\t\n",
      "Epoch: [159][1200/2069]\tBatch Time 0.210 (0.215)\tData Time 0.000 (0.009)\tLoss 2.8187 (2.3133)\t\n",
      "Epoch: [159][1400/2069]\tBatch Time 0.198 (0.215)\tData Time 0.000 (0.009)\tLoss 2.1050 (2.3183)\t\n",
      "Epoch: [159][1600/2069]\tBatch Time 0.193 (0.215)\tData Time 0.000 (0.008)\tLoss 2.4128 (2.3249)\t\n",
      "Epoch: [159][1800/2069]\tBatch Time 0.333 (0.215)\tData Time 0.148 (0.009)\tLoss 2.5858 (2.3263)\t\n",
      "Epoch: [159][2000/2069]\tBatch Time 0.187 (0.216)\tData Time 0.000 (0.009)\tLoss 1.6466 (2.3278)\t\n",
      "[0/619]\tBatch Time 0.284 (0.284)\tLoss 2.1518 (2.1518)\t\n",
      "[200/619]\tBatch Time 0.077 (0.099)\tLoss 2.7872 (2.7209)\t\n",
      "[400/619]\tBatch Time 0.109 (0.097)\tLoss 2.9947 (2.6905)\t\n",
      "[600/619]\tBatch Time 0.119 (0.098)\tLoss 2.8419 (2.6791)\t\n",
      "\n",
      " * LOSS - 2.677\n",
      "\n",
      "Epoch: [160][0/2069]\tBatch Time 0.690 (0.690)\tData Time 0.471 (0.471)\tLoss 2.4082 (2.4082)\t\n",
      "Epoch: [160][200/2069]\tBatch Time 0.243 (0.218)\tData Time 0.000 (0.012)\tLoss 2.3567 (2.3823)\t\n",
      "Epoch: [160][400/2069]\tBatch Time 0.207 (0.217)\tData Time 0.000 (0.011)\tLoss 2.7976 (2.3561)\t\n",
      "Epoch: [160][600/2069]\tBatch Time 0.189 (0.215)\tData Time 0.001 (0.009)\tLoss 2.2074 (2.3462)\t\n",
      "Epoch: [160][800/2069]\tBatch Time 0.247 (0.215)\tData Time 0.000 (0.008)\tLoss 2.2890 (2.3311)\t\n",
      "Epoch: [160][1000/2069]\tBatch Time 0.199 (0.214)\tData Time 0.000 (0.008)\tLoss 2.2277 (2.3265)\t\n",
      "Epoch: [160][1200/2069]\tBatch Time 0.201 (0.215)\tData Time 0.000 (0.008)\tLoss 2.3696 (2.3274)\t\n",
      "Epoch: [160][1400/2069]\tBatch Time 0.187 (0.214)\tData Time 0.001 (0.008)\tLoss 2.1671 (2.3261)\t\n",
      "Epoch: [160][1600/2069]\tBatch Time 0.197 (0.214)\tData Time 0.000 (0.008)\tLoss 2.1081 (2.3220)\t\n",
      "Epoch: [160][1800/2069]\tBatch Time 0.194 (0.214)\tData Time 0.004 (0.007)\tLoss 2.4968 (2.3214)\t\n",
      "Epoch: [160][2000/2069]\tBatch Time 0.182 (0.214)\tData Time 0.000 (0.007)\tLoss 2.6779 (2.3276)\t\n",
      "[0/619]\tBatch Time 0.262 (0.262)\tLoss 2.6674 (2.6674)\t\n",
      "[200/619]\tBatch Time 0.081 (0.098)\tLoss 2.8979 (2.7277)\t\n",
      "[400/619]\tBatch Time 0.102 (0.098)\tLoss 2.5020 (2.7353)\t\n",
      "[600/619]\tBatch Time 0.116 (0.097)\tLoss 2.8693 (2.7434)\t\n",
      "\n",
      " * LOSS - 2.745\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n",
      "Epoch: [161][0/2069]\tBatch Time 0.772 (0.772)\tData Time 0.547 (0.547)\tLoss 2.1361 (2.1361)\t\n",
      "Epoch: [161][200/2069]\tBatch Time 0.228 (0.213)\tData Time 0.008 (0.008)\tLoss 2.2431 (2.2559)\t\n",
      "Epoch: [161][400/2069]\tBatch Time 0.199 (0.213)\tData Time 0.000 (0.009)\tLoss 2.3095 (2.2450)\t\n",
      "Epoch: [161][600/2069]\tBatch Time 0.201 (0.213)\tData Time 0.000 (0.008)\tLoss 2.6187 (2.2742)\t\n",
      "Epoch: [161][800/2069]\tBatch Time 0.193 (0.213)\tData Time 0.000 (0.008)\tLoss 2.1128 (2.2901)\t\n",
      "Epoch: [161][1000/2069]\tBatch Time 0.250 (0.212)\tData Time 0.045 (0.007)\tLoss 3.1260 (2.2994)\t\n",
      "Epoch: [161][1200/2069]\tBatch Time 0.208 (0.213)\tData Time 0.001 (0.008)\tLoss 1.4428 (2.2992)\t\n",
      "Epoch: [161][1400/2069]\tBatch Time 0.189 (0.212)\tData Time 0.000 (0.007)\tLoss 2.1756 (2.3119)\t\n",
      "Epoch: [161][1600/2069]\tBatch Time 0.207 (0.212)\tData Time 0.000 (0.007)\tLoss 1.8213 (2.3191)\t\n",
      "Epoch: [161][1800/2069]\tBatch Time 0.189 (0.212)\tData Time 0.000 (0.006)\tLoss 1.3475 (2.3182)\t\n",
      "Epoch: [161][2000/2069]\tBatch Time 0.216 (0.212)\tData Time 0.000 (0.006)\tLoss 3.1103 (2.3215)\t\n",
      "[0/619]\tBatch Time 0.355 (0.355)\tLoss 3.0007 (3.0007)\t\n",
      "[200/619]\tBatch Time 0.128 (0.097)\tLoss 3.2476 (2.7481)\t\n",
      "[400/619]\tBatch Time 0.087 (0.097)\tLoss 2.4588 (2.7453)\t\n",
      "[600/619]\tBatch Time 0.098 (0.097)\tLoss 2.2734 (2.7393)\t\n",
      "\n",
      " * LOSS - 2.740\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n",
      "Epoch: [162][0/2069]\tBatch Time 0.800 (0.800)\tData Time 0.520 (0.520)\tLoss 1.8690 (1.8690)\t\n",
      "Epoch: [162][200/2069]\tBatch Time 0.217 (0.217)\tData Time 0.000 (0.008)\tLoss 2.9058 (2.2769)\t\n",
      "Epoch: [162][400/2069]\tBatch Time 0.210 (0.217)\tData Time 0.000 (0.008)\tLoss 2.2375 (2.2942)\t\n",
      "Epoch: [162][600/2069]\tBatch Time 0.218 (0.216)\tData Time 0.016 (0.009)\tLoss 2.6686 (2.3172)\t\n",
      "Epoch: [162][800/2069]\tBatch Time 0.208 (0.217)\tData Time 0.000 (0.010)\tLoss 2.2365 (2.3185)\t\n",
      "Epoch: [162][1000/2069]\tBatch Time 0.185 (0.216)\tData Time 0.000 (0.009)\tLoss 2.2969 (2.3162)\t\n",
      "Epoch: [162][1200/2069]\tBatch Time 0.184 (0.216)\tData Time 0.000 (0.009)\tLoss 1.7431 (2.3197)\t\n",
      "Epoch: [162][1400/2069]\tBatch Time 0.208 (0.215)\tData Time 0.000 (0.009)\tLoss 1.9441 (2.3103)\t\n",
      "Epoch: [162][1600/2069]\tBatch Time 0.192 (0.215)\tData Time 0.000 (0.008)\tLoss 2.2791 (2.3103)\t\n",
      "Epoch: [162][1800/2069]\tBatch Time 0.220 (0.215)\tData Time 0.000 (0.008)\tLoss 1.5792 (2.3071)\t\n",
      "Epoch: [162][2000/2069]\tBatch Time 0.219 (0.214)\tData Time 0.009 (0.008)\tLoss 2.5090 (2.3133)\t\n",
      "[0/619]\tBatch Time 0.281 (0.281)\tLoss 2.6278 (2.6278)\t\n",
      "[200/619]\tBatch Time 0.103 (0.098)\tLoss 2.0323 (2.7145)\t\n",
      "[400/619]\tBatch Time 0.103 (0.098)\tLoss 3.2729 (2.7113)\t\n",
      "[600/619]\tBatch Time 0.097 (0.097)\tLoss 2.3491 (2.7053)\t\n",
      "\n",
      " * LOSS - 2.704\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n",
      "Epoch: [163][0/2069]\tBatch Time 0.604 (0.604)\tData Time 0.378 (0.378)\tLoss 2.9427 (2.9427)\t\n",
      "Epoch: [163][200/2069]\tBatch Time 0.229 (0.213)\tData Time 0.000 (0.007)\tLoss 2.2827 (2.2573)\t\n",
      "Epoch: [163][400/2069]\tBatch Time 0.199 (0.211)\tData Time 0.001 (0.007)\tLoss 2.0992 (2.3044)\t\n",
      "Epoch: [163][600/2069]\tBatch Time 0.184 (0.210)\tData Time 0.000 (0.005)\tLoss 1.5219 (2.3125)\t\n",
      "Epoch: [163][800/2069]\tBatch Time 0.236 (0.210)\tData Time 0.000 (0.005)\tLoss 1.8901 (2.3211)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [163][1000/2069]\tBatch Time 0.193 (0.210)\tData Time 0.007 (0.005)\tLoss 2.3010 (2.3175)\t\n",
      "Epoch: [163][1200/2069]\tBatch Time 0.219 (0.210)\tData Time 0.000 (0.005)\tLoss 2.1658 (2.3137)\t\n",
      "Epoch: [163][1400/2069]\tBatch Time 0.319 (0.209)\tData Time 0.133 (0.005)\tLoss 2.6877 (2.3210)\t\n",
      "Epoch: [163][1600/2069]\tBatch Time 0.257 (0.210)\tData Time 0.000 (0.005)\tLoss 1.8838 (2.3226)\t\n",
      "Epoch: [163][1800/2069]\tBatch Time 0.215 (0.210)\tData Time 0.001 (0.005)\tLoss 2.5150 (2.3233)\t\n",
      "Epoch: [163][2000/2069]\tBatch Time 0.180 (0.210)\tData Time 0.000 (0.005)\tLoss 3.2705 (2.3226)\t\n",
      "[0/619]\tBatch Time 0.295 (0.295)\tLoss 3.6317 (3.6317)\t\n",
      "[200/619]\tBatch Time 0.087 (0.097)\tLoss 3.4692 (2.7073)\t\n",
      "[400/619]\tBatch Time 0.077 (0.098)\tLoss 3.1757 (2.6915)\t\n",
      "[600/619]\tBatch Time 0.096 (0.098)\tLoss 3.6061 (2.7016)\t\n",
      "\n",
      " * LOSS - 2.701\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n",
      "Epoch: [164][0/2069]\tBatch Time 0.903 (0.903)\tData Time 0.680 (0.680)\tLoss 2.4722 (2.4722)\t\n",
      "Epoch: [164][200/2069]\tBatch Time 0.211 (0.213)\tData Time 0.000 (0.007)\tLoss 1.8288 (2.2944)\t\n",
      "Epoch: [164][400/2069]\tBatch Time 0.205 (0.211)\tData Time 0.000 (0.005)\tLoss 2.5660 (2.2733)\t\n",
      "Epoch: [164][600/2069]\tBatch Time 0.261 (0.210)\tData Time 0.000 (0.004)\tLoss 2.7064 (2.2863)\t\n",
      "Epoch: [164][800/2069]\tBatch Time 0.262 (0.210)\tData Time 0.000 (0.005)\tLoss 2.9158 (2.2914)\t\n",
      "Epoch: [164][1000/2069]\tBatch Time 0.193 (0.209)\tData Time 0.000 (0.005)\tLoss 2.6770 (2.2980)\t\n",
      "Epoch: [164][1200/2069]\tBatch Time 0.240 (0.210)\tData Time 0.000 (0.005)\tLoss 2.4174 (2.3061)\t\n",
      "Epoch: [164][1400/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.005)\tLoss 2.8888 (2.3091)\t\n",
      "Epoch: [164][1600/2069]\tBatch Time 0.185 (0.210)\tData Time 0.001 (0.005)\tLoss 1.8211 (2.3051)\t\n",
      "Epoch: [164][1800/2069]\tBatch Time 0.186 (0.210)\tData Time 0.000 (0.005)\tLoss 1.9188 (2.3136)\t\n",
      "Epoch: [164][2000/2069]\tBatch Time 0.205 (0.210)\tData Time 0.000 (0.005)\tLoss 1.4179 (2.3174)\t\n",
      "[0/619]\tBatch Time 0.253 (0.253)\tLoss 2.9119 (2.9119)\t\n",
      "[200/619]\tBatch Time 0.078 (0.096)\tLoss 2.5844 (2.7416)\t\n",
      "[400/619]\tBatch Time 0.089 (0.096)\tLoss 2.4778 (2.7270)\t\n",
      "[600/619]\tBatch Time 0.103 (0.096)\tLoss 2.3655 (2.7303)\t\n",
      "\n",
      " * LOSS - 2.731\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n",
      "Epoch: [165][0/2069]\tBatch Time 0.667 (0.667)\tData Time 0.451 (0.451)\tLoss 2.3797 (2.3797)\t\n",
      "Epoch: [165][200/2069]\tBatch Time 0.272 (0.212)\tData Time 0.000 (0.009)\tLoss 2.7933 (2.3041)\t\n",
      "Epoch: [165][400/2069]\tBatch Time 0.189 (0.209)\tData Time 0.000 (0.006)\tLoss 2.3196 (2.2974)\t\n",
      "Epoch: [165][600/2069]\tBatch Time 0.214 (0.209)\tData Time 0.000 (0.005)\tLoss 2.7100 (2.3315)\t\n",
      "Epoch: [165][800/2069]\tBatch Time 0.206 (0.209)\tData Time 0.008 (0.005)\tLoss 2.3270 (2.3222)\t\n",
      "Epoch: [165][1000/2069]\tBatch Time 0.191 (0.209)\tData Time 0.000 (0.005)\tLoss 2.6815 (2.3214)\t\n",
      "Epoch: [165][1200/2069]\tBatch Time 0.180 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3343 (2.3158)\t\n",
      "Epoch: [165][1400/2069]\tBatch Time 0.195 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4268 (2.3249)\t\n",
      "Epoch: [165][1600/2069]\tBatch Time 0.237 (0.211)\tData Time 0.000 (0.006)\tLoss 2.2601 (2.3195)\t\n",
      "Epoch: [165][1800/2069]\tBatch Time 0.216 (0.210)\tData Time 0.009 (0.006)\tLoss 2.9948 (2.3195)\t\n",
      "Epoch: [165][2000/2069]\tBatch Time 0.229 (0.211)\tData Time 0.002 (0.006)\tLoss 2.7003 (2.3182)\t\n",
      "[0/619]\tBatch Time 0.397 (0.397)\tLoss 2.4067 (2.4067)\t\n",
      "[200/619]\tBatch Time 0.119 (0.099)\tLoss 3.6517 (2.7694)\t\n",
      "[400/619]\tBatch Time 0.102 (0.097)\tLoss 2.6548 (2.7959)\t\n",
      "[600/619]\tBatch Time 0.095 (0.097)\tLoss 3.5928 (2.7702)\t\n",
      "\n",
      " * LOSS - 2.768\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n",
      "Epoch: [166][0/2069]\tBatch Time 0.568 (0.568)\tData Time 0.369 (0.369)\tLoss 2.7067 (2.7067)\t\n",
      "Epoch: [166][200/2069]\tBatch Time 0.234 (0.213)\tData Time 0.000 (0.011)\tLoss 2.5098 (2.3325)\t\n",
      "Epoch: [166][400/2069]\tBatch Time 0.234 (0.211)\tData Time 0.007 (0.008)\tLoss 2.1294 (2.3173)\t\n",
      "Epoch: [166][600/2069]\tBatch Time 0.198 (0.211)\tData Time 0.000 (0.007)\tLoss 2.8159 (2.3041)\t\n",
      "Epoch: [166][800/2069]\tBatch Time 0.200 (0.211)\tData Time 0.000 (0.007)\tLoss 1.9812 (2.3163)\t\n",
      "Epoch: [166][1000/2069]\tBatch Time 0.241 (0.210)\tData Time 0.005 (0.006)\tLoss 2.0879 (2.3149)\t\n",
      "Epoch: [166][1200/2069]\tBatch Time 0.255 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4118 (2.3235)\t\n",
      "Epoch: [166][1400/2069]\tBatch Time 0.209 (0.211)\tData Time 0.000 (0.007)\tLoss 1.7065 (2.3241)\t\n",
      "Epoch: [166][1600/2069]\tBatch Time 0.201 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9631 (2.3200)\t\n",
      "Epoch: [166][1800/2069]\tBatch Time 0.215 (0.210)\tData Time 0.000 (0.006)\tLoss 2.5450 (2.3190)\t\n",
      "Epoch: [166][2000/2069]\tBatch Time 0.218 (0.210)\tData Time 0.008 (0.006)\tLoss 3.5657 (2.3232)\t\n",
      "[0/619]\tBatch Time 0.355 (0.355)\tLoss 2.7053 (2.7053)\t\n",
      "[200/619]\tBatch Time 0.101 (0.100)\tLoss 3.0122 (2.7597)\t\n",
      "[400/619]\tBatch Time 0.098 (0.100)\tLoss 2.4191 (2.7472)\t\n",
      "[600/619]\tBatch Time 0.081 (0.099)\tLoss 2.7289 (2.7599)\t\n",
      "\n",
      " * LOSS - 2.759\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n",
      "Epoch: [167][0/2069]\tBatch Time 0.573 (0.573)\tData Time 0.375 (0.375)\tLoss 1.7899 (1.7899)\t\n",
      "Epoch: [167][200/2069]\tBatch Time 0.205 (0.219)\tData Time 0.000 (0.011)\tLoss 2.4862 (2.2979)\t\n",
      "Epoch: [167][400/2069]\tBatch Time 0.269 (0.216)\tData Time 0.024 (0.011)\tLoss 2.8365 (2.3317)\t\n",
      "Epoch: [167][600/2069]\tBatch Time 0.234 (0.213)\tData Time 0.000 (0.008)\tLoss 2.1264 (2.3241)\t\n",
      "Epoch: [167][800/2069]\tBatch Time 0.181 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2577 (2.3254)\t\n",
      "Epoch: [167][1000/2069]\tBatch Time 0.220 (0.212)\tData Time 0.000 (0.007)\tLoss 2.7081 (2.3261)\t\n",
      "Epoch: [167][1200/2069]\tBatch Time 0.184 (0.212)\tData Time 0.000 (0.007)\tLoss 1.4024 (2.3294)\t\n",
      "Epoch: [167][1400/2069]\tBatch Time 0.207 (0.211)\tData Time 0.000 (0.007)\tLoss 2.3303 (2.3195)\t\n",
      "Epoch: [167][1600/2069]\tBatch Time 0.229 (0.212)\tData Time 0.008 (0.007)\tLoss 3.0366 (2.3200)\t\n",
      "Epoch: [167][1800/2069]\tBatch Time 0.226 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1803 (2.3164)\t\n",
      "Epoch: [167][2000/2069]\tBatch Time 0.182 (0.211)\tData Time 0.000 (0.006)\tLoss 2.2251 (2.3196)\t\n",
      "[0/619]\tBatch Time 0.306 (0.306)\tLoss 2.5643 (2.5643)\t\n",
      "[200/619]\tBatch Time 0.080 (0.098)\tLoss 2.0149 (2.7233)\t\n",
      "[400/619]\tBatch Time 0.085 (0.099)\tLoss 3.1897 (2.7322)\t\n",
      "[600/619]\tBatch Time 0.099 (0.098)\tLoss 2.2204 (2.7205)\t\n",
      "\n",
      " * LOSS - 2.722\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "Epoch: [168][0/2069]\tBatch Time 1.005 (1.005)\tData Time 0.769 (0.769)\tLoss 2.4207 (2.4207)\t\n",
      "Epoch: [168][200/2069]\tBatch Time 0.210 (0.213)\tData Time 0.000 (0.009)\tLoss 2.4155 (2.2913)\t\n",
      "Epoch: [168][400/2069]\tBatch Time 0.195 (0.212)\tData Time 0.000 (0.008)\tLoss 2.2640 (2.2918)\t\n",
      "Epoch: [168][600/2069]\tBatch Time 0.191 (0.211)\tData Time 0.000 (0.008)\tLoss 2.9214 (2.2926)\t\n",
      "Epoch: [168][800/2069]\tBatch Time 0.195 (0.210)\tData Time 0.000 (0.007)\tLoss 2.5407 (2.2834)\t\n",
      "Epoch: [168][1000/2069]\tBatch Time 0.211 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2773 (2.2990)\t\n",
      "Epoch: [168][1200/2069]\tBatch Time 0.216 (0.210)\tData Time 0.000 (0.006)\tLoss 2.7332 (2.3090)\t\n",
      "Epoch: [168][1400/2069]\tBatch Time 0.224 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6018 (2.3133)\t\n",
      "Epoch: [168][1600/2069]\tBatch Time 0.191 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6915 (2.3164)\t\n",
      "Epoch: [168][1800/2069]\tBatch Time 0.207 (0.210)\tData Time 0.000 (0.006)\tLoss 1.7991 (2.3225)\t\n",
      "Epoch: [168][2000/2069]\tBatch Time 0.202 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2429 (2.3280)\t\n",
      "[0/619]\tBatch Time 0.324 (0.324)\tLoss 3.0120 (3.0120)\t\n",
      "[200/619]\tBatch Time 0.089 (0.099)\tLoss 3.2830 (2.7267)\t\n",
      "[400/619]\tBatch Time 0.098 (0.098)\tLoss 2.9860 (2.7293)\t\n",
      "[600/619]\tBatch Time 0.075 (0.098)\tLoss 2.9288 (2.7331)\t\n",
      "\n",
      " * LOSS - 2.736\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n",
      "Epoch: [169][0/2069]\tBatch Time 0.587 (0.587)\tData Time 0.386 (0.386)\tLoss 2.5359 (2.5359)\t\n",
      "Epoch: [169][200/2069]\tBatch Time 0.182 (0.218)\tData Time 0.000 (0.014)\tLoss 2.2808 (2.2965)\t\n",
      "Epoch: [169][400/2069]\tBatch Time 0.193 (0.213)\tData Time 0.000 (0.009)\tLoss 1.6615 (2.2963)\t\n",
      "Epoch: [169][600/2069]\tBatch Time 0.207 (0.213)\tData Time 0.001 (0.009)\tLoss 2.1961 (2.2915)\t\n",
      "Epoch: [169][800/2069]\tBatch Time 0.204 (0.213)\tData Time 0.000 (0.008)\tLoss 2.0438 (2.3107)\t\n",
      "Epoch: [169][1000/2069]\tBatch Time 0.192 (0.213)\tData Time 0.001 (0.009)\tLoss 2.2518 (2.3074)\t\n",
      "Epoch: [169][1200/2069]\tBatch Time 0.225 (0.212)\tData Time 0.000 (0.008)\tLoss 1.8870 (2.3104)\t\n",
      "Epoch: [169][1400/2069]\tBatch Time 0.373 (0.212)\tData Time 0.145 (0.008)\tLoss 2.1393 (2.3137)\t\n",
      "Epoch: [169][1600/2069]\tBatch Time 0.213 (0.211)\tData Time 0.001 (0.008)\tLoss 2.3106 (2.3149)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [169][1800/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.007)\tLoss 2.6735 (2.3210)\t\n",
      "Epoch: [169][2000/2069]\tBatch Time 0.233 (0.211)\tData Time 0.024 (0.007)\tLoss 3.2560 (2.3220)\t\n",
      "[0/619]\tBatch Time 0.311 (0.311)\tLoss 2.0552 (2.0552)\t\n",
      "[200/619]\tBatch Time 0.100 (0.097)\tLoss 1.7513 (2.7367)\t\n",
      "[400/619]\tBatch Time 0.093 (0.096)\tLoss 3.0650 (2.7360)\t\n",
      "[600/619]\tBatch Time 0.089 (0.096)\tLoss 1.8656 (2.7507)\t\n",
      "\n",
      " * LOSS - 2.757\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n",
      "Epoch: [170][0/2069]\tBatch Time 0.687 (0.687)\tData Time 0.439 (0.439)\tLoss 2.0087 (2.0087)\t\n",
      "Epoch: [170][200/2069]\tBatch Time 0.207 (0.214)\tData Time 0.000 (0.010)\tLoss 2.2501 (2.3073)\t\n",
      "Epoch: [170][400/2069]\tBatch Time 0.219 (0.211)\tData Time 0.000 (0.006)\tLoss 2.4520 (2.2960)\t\n",
      "Epoch: [170][600/2069]\tBatch Time 0.186 (0.210)\tData Time 0.000 (0.007)\tLoss 2.0720 (2.2901)\t\n",
      "Epoch: [170][800/2069]\tBatch Time 0.217 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2647 (2.3100)\t\n",
      "Epoch: [170][1000/2069]\tBatch Time 0.196 (0.211)\tData Time 0.000 (0.006)\tLoss 1.6815 (2.3133)\t\n",
      "Epoch: [170][1200/2069]\tBatch Time 0.186 (0.211)\tData Time 0.000 (0.007)\tLoss 1.7058 (2.3116)\t\n",
      "Epoch: [170][1400/2069]\tBatch Time 0.196 (0.211)\tData Time 0.000 (0.007)\tLoss 2.5370 (2.3147)\t\n",
      "Epoch: [170][1600/2069]\tBatch Time 0.206 (0.211)\tData Time 0.000 (0.006)\tLoss 2.7455 (2.3130)\t\n",
      "Epoch: [170][1800/2069]\tBatch Time 0.191 (0.211)\tData Time 0.000 (0.006)\tLoss 2.2262 (2.3143)\t\n",
      "Epoch: [170][2000/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.006)\tLoss 2.4145 (2.3105)\t\n",
      "[0/619]\tBatch Time 0.351 (0.351)\tLoss 3.1230 (3.1230)\t\n",
      "[200/619]\tBatch Time 0.104 (0.095)\tLoss 1.2465 (2.8004)\t\n",
      "[400/619]\tBatch Time 0.072 (0.096)\tLoss 2.6328 (2.8183)\t\n",
      "[600/619]\tBatch Time 0.089 (0.096)\tLoss 1.9617 (2.8255)\t\n",
      "\n",
      " * LOSS - 2.825\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n",
      "Epoch: [171][0/2069]\tBatch Time 0.860 (0.860)\tData Time 0.648 (0.648)\tLoss 3.4089 (3.4089)\t\n",
      "Epoch: [171][200/2069]\tBatch Time 0.227 (0.216)\tData Time 0.000 (0.010)\tLoss 2.4396 (2.3159)\t\n",
      "Epoch: [171][400/2069]\tBatch Time 0.183 (0.213)\tData Time 0.000 (0.007)\tLoss 2.1696 (2.3383)\t\n",
      "Epoch: [171][600/2069]\tBatch Time 0.206 (0.210)\tData Time 0.008 (0.005)\tLoss 3.2940 (2.3169)\t\n",
      "Epoch: [171][800/2069]\tBatch Time 0.185 (0.209)\tData Time 0.000 (0.005)\tLoss 1.5998 (2.3050)\t\n",
      "Epoch: [171][1000/2069]\tBatch Time 0.219 (0.210)\tData Time 0.000 (0.005)\tLoss 1.9528 (2.3054)\t\n",
      "Epoch: [171][1200/2069]\tBatch Time 0.198 (0.209)\tData Time 0.001 (0.005)\tLoss 2.0342 (2.3104)\t\n",
      "Epoch: [171][1400/2069]\tBatch Time 0.207 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3914 (2.3144)\t\n",
      "Epoch: [171][1600/2069]\tBatch Time 0.189 (0.210)\tData Time 0.000 (0.006)\tLoss 2.7102 (2.3216)\t\n",
      "Epoch: [171][1800/2069]\tBatch Time 0.213 (0.210)\tData Time 0.000 (0.005)\tLoss 2.6202 (2.3214)\t\n",
      "Epoch: [171][2000/2069]\tBatch Time 0.241 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5925 (2.3226)\t\n",
      "[0/619]\tBatch Time 0.310 (0.310)\tLoss 2.2764 (2.2764)\t\n",
      "[200/619]\tBatch Time 0.115 (0.098)\tLoss 2.1210 (2.6862)\t\n",
      "[400/619]\tBatch Time 0.100 (0.097)\tLoss 3.2040 (2.6815)\t\n",
      "[600/619]\tBatch Time 0.077 (0.097)\tLoss 3.4794 (2.7051)\t\n",
      "\n",
      " * LOSS - 2.704\n",
      "\n",
      "\n",
      "Epochs since last improvement: 12\n",
      "\n",
      "Epoch: [172][0/2069]\tBatch Time 0.995 (0.995)\tData Time 0.736 (0.736)\tLoss 2.1477 (2.1477)\t\n",
      "Epoch: [172][200/2069]\tBatch Time 0.185 (0.215)\tData Time 0.001 (0.009)\tLoss 2.4861 (2.3008)\t\n",
      "Epoch: [172][400/2069]\tBatch Time 0.352 (0.214)\tData Time 0.000 (0.009)\tLoss 2.3774 (2.3082)\t\n",
      "Epoch: [172][600/2069]\tBatch Time 0.233 (0.212)\tData Time 0.001 (0.007)\tLoss 2.7861 (2.3233)\t\n",
      "Epoch: [172][800/2069]\tBatch Time 0.179 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1252 (2.3178)\t\n",
      "Epoch: [172][1000/2069]\tBatch Time 0.200 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1903 (2.3147)\t\n",
      "Epoch: [172][1200/2069]\tBatch Time 0.192 (0.211)\tData Time 0.001 (0.006)\tLoss 2.6057 (2.3096)\t\n",
      "Epoch: [172][1400/2069]\tBatch Time 0.194 (0.211)\tData Time 0.000 (0.006)\tLoss 2.3848 (2.3104)\t\n",
      "Epoch: [172][1600/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5898 (2.3110)\t\n",
      "Epoch: [172][1800/2069]\tBatch Time 0.217 (0.210)\tData Time 0.000 (0.005)\tLoss 2.6263 (2.3165)\t\n",
      "Epoch: [172][2000/2069]\tBatch Time 0.189 (0.210)\tData Time 0.000 (0.005)\tLoss 2.9109 (2.3200)\t\n",
      "[0/619]\tBatch Time 0.305 (0.305)\tLoss 2.8202 (2.8202)\t\n",
      "[200/619]\tBatch Time 0.084 (0.097)\tLoss 2.9129 (2.7230)\t\n",
      "[400/619]\tBatch Time 0.132 (0.097)\tLoss 3.0093 (2.7260)\t\n",
      "[600/619]\tBatch Time 0.105 (0.097)\tLoss 2.0628 (2.7404)\t\n",
      "\n",
      " * LOSS - 2.738\n",
      "\n",
      "\n",
      "Epochs since last improvement: 13\n",
      "\n",
      "Epoch: [173][0/2069]\tBatch Time 0.520 (0.520)\tData Time 0.305 (0.305)\tLoss 2.0030 (2.0030)\t\n",
      "Epoch: [173][200/2069]\tBatch Time 0.208 (0.215)\tData Time 0.000 (0.011)\tLoss 2.6579 (2.3230)\t\n",
      "Epoch: [173][400/2069]\tBatch Time 0.185 (0.212)\tData Time 0.001 (0.008)\tLoss 2.6270 (2.2823)\t\n",
      "Epoch: [173][600/2069]\tBatch Time 0.212 (0.211)\tData Time 0.000 (0.008)\tLoss 1.7508 (2.3116)\t\n",
      "Epoch: [173][800/2069]\tBatch Time 0.218 (0.211)\tData Time 0.001 (0.007)\tLoss 2.0033 (2.3115)\t\n",
      "Epoch: [173][1000/2069]\tBatch Time 0.194 (0.210)\tData Time 0.000 (0.006)\tLoss 2.1489 (2.2997)\t\n",
      "Epoch: [173][1200/2069]\tBatch Time 0.184 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3223 (2.3037)\t\n",
      "Epoch: [173][1400/2069]\tBatch Time 0.266 (0.210)\tData Time 0.023 (0.006)\tLoss 1.9910 (2.3098)\t\n",
      "Epoch: [173][1600/2069]\tBatch Time 0.230 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3900 (2.3013)\t\n",
      "Epoch: [173][1800/2069]\tBatch Time 0.200 (0.210)\tData Time 0.000 (0.006)\tLoss 1.8579 (2.3030)\t\n",
      "Epoch: [173][2000/2069]\tBatch Time 0.192 (0.210)\tData Time 0.000 (0.006)\tLoss 2.7037 (2.3064)\t\n",
      "[0/619]\tBatch Time 0.351 (0.351)\tLoss 2.8945 (2.8945)\t\n",
      "[200/619]\tBatch Time 0.107 (0.098)\tLoss 2.2606 (2.7303)\t\n",
      "[400/619]\tBatch Time 0.087 (0.098)\tLoss 2.3005 (2.7441)\t\n",
      "[600/619]\tBatch Time 0.078 (0.098)\tLoss 3.1005 (2.7481)\t\n",
      "\n",
      " * LOSS - 2.743\n",
      "\n",
      "\n",
      "Epochs since last improvement: 14\n",
      "\n",
      "Epoch: [174][0/2069]\tBatch Time 0.816 (0.816)\tData Time 0.612 (0.612)\tLoss 3.4349 (3.4349)\t\n",
      "Epoch: [174][200/2069]\tBatch Time 0.184 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1564 (2.3555)\t\n",
      "Epoch: [174][400/2069]\tBatch Time 0.214 (0.209)\tData Time 0.001 (0.005)\tLoss 1.6001 (2.3211)\t\n",
      "Epoch: [174][600/2069]\tBatch Time 0.221 (0.208)\tData Time 0.000 (0.004)\tLoss 2.8341 (2.2990)\t\n",
      "Epoch: [174][800/2069]\tBatch Time 0.191 (0.209)\tData Time 0.000 (0.005)\tLoss 1.8006 (2.3046)\t\n",
      "Epoch: [174][1000/2069]\tBatch Time 0.210 (0.208)\tData Time 0.000 (0.004)\tLoss 1.7514 (2.3103)\t\n",
      "Epoch: [174][1200/2069]\tBatch Time 0.201 (0.209)\tData Time 0.000 (0.004)\tLoss 2.8611 (2.3151)\t\n",
      "Epoch: [174][1400/2069]\tBatch Time 0.227 (0.208)\tData Time 0.001 (0.004)\tLoss 2.6562 (2.3134)\t\n",
      "Epoch: [174][1600/2069]\tBatch Time 0.186 (0.209)\tData Time 0.001 (0.004)\tLoss 2.4586 (2.3123)\t\n",
      "Epoch: [174][1800/2069]\tBatch Time 0.187 (0.209)\tData Time 0.000 (0.005)\tLoss 2.4119 (2.3124)\t\n",
      "Epoch: [174][2000/2069]\tBatch Time 0.191 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3909 (2.3150)\t\n",
      "[0/619]\tBatch Time 0.350 (0.350)\tLoss 3.1173 (3.1173)\t\n",
      "[200/619]\tBatch Time 0.084 (0.099)\tLoss 2.3459 (2.7568)\t\n",
      "[400/619]\tBatch Time 0.091 (0.096)\tLoss 1.9819 (2.7831)\t\n",
      "[600/619]\tBatch Time 0.100 (0.097)\tLoss 3.2556 (2.7572)\t\n",
      "\n",
      " * LOSS - 2.755\n",
      "\n",
      "\n",
      "Epochs since last improvement: 15\n",
      "\n",
      "Epoch: [175][0/2069]\tBatch Time 0.737 (0.737)\tData Time 0.504 (0.504)\tLoss 2.5072 (2.5072)\t\n",
      "Epoch: [175][200/2069]\tBatch Time 0.189 (0.213)\tData Time 0.000 (0.008)\tLoss 2.3194 (2.3037)\t\n",
      "Epoch: [175][400/2069]\tBatch Time 0.212 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3751 (2.3066)\t\n",
      "Epoch: [175][600/2069]\tBatch Time 0.236 (0.210)\tData Time 0.000 (0.005)\tLoss 2.0219 (2.3068)\t\n",
      "Epoch: [175][800/2069]\tBatch Time 0.214 (0.211)\tData Time 0.000 (0.007)\tLoss 2.9390 (2.2952)\t\n",
      "Epoch: [175][1000/2069]\tBatch Time 0.188 (0.211)\tData Time 0.000 (0.007)\tLoss 2.3940 (2.2931)\t\n",
      "Epoch: [175][1200/2069]\tBatch Time 0.226 (0.211)\tData Time 0.000 (0.006)\tLoss 2.7236 (2.2952)\t\n",
      "Epoch: [175][1400/2069]\tBatch Time 0.195 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1311 (2.2959)\t\n",
      "Epoch: [175][1600/2069]\tBatch Time 0.225 (0.211)\tData Time 0.008 (0.007)\tLoss 1.5553 (2.2958)\t\n",
      "Epoch: [175][1800/2069]\tBatch Time 0.224 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1496 (2.2965)\t\n",
      "Epoch: [175][2000/2069]\tBatch Time 0.201 (0.211)\tData Time 0.000 (0.006)\tLoss 2.6327 (2.2960)\t\n",
      "[0/619]\tBatch Time 0.264 (0.264)\tLoss 2.8290 (2.8290)\t\n",
      "[200/619]\tBatch Time 0.106 (0.097)\tLoss 3.7029 (2.7300)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400/619]\tBatch Time 0.109 (0.098)\tLoss 2.0841 (2.7266)\t\n",
      "[600/619]\tBatch Time 0.085 (0.099)\tLoss 3.7547 (2.7416)\t\n",
      "\n",
      " * LOSS - 2.738\n",
      "\n",
      "\n",
      "Epochs since last improvement: 16\n",
      "\n",
      "Epoch: [176][0/2069]\tBatch Time 0.750 (0.750)\tData Time 0.503 (0.503)\tLoss 3.1056 (3.1056)\t\n",
      "Epoch: [176][200/2069]\tBatch Time 0.192 (0.216)\tData Time 0.000 (0.010)\tLoss 2.5008 (2.3290)\t\n",
      "Epoch: [176][400/2069]\tBatch Time 0.184 (0.215)\tData Time 0.000 (0.009)\tLoss 1.6689 (2.2955)\t\n",
      "Epoch: [176][600/2069]\tBatch Time 0.222 (0.212)\tData Time 0.008 (0.007)\tLoss 3.1760 (2.2936)\t\n",
      "Epoch: [176][800/2069]\tBatch Time 0.200 (0.211)\tData Time 0.009 (0.007)\tLoss 1.9302 (2.2901)\t\n",
      "Epoch: [176][1000/2069]\tBatch Time 0.208 (0.211)\tData Time 0.000 (0.006)\tLoss 3.1411 (2.2880)\t\n",
      "Epoch: [176][1200/2069]\tBatch Time 0.205 (0.211)\tData Time 0.000 (0.006)\tLoss 1.5644 (2.2931)\t\n",
      "Epoch: [176][1400/2069]\tBatch Time 0.203 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6747 (2.2987)\t\n",
      "Epoch: [176][1600/2069]\tBatch Time 0.182 (0.211)\tData Time 0.000 (0.006)\tLoss 2.0489 (2.2938)\t\n",
      "Epoch: [176][1800/2069]\tBatch Time 0.210 (0.210)\tData Time 0.000 (0.006)\tLoss 1.3120 (2.2954)\t\n",
      "Epoch: [176][2000/2069]\tBatch Time 0.213 (0.211)\tData Time 0.000 (0.006)\tLoss 2.7351 (2.2940)\t\n",
      "[0/619]\tBatch Time 0.288 (0.288)\tLoss 3.0254 (3.0254)\t\n",
      "[200/619]\tBatch Time 0.099 (0.100)\tLoss 3.3610 (2.7341)\t\n",
      "[400/619]\tBatch Time 0.098 (0.098)\tLoss 2.0421 (2.7603)\t\n",
      "[600/619]\tBatch Time 0.126 (0.098)\tLoss 2.4600 (2.7724)\t\n",
      "\n",
      " * LOSS - 2.768\n",
      "\n",
      "\n",
      "Epochs since last improvement: 17\n",
      "\n",
      "Epoch: [177][0/2069]\tBatch Time 0.863 (0.863)\tData Time 0.650 (0.650)\tLoss 2.4631 (2.4631)\t\n",
      "Epoch: [177][200/2069]\tBatch Time 0.211 (0.214)\tData Time 0.010 (0.009)\tLoss 2.0661 (2.2513)\t\n",
      "Epoch: [177][400/2069]\tBatch Time 0.204 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2294 (2.2515)\t\n",
      "Epoch: [177][600/2069]\tBatch Time 0.236 (0.211)\tData Time 0.000 (0.006)\tLoss 2.0481 (2.2546)\t\n",
      "Epoch: [177][800/2069]\tBatch Time 0.213 (0.210)\tData Time 0.007 (0.006)\tLoss 3.2690 (2.2654)\t\n",
      "Epoch: [177][1000/2069]\tBatch Time 0.233 (0.210)\tData Time 0.000 (0.006)\tLoss 2.9640 (2.2701)\t\n",
      "Epoch: [177][1200/2069]\tBatch Time 0.229 (0.210)\tData Time 0.000 (0.006)\tLoss 2.1254 (2.2730)\t\n",
      "Epoch: [177][1400/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.006)\tLoss 1.7952 (2.2877)\t\n",
      "Epoch: [177][1600/2069]\tBatch Time 0.185 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4697 (2.2881)\t\n",
      "Epoch: [177][1800/2069]\tBatch Time 0.189 (0.209)\tData Time 0.000 (0.006)\tLoss 1.3486 (2.2837)\t\n",
      "Epoch: [177][2000/2069]\tBatch Time 0.194 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2802 (2.2869)\t\n",
      "[0/619]\tBatch Time 0.311 (0.311)\tLoss 3.5942 (3.5942)\t\n",
      "[200/619]\tBatch Time 0.119 (0.097)\tLoss 2.7191 (2.8265)\t\n",
      "[400/619]\tBatch Time 0.138 (0.098)\tLoss 2.9598 (2.7979)\t\n",
      "[600/619]\tBatch Time 0.099 (0.098)\tLoss 3.5669 (2.7938)\t\n",
      "\n",
      " * LOSS - 2.795\n",
      "\n",
      "\n",
      "Epochs since last improvement: 18\n",
      "\n",
      "Epoch: [178][0/2069]\tBatch Time 0.897 (0.897)\tData Time 0.677 (0.677)\tLoss 3.2612 (3.2612)\t\n",
      "Epoch: [178][200/2069]\tBatch Time 0.266 (0.218)\tData Time 0.000 (0.011)\tLoss 2.0970 (2.2888)\t\n",
      "Epoch: [178][400/2069]\tBatch Time 0.213 (0.215)\tData Time 0.000 (0.009)\tLoss 2.4003 (2.3201)\t\n",
      "Epoch: [178][600/2069]\tBatch Time 0.186 (0.213)\tData Time 0.001 (0.008)\tLoss 1.8645 (2.3171)\t\n",
      "Epoch: [178][800/2069]\tBatch Time 0.186 (0.212)\tData Time 0.000 (0.007)\tLoss 1.6318 (2.3131)\t\n",
      "Epoch: [178][1000/2069]\tBatch Time 0.192 (0.213)\tData Time 0.000 (0.007)\tLoss 1.9430 (2.3146)\t\n",
      "Epoch: [178][1200/2069]\tBatch Time 0.254 (0.212)\tData Time 0.026 (0.007)\tLoss 1.7303 (2.3010)\t\n",
      "Epoch: [178][1400/2069]\tBatch Time 0.218 (0.211)\tData Time 0.009 (0.007)\tLoss 1.8482 (2.2950)\t\n",
      "Epoch: [178][1600/2069]\tBatch Time 0.211 (0.211)\tData Time 0.000 (0.007)\tLoss 2.4389 (2.2968)\t\n",
      "Epoch: [178][1800/2069]\tBatch Time 0.196 (0.211)\tData Time 0.000 (0.006)\tLoss 1.0531 (2.2959)\t\n",
      "Epoch: [178][2000/2069]\tBatch Time 0.201 (0.211)\tData Time 0.000 (0.006)\tLoss 1.7319 (2.2907)\t\n",
      "[0/619]\tBatch Time 0.333 (0.333)\tLoss 2.8622 (2.8622)\t\n",
      "[200/619]\tBatch Time 0.102 (0.099)\tLoss 3.0856 (2.7086)\t\n",
      "[400/619]\tBatch Time 0.080 (0.099)\tLoss 1.8650 (2.7269)\t\n",
      "[600/619]\tBatch Time 0.117 (0.098)\tLoss 3.4512 (2.7286)\t\n",
      "\n",
      " * LOSS - 2.730\n",
      "\n",
      "\n",
      "Epochs since last improvement: 19\n",
      "\n",
      "Epoch: [179][0/2069]\tBatch Time 0.664 (0.664)\tData Time 0.430 (0.430)\tLoss 2.2503 (2.2503)\t\n",
      "Epoch: [179][200/2069]\tBatch Time 0.193 (0.216)\tData Time 0.000 (0.011)\tLoss 1.9076 (2.2456)\t\n",
      "Epoch: [179][400/2069]\tBatch Time 0.210 (0.213)\tData Time 0.004 (0.009)\tLoss 2.0522 (2.2445)\t\n",
      "Epoch: [179][600/2069]\tBatch Time 0.219 (0.211)\tData Time 0.000 (0.007)\tLoss 1.9038 (2.2744)\t\n",
      "Epoch: [179][800/2069]\tBatch Time 0.196 (0.211)\tData Time 0.000 (0.006)\tLoss 2.8704 (2.2784)\t\n",
      "Epoch: [179][1000/2069]\tBatch Time 0.229 (0.210)\tData Time 0.008 (0.006)\tLoss 2.3959 (2.2820)\t\n",
      "Epoch: [179][1200/2069]\tBatch Time 0.273 (0.211)\tData Time 0.050 (0.007)\tLoss 2.5242 (2.2856)\t\n",
      "Epoch: [179][1400/2069]\tBatch Time 0.195 (0.211)\tData Time 0.000 (0.006)\tLoss 2.2949 (2.2879)\t\n",
      "Epoch: [179][1600/2069]\tBatch Time 0.179 (0.211)\tData Time 0.001 (0.006)\tLoss 2.6657 (2.2879)\t\n",
      "Epoch: [179][1800/2069]\tBatch Time 0.189 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6517 (2.2918)\t\n",
      "Epoch: [179][2000/2069]\tBatch Time 0.209 (0.210)\tData Time 0.000 (0.006)\tLoss 3.4954 (2.2928)\t\n",
      "[0/619]\tBatch Time 0.293 (0.293)\tLoss 3.3611 (3.3611)\t\n",
      "[200/619]\tBatch Time 0.100 (0.099)\tLoss 2.3463 (2.6858)\t\n",
      "[400/619]\tBatch Time 0.106 (0.098)\tLoss 2.6382 (2.6727)\t\n",
      "[600/619]\tBatch Time 0.096 (0.098)\tLoss 2.7088 (2.6769)\t\n",
      "\n",
      " * LOSS - 2.681\n",
      "\n",
      "\n",
      "Epochs since last improvement: 20\n",
      "\n",
      "Epoch: [180][0/2069]\tBatch Time 0.860 (0.860)\tData Time 0.627 (0.627)\tLoss 1.8066 (1.8066)\t\n",
      "Epoch: [180][200/2069]\tBatch Time 0.187 (0.216)\tData Time 0.000 (0.011)\tLoss 2.8173 (2.3228)\t\n",
      "Epoch: [180][400/2069]\tBatch Time 0.252 (0.217)\tData Time 0.000 (0.011)\tLoss 2.7533 (2.3390)\t\n",
      "Epoch: [180][600/2069]\tBatch Time 0.228 (0.214)\tData Time 0.008 (0.009)\tLoss 2.6017 (2.3137)\t\n",
      "Epoch: [180][800/2069]\tBatch Time 0.193 (0.213)\tData Time 0.000 (0.007)\tLoss 1.6872 (2.3062)\t\n",
      "Epoch: [180][1000/2069]\tBatch Time 0.447 (0.213)\tData Time 0.221 (0.008)\tLoss 1.7518 (2.3101)\t\n",
      "Epoch: [180][1200/2069]\tBatch Time 0.199 (0.212)\tData Time 0.001 (0.007)\tLoss 2.3992 (2.3140)\t\n",
      "Epoch: [180][1400/2069]\tBatch Time 0.210 (0.211)\tData Time 0.000 (0.006)\tLoss 2.3990 (2.3078)\t\n",
      "Epoch: [180][1600/2069]\tBatch Time 0.188 (0.211)\tData Time 0.000 (0.006)\tLoss 2.4236 (2.3139)\t\n",
      "Epoch: [180][1800/2069]\tBatch Time 0.224 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1562 (2.3162)\t\n",
      "Epoch: [180][2000/2069]\tBatch Time 0.360 (0.211)\tData Time 0.156 (0.006)\tLoss 2.8472 (2.3154)\t\n",
      "[0/619]\tBatch Time 0.300 (0.300)\tLoss 2.3388 (2.3388)\t\n",
      "[200/619]\tBatch Time 0.118 (0.099)\tLoss 3.7479 (2.7709)\t\n",
      "[400/619]\tBatch Time 0.097 (0.097)\tLoss 2.6424 (2.7306)\t\n",
      "[600/619]\tBatch Time 0.093 (0.097)\tLoss 2.3595 (2.7526)\t\n",
      "\n",
      " * LOSS - 2.756\n",
      "\n",
      "\n",
      "Epochs since last improvement: 21\n",
      "\n",
      "Epoch: [181][0/2069]\tBatch Time 0.698 (0.698)\tData Time 0.470 (0.470)\tLoss 1.6525 (1.6525)\t\n",
      "Epoch: [181][200/2069]\tBatch Time 0.201 (0.212)\tData Time 0.000 (0.006)\tLoss 1.8270 (2.2625)\t\n",
      "Epoch: [181][400/2069]\tBatch Time 0.273 (0.212)\tData Time 0.037 (0.006)\tLoss 2.6537 (2.2996)\t\n",
      "Epoch: [181][600/2069]\tBatch Time 0.438 (0.211)\tData Time 0.225 (0.007)\tLoss 1.8194 (2.2738)\t\n",
      "Epoch: [181][800/2069]\tBatch Time 0.234 (0.210)\tData Time 0.000 (0.006)\tLoss 1.7737 (2.2664)\t\n",
      "Epoch: [181][1000/2069]\tBatch Time 0.206 (0.209)\tData Time 0.000 (0.006)\tLoss 2.6620 (2.2673)\t\n",
      "Epoch: [181][1200/2069]\tBatch Time 0.201 (0.210)\tData Time 0.004 (0.006)\tLoss 2.4597 (2.2700)\t\n",
      "Epoch: [181][1400/2069]\tBatch Time 0.189 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2342 (2.2763)\t\n",
      "Epoch: [181][1600/2069]\tBatch Time 0.205 (0.210)\tData Time 0.000 (0.006)\tLoss 2.4315 (2.2802)\t\n",
      "Epoch: [181][1800/2069]\tBatch Time 0.179 (0.211)\tData Time 0.000 (0.006)\tLoss 1.9372 (2.2911)\t\n",
      "Epoch: [181][2000/2069]\tBatch Time 0.180 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1129 (2.2956)\t\n",
      "[0/619]\tBatch Time 0.305 (0.305)\tLoss 3.0622 (3.0622)\t\n",
      "[200/619]\tBatch Time 0.109 (0.098)\tLoss 2.6397 (2.7414)\t\n",
      "[400/619]\tBatch Time 0.095 (0.097)\tLoss 2.2451 (2.7293)\t\n",
      "[600/619]\tBatch Time 0.102 (0.097)\tLoss 2.7347 (2.7347)\t\n",
      "\n",
      " * LOSS - 2.738\n",
      "\n",
      "\n",
      "Epochs since last improvement: 22\n",
      "\n",
      "Epoch: [182][0/2069]\tBatch Time 0.917 (0.917)\tData Time 0.693 (0.693)\tLoss 2.1377 (2.1377)\t\n",
      "Epoch: [182][200/2069]\tBatch Time 0.184 (0.211)\tData Time 0.000 (0.010)\tLoss 1.3160 (2.2560)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [182][400/2069]\tBatch Time 0.224 (0.210)\tData Time 0.004 (0.007)\tLoss 2.4987 (2.2707)\t\n",
      "Epoch: [182][600/2069]\tBatch Time 0.235 (0.209)\tData Time 0.000 (0.005)\tLoss 2.3991 (2.2742)\t\n",
      "Epoch: [182][800/2069]\tBatch Time 0.216 (0.209)\tData Time 0.000 (0.005)\tLoss 2.6716 (2.2759)\t\n",
      "Epoch: [182][1000/2069]\tBatch Time 0.200 (0.209)\tData Time 0.000 (0.005)\tLoss 2.8608 (2.2724)\t\n",
      "Epoch: [182][1200/2069]\tBatch Time 0.299 (0.209)\tData Time 0.094 (0.005)\tLoss 1.6280 (2.2826)\t\n",
      "Epoch: [182][1400/2069]\tBatch Time 0.200 (0.209)\tData Time 0.000 (0.006)\tLoss 2.0863 (2.2865)\t\n",
      "Epoch: [182][1600/2069]\tBatch Time 0.253 (0.209)\tData Time 0.006 (0.006)\tLoss 2.1485 (2.2902)\t\n",
      "Epoch: [182][1800/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.006)\tLoss 2.8851 (2.2965)\t\n",
      "Epoch: [182][2000/2069]\tBatch Time 0.207 (0.210)\tData Time 0.000 (0.006)\tLoss 2.8962 (2.3017)\t\n",
      "[0/619]\tBatch Time 0.282 (0.282)\tLoss 2.0421 (2.0421)\t\n",
      "[200/619]\tBatch Time 0.085 (0.096)\tLoss 2.0757 (2.7094)\t\n",
      "[400/619]\tBatch Time 0.134 (0.096)\tLoss 2.8666 (2.7538)\t\n",
      "[600/619]\tBatch Time 0.088 (0.097)\tLoss 3.0574 (2.7074)\t\n",
      "\n",
      " * LOSS - 2.710\n",
      "\n",
      "\n",
      "Epochs since last improvement: 23\n",
      "\n",
      "Epoch: [183][0/2069]\tBatch Time 0.754 (0.754)\tData Time 0.525 (0.525)\tLoss 2.2073 (2.2073)\t\n",
      "Epoch: [183][200/2069]\tBatch Time 0.204 (0.215)\tData Time 0.000 (0.007)\tLoss 1.9219 (2.2907)\t\n",
      "Epoch: [183][400/2069]\tBatch Time 0.199 (0.212)\tData Time 0.000 (0.005)\tLoss 1.5081 (2.2741)\t\n",
      "Epoch: [183][600/2069]\tBatch Time 0.199 (0.212)\tData Time 0.000 (0.005)\tLoss 2.1031 (2.2778)\t\n",
      "Epoch: [183][800/2069]\tBatch Time 0.245 (0.212)\tData Time 0.000 (0.006)\tLoss 1.4505 (2.2705)\t\n",
      "Epoch: [183][1000/2069]\tBatch Time 0.199 (0.211)\tData Time 0.000 (0.005)\tLoss 2.4605 (2.2697)\t\n",
      "Epoch: [183][1200/2069]\tBatch Time 0.229 (0.212)\tData Time 0.024 (0.005)\tLoss 2.6945 (2.2837)\t\n",
      "Epoch: [183][1400/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.005)\tLoss 2.8216 (2.2810)\t\n",
      "Epoch: [183][1600/2069]\tBatch Time 0.183 (0.211)\tData Time 0.000 (0.005)\tLoss 1.9418 (2.2838)\t\n",
      "Epoch: [183][1800/2069]\tBatch Time 0.183 (0.210)\tData Time 0.000 (0.005)\tLoss 2.1603 (2.2860)\t\n",
      "Epoch: [183][2000/2069]\tBatch Time 0.210 (0.210)\tData Time 0.000 (0.005)\tLoss 2.1520 (2.2905)\t\n",
      "[0/619]\tBatch Time 0.310 (0.310)\tLoss 3.0060 (3.0060)\t\n",
      "[200/619]\tBatch Time 0.103 (0.098)\tLoss 3.2732 (2.8141)\t\n",
      "[400/619]\tBatch Time 0.072 (0.098)\tLoss 2.9271 (2.7807)\t\n",
      "[600/619]\tBatch Time 0.085 (0.098)\tLoss 2.8113 (2.7656)\t\n",
      "\n",
      " * LOSS - 2.764\n",
      "\n",
      "\n",
      "Epochs since last improvement: 24\n",
      "\n",
      "Epoch: [184][0/2069]\tBatch Time 0.727 (0.727)\tData Time 0.501 (0.501)\tLoss 1.6088 (1.6088)\t\n",
      "Epoch: [184][200/2069]\tBatch Time 0.242 (0.213)\tData Time 0.000 (0.008)\tLoss 2.3493 (2.3447)\t\n",
      "Epoch: [184][400/2069]\tBatch Time 0.225 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3741 (2.3274)\t\n",
      "Epoch: [184][600/2069]\tBatch Time 0.243 (0.210)\tData Time 0.002 (0.005)\tLoss 2.1705 (2.3263)\t\n",
      "Epoch: [184][800/2069]\tBatch Time 0.196 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5239 (2.3182)\t\n",
      "Epoch: [184][1000/2069]\tBatch Time 0.184 (0.210)\tData Time 0.000 (0.005)\tLoss 2.6804 (2.3071)\t\n",
      "Epoch: [184][1200/2069]\tBatch Time 0.192 (0.209)\tData Time 0.000 (0.005)\tLoss 2.6416 (2.3049)\t\n",
      "Epoch: [184][1400/2069]\tBatch Time 0.216 (0.210)\tData Time 0.000 (0.005)\tLoss 2.8219 (2.3055)\t\n",
      "Epoch: [184][1600/2069]\tBatch Time 0.179 (0.210)\tData Time 0.000 (0.005)\tLoss 2.0257 (2.3070)\t\n",
      "Epoch: [184][1800/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.005)\tLoss 1.8350 (2.3124)\t\n",
      "Epoch: [184][2000/2069]\tBatch Time 0.194 (0.210)\tData Time 0.009 (0.005)\tLoss 2.3059 (2.3135)\t\n",
      "[0/619]\tBatch Time 0.298 (0.298)\tLoss 2.6609 (2.6609)\t\n",
      "[200/619]\tBatch Time 0.087 (0.098)\tLoss 3.0287 (2.7259)\t\n",
      "[400/619]\tBatch Time 0.082 (0.099)\tLoss 2.2134 (2.7124)\t\n",
      "[600/619]\tBatch Time 0.097 (0.098)\tLoss 2.7276 (2.7126)\t\n",
      "\n",
      " * LOSS - 2.718\n",
      "\n",
      "\n",
      "Epochs since last improvement: 25\n",
      "\n",
      "Epoch: [185][0/2069]\tBatch Time 0.744 (0.744)\tData Time 0.521 (0.521)\tLoss 1.8973 (1.8973)\t\n",
      "Epoch: [185][200/2069]\tBatch Time 0.209 (0.213)\tData Time 0.000 (0.007)\tLoss 2.4886 (2.2686)\t\n",
      "Epoch: [185][400/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.007)\tLoss 2.4518 (2.2931)\t\n",
      "Epoch: [185][600/2069]\tBatch Time 0.180 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0373 (2.2902)\t\n",
      "Epoch: [185][800/2069]\tBatch Time 0.244 (0.209)\tData Time 0.000 (0.005)\tLoss 2.0927 (2.2896)\t\n",
      "Epoch: [185][1000/2069]\tBatch Time 0.181 (0.209)\tData Time 0.000 (0.006)\tLoss 2.6571 (2.2847)\t\n",
      "Epoch: [185][1200/2069]\tBatch Time 0.216 (0.209)\tData Time 0.000 (0.006)\tLoss 1.8008 (2.2924)\t\n",
      "Epoch: [185][1400/2069]\tBatch Time 0.184 (0.209)\tData Time 0.000 (0.006)\tLoss 3.0665 (2.2961)\t\n",
      "Epoch: [185][1600/2069]\tBatch Time 0.241 (0.209)\tData Time 0.001 (0.005)\tLoss 1.8457 (2.2970)\t\n",
      "Epoch: [185][1800/2069]\tBatch Time 0.215 (0.209)\tData Time 0.009 (0.006)\tLoss 2.0063 (2.3013)\t\n",
      "Epoch: [185][2000/2069]\tBatch Time 0.211 (0.209)\tData Time 0.000 (0.006)\tLoss 2.2298 (2.3010)\t\n",
      "[0/619]\tBatch Time 0.291 (0.291)\tLoss 3.4785 (3.4785)\t\n",
      "[200/619]\tBatch Time 0.083 (0.097)\tLoss 3.5800 (2.7449)\t\n",
      "[400/619]\tBatch Time 0.099 (0.097)\tLoss 2.4494 (2.7178)\t\n",
      "[600/619]\tBatch Time 0.095 (0.097)\tLoss 4.4357 (2.7101)\t\n",
      "\n",
      " * LOSS - 2.713\n",
      "\n",
      "\n",
      "Epochs since last improvement: 26\n",
      "\n",
      "Epoch: [186][0/2069]\tBatch Time 0.722 (0.722)\tData Time 0.515 (0.515)\tLoss 2.8640 (2.8640)\t\n",
      "Epoch: [186][200/2069]\tBatch Time 0.303 (0.215)\tData Time 0.108 (0.011)\tLoss 2.1462 (2.2755)\t\n",
      "Epoch: [186][400/2069]\tBatch Time 0.192 (0.214)\tData Time 0.000 (0.010)\tLoss 3.2344 (2.2634)\t\n",
      "Epoch: [186][600/2069]\tBatch Time 0.210 (0.213)\tData Time 0.000 (0.009)\tLoss 2.5642 (2.2811)\t\n",
      "Epoch: [186][800/2069]\tBatch Time 0.192 (0.213)\tData Time 0.000 (0.009)\tLoss 2.8010 (2.2821)\t\n",
      "Epoch: [186][1000/2069]\tBatch Time 0.236 (0.213)\tData Time 0.000 (0.009)\tLoss 2.3334 (2.2867)\t\n",
      "Epoch: [186][1200/2069]\tBatch Time 0.181 (0.212)\tData Time 0.000 (0.008)\tLoss 1.5355 (2.2913)\t\n",
      "Epoch: [186][1400/2069]\tBatch Time 0.231 (0.211)\tData Time 0.001 (0.007)\tLoss 2.4528 (2.2871)\t\n",
      "Epoch: [186][1600/2069]\tBatch Time 0.252 (0.211)\tData Time 0.000 (0.007)\tLoss 2.2550 (2.2947)\t\n",
      "Epoch: [186][1800/2069]\tBatch Time 0.189 (0.211)\tData Time 0.000 (0.007)\tLoss 2.0616 (2.2970)\t\n",
      "Epoch: [186][2000/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.006)\tLoss 2.0205 (2.2922)\t\n",
      "[0/619]\tBatch Time 0.272 (0.272)\tLoss 3.7632 (3.7632)\t\n",
      "[200/619]\tBatch Time 0.079 (0.096)\tLoss 3.3487 (2.7654)\t\n",
      "[400/619]\tBatch Time 0.099 (0.096)\tLoss 2.7734 (2.7504)\t\n",
      "[600/619]\tBatch Time 0.103 (0.096)\tLoss 3.3837 (2.7214)\t\n",
      "\n",
      " * LOSS - 2.719\n",
      "\n",
      "\n",
      "Epochs since last improvement: 27\n",
      "\n",
      "Epoch: [187][0/2069]\tBatch Time 0.622 (0.622)\tData Time 0.397 (0.397)\tLoss 2.1963 (2.1963)\t\n",
      "Epoch: [187][200/2069]\tBatch Time 0.222 (0.212)\tData Time 0.000 (0.008)\tLoss 2.0462 (2.2161)\t\n",
      "Epoch: [187][400/2069]\tBatch Time 0.198 (0.211)\tData Time 0.000 (0.006)\tLoss 2.8951 (2.2346)\t\n",
      "Epoch: [187][600/2069]\tBatch Time 0.185 (0.210)\tData Time 0.000 (0.005)\tLoss 2.2878 (2.2639)\t\n",
      "Epoch: [187][800/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.006)\tLoss 2.0246 (2.2718)\t\n",
      "Epoch: [187][1000/2069]\tBatch Time 0.190 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3514 (2.2660)\t\n",
      "Epoch: [187][1200/2069]\tBatch Time 0.332 (0.210)\tData Time 0.139 (0.006)\tLoss 2.7688 (2.2737)\t\n",
      "Epoch: [187][1400/2069]\tBatch Time 0.297 (0.211)\tData Time 0.089 (0.006)\tLoss 2.7067 (2.2817)\t\n",
      "Epoch: [187][1600/2069]\tBatch Time 0.381 (0.211)\tData Time 0.173 (0.006)\tLoss 2.6026 (2.2884)\t\n",
      "Epoch: [187][1800/2069]\tBatch Time 0.222 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6567 (2.2903)\t\n",
      "Epoch: [187][2000/2069]\tBatch Time 0.194 (0.210)\tData Time 0.000 (0.006)\tLoss 2.1561 (2.2919)\t\n",
      "[0/619]\tBatch Time 0.300 (0.300)\tLoss 2.4343 (2.4343)\t\n",
      "[200/619]\tBatch Time 0.103 (0.097)\tLoss 1.6293 (2.7345)\t\n",
      "[400/619]\tBatch Time 0.094 (0.097)\tLoss 3.0121 (2.7198)\t\n",
      "[600/619]\tBatch Time 0.098 (0.097)\tLoss 3.4260 (2.7077)\t\n",
      "\n",
      " * LOSS - 2.711\n",
      "\n",
      "\n",
      "Epochs since last improvement: 28\n",
      "\n",
      "Epoch: [188][0/2069]\tBatch Time 0.845 (0.845)\tData Time 0.630 (0.630)\tLoss 2.0449 (2.0449)\t\n",
      "Epoch: [188][200/2069]\tBatch Time 0.204 (0.213)\tData Time 0.000 (0.009)\tLoss 2.8776 (2.2685)\t\n",
      "Epoch: [188][400/2069]\tBatch Time 0.211 (0.210)\tData Time 0.000 (0.006)\tLoss 2.6534 (2.2575)\t\n",
      "Epoch: [188][600/2069]\tBatch Time 0.184 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3813 (2.2838)\t\n",
      "Epoch: [188][800/2069]\tBatch Time 0.199 (0.210)\tData Time 0.002 (0.005)\tLoss 1.8686 (2.2734)\t\n",
      "Epoch: [188][1000/2069]\tBatch Time 0.227 (0.210)\tData Time 0.009 (0.005)\tLoss 2.4158 (2.2790)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [188][1200/2069]\tBatch Time 0.183 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3668 (2.2826)\t\n",
      "Epoch: [188][1400/2069]\tBatch Time 0.183 (0.210)\tData Time 0.000 (0.005)\tLoss 1.6739 (2.2906)\t\n",
      "Epoch: [188][1600/2069]\tBatch Time 0.224 (0.210)\tData Time 0.000 (0.005)\tLoss 2.8817 (2.2924)\t\n",
      "Epoch: [188][1800/2069]\tBatch Time 0.203 (0.210)\tData Time 0.000 (0.005)\tLoss 2.3607 (2.2944)\t\n",
      "Epoch: [188][2000/2069]\tBatch Time 0.306 (0.210)\tData Time 0.000 (0.005)\tLoss 2.8997 (2.2979)\t\n",
      "[0/619]\tBatch Time 0.287 (0.287)\tLoss 3.3131 (3.3131)\t\n",
      "[200/619]\tBatch Time 0.106 (0.100)\tLoss 3.3435 (2.7496)\t\n",
      "[400/619]\tBatch Time 0.076 (0.098)\tLoss 2.7409 (2.7762)\t\n",
      "[600/619]\tBatch Time 0.104 (0.098)\tLoss 1.9985 (2.7617)\t\n",
      "\n",
      " * LOSS - 2.764\n",
      "\n",
      "\n",
      "Epochs since last improvement: 29\n",
      "\n",
      "Epoch: [189][0/2069]\tBatch Time 1.048 (1.048)\tData Time 0.819 (0.819)\tLoss 2.3210 (2.3210)\t\n",
      "Epoch: [189][200/2069]\tBatch Time 0.185 (0.217)\tData Time 0.000 (0.014)\tLoss 2.0818 (2.3125)\t\n",
      "Epoch: [189][400/2069]\tBatch Time 0.212 (0.212)\tData Time 0.008 (0.008)\tLoss 2.2776 (2.2817)\t\n",
      "Epoch: [189][600/2069]\tBatch Time 0.260 (0.211)\tData Time 0.003 (0.007)\tLoss 2.5103 (2.2723)\t\n",
      "Epoch: [189][800/2069]\tBatch Time 0.192 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2874 (2.2658)\t\n",
      "Epoch: [189][1000/2069]\tBatch Time 0.221 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3432 (2.2712)\t\n",
      "Epoch: [189][1200/2069]\tBatch Time 0.184 (0.211)\tData Time 0.003 (0.007)\tLoss 3.1939 (2.2721)\t\n",
      "Epoch: [189][1400/2069]\tBatch Time 0.183 (0.212)\tData Time 0.000 (0.007)\tLoss 1.9699 (2.2804)\t\n",
      "Epoch: [189][1600/2069]\tBatch Time 0.220 (0.211)\tData Time 0.008 (0.007)\tLoss 1.9773 (2.2837)\t\n",
      "Epoch: [189][1800/2069]\tBatch Time 0.213 (0.211)\tData Time 0.000 (0.007)\tLoss 1.3752 (2.2852)\t\n",
      "Epoch: [189][2000/2069]\tBatch Time 0.199 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1416 (2.2872)\t\n",
      "[0/619]\tBatch Time 0.312 (0.312)\tLoss 2.5685 (2.5685)\t\n",
      "[200/619]\tBatch Time 0.104 (0.097)\tLoss 3.4533 (2.7770)\t\n",
      "[400/619]\tBatch Time 0.100 (0.097)\tLoss 2.6084 (2.7703)\t\n",
      "[600/619]\tBatch Time 0.135 (0.097)\tLoss 2.1645 (2.7782)\t\n",
      "\n",
      " * LOSS - 2.770\n",
      "\n",
      "\n",
      "Epochs since last improvement: 30\n",
      "\n",
      "Epoch: [190][0/2069]\tBatch Time 0.666 (0.666)\tData Time 0.439 (0.439)\tLoss 2.5208 (2.5208)\t\n",
      "Epoch: [190][200/2069]\tBatch Time 0.235 (0.214)\tData Time 0.000 (0.011)\tLoss 1.7220 (2.3505)\t\n",
      "Epoch: [190][400/2069]\tBatch Time 0.320 (0.212)\tData Time 0.026 (0.008)\tLoss 2.6939 (2.3519)\t\n",
      "Epoch: [190][600/2069]\tBatch Time 0.189 (0.212)\tData Time 0.001 (0.007)\tLoss 1.6715 (2.3355)\t\n",
      "Epoch: [190][800/2069]\tBatch Time 0.259 (0.212)\tData Time 0.000 (0.008)\tLoss 2.1269 (2.3319)\t\n",
      "Epoch: [190][1000/2069]\tBatch Time 0.206 (0.212)\tData Time 0.001 (0.008)\tLoss 2.8603 (2.3212)\t\n",
      "Epoch: [190][1200/2069]\tBatch Time 0.203 (0.212)\tData Time 0.007 (0.007)\tLoss 1.9932 (2.3195)\t\n",
      "Epoch: [190][1400/2069]\tBatch Time 0.242 (0.212)\tData Time 0.037 (0.007)\tLoss 2.4958 (2.3095)\t\n",
      "Epoch: [190][1600/2069]\tBatch Time 0.207 (0.212)\tData Time 0.000 (0.007)\tLoss 2.4521 (2.3111)\t\n",
      "Epoch: [190][1800/2069]\tBatch Time 0.187 (0.212)\tData Time 0.000 (0.006)\tLoss 2.3201 (2.3081)\t\n",
      "Epoch: [190][2000/2069]\tBatch Time 0.205 (0.212)\tData Time 0.000 (0.007)\tLoss 1.9946 (2.3109)\t\n",
      "[0/619]\tBatch Time 0.353 (0.353)\tLoss 2.6048 (2.6048)\t\n",
      "[200/619]\tBatch Time 0.079 (0.094)\tLoss 2.5253 (2.7133)\t\n",
      "[400/619]\tBatch Time 0.087 (0.094)\tLoss 2.9291 (2.7266)\t\n",
      "[600/619]\tBatch Time 0.136 (0.095)\tLoss 2.9828 (2.7055)\t\n",
      "\n",
      " * LOSS - 2.707\n",
      "\n",
      "\n",
      "Epochs since last improvement: 31\n",
      "\n",
      "Epoch: [191][0/2069]\tBatch Time 0.744 (0.744)\tData Time 0.511 (0.511)\tLoss 2.3004 (2.3004)\t\n",
      "Epoch: [191][200/2069]\tBatch Time 0.199 (0.219)\tData Time 0.000 (0.015)\tLoss 1.9509 (2.2998)\t\n",
      "Epoch: [191][400/2069]\tBatch Time 0.249 (0.216)\tData Time 0.000 (0.012)\tLoss 1.6603 (2.2845)\t\n",
      "Epoch: [191][600/2069]\tBatch Time 0.215 (0.214)\tData Time 0.000 (0.009)\tLoss 2.1649 (2.2825)\t\n",
      "Epoch: [191][800/2069]\tBatch Time 0.221 (0.213)\tData Time 0.000 (0.010)\tLoss 1.6835 (2.2791)\t\n",
      "Epoch: [191][1000/2069]\tBatch Time 0.418 (0.212)\tData Time 0.231 (0.009)\tLoss 2.6047 (2.2838)\t\n",
      "Epoch: [191][1200/2069]\tBatch Time 0.253 (0.213)\tData Time 0.008 (0.009)\tLoss 2.4187 (2.2841)\t\n",
      "Epoch: [191][1400/2069]\tBatch Time 0.226 (0.212)\tData Time 0.001 (0.008)\tLoss 2.7598 (2.2932)\t\n",
      "Epoch: [191][1600/2069]\tBatch Time 0.235 (0.211)\tData Time 0.000 (0.007)\tLoss 1.6696 (2.2943)\t\n",
      "Epoch: [191][1800/2069]\tBatch Time 0.260 (0.211)\tData Time 0.024 (0.007)\tLoss 1.9731 (2.2974)\t\n",
      "Epoch: [191][2000/2069]\tBatch Time 0.197 (0.211)\tData Time 0.000 (0.007)\tLoss 2.5790 (2.2999)\t\n",
      "[0/619]\tBatch Time 0.272 (0.272)\tLoss 2.8947 (2.8947)\t\n",
      "[200/619]\tBatch Time 0.075 (0.096)\tLoss 2.9856 (2.7281)\t\n",
      "[400/619]\tBatch Time 0.114 (0.097)\tLoss 2.1714 (2.7442)\t\n",
      "[600/619]\tBatch Time 0.099 (0.097)\tLoss 4.5121 (2.7273)\t\n",
      "\n",
      " * LOSS - 2.727\n",
      "\n",
      "\n",
      "Epochs since last improvement: 32\n",
      "\n",
      "Epoch: [192][0/2069]\tBatch Time 0.876 (0.876)\tData Time 0.688 (0.688)\tLoss 2.8647 (2.8647)\t\n",
      "Epoch: [192][200/2069]\tBatch Time 0.220 (0.220)\tData Time 0.030 (0.014)\tLoss 2.4002 (2.2332)\t\n",
      "Epoch: [192][400/2069]\tBatch Time 0.221 (0.216)\tData Time 0.006 (0.010)\tLoss 2.2564 (2.2400)\t\n",
      "Epoch: [192][600/2069]\tBatch Time 0.190 (0.213)\tData Time 0.000 (0.009)\tLoss 2.8142 (2.2741)\t\n",
      "Epoch: [192][800/2069]\tBatch Time 0.216 (0.213)\tData Time 0.000 (0.008)\tLoss 1.6710 (2.2825)\t\n",
      "Epoch: [192][1000/2069]\tBatch Time 0.221 (0.213)\tData Time 0.008 (0.008)\tLoss 2.4945 (2.2905)\t\n",
      "Epoch: [192][1200/2069]\tBatch Time 0.195 (0.212)\tData Time 0.000 (0.007)\tLoss 2.2931 (2.2871)\t\n",
      "Epoch: [192][1400/2069]\tBatch Time 0.191 (0.211)\tData Time 0.000 (0.007)\tLoss 2.2221 (2.2929)\t\n",
      "Epoch: [192][1600/2069]\tBatch Time 0.196 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1402 (2.2915)\t\n",
      "Epoch: [192][1800/2069]\tBatch Time 0.235 (0.211)\tData Time 0.020 (0.006)\tLoss 1.6765 (2.2985)\t\n",
      "Epoch: [192][2000/2069]\tBatch Time 0.219 (0.211)\tData Time 0.000 (0.006)\tLoss 2.4903 (2.2972)\t\n",
      "[0/619]\tBatch Time 0.379 (0.379)\tLoss 2.7266 (2.7266)\t\n",
      "[200/619]\tBatch Time 0.088 (0.098)\tLoss 3.1592 (2.7250)\t\n",
      "[400/619]\tBatch Time 0.116 (0.098)\tLoss 2.7787 (2.7291)\t\n",
      "[600/619]\tBatch Time 0.103 (0.098)\tLoss 3.3873 (2.7442)\t\n",
      "\n",
      " * LOSS - 2.743\n",
      "\n",
      "\n",
      "Epochs since last improvement: 33\n",
      "\n",
      "Epoch: [193][0/2069]\tBatch Time 0.962 (0.962)\tData Time 0.756 (0.756)\tLoss 3.5835 (3.5835)\t\n",
      "Epoch: [193][200/2069]\tBatch Time 0.249 (0.217)\tData Time 0.024 (0.011)\tLoss 3.0865 (2.2785)\t\n",
      "Epoch: [193][400/2069]\tBatch Time 0.214 (0.211)\tData Time 0.011 (0.007)\tLoss 2.0323 (2.2556)\t\n",
      "Epoch: [193][600/2069]\tBatch Time 0.186 (0.211)\tData Time 0.000 (0.006)\tLoss 1.6111 (2.2624)\t\n",
      "Epoch: [193][800/2069]\tBatch Time 0.211 (0.211)\tData Time 0.000 (0.006)\tLoss 3.3575 (2.2688)\t\n",
      "Epoch: [193][1000/2069]\tBatch Time 0.206 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3189 (2.2771)\t\n",
      "Epoch: [193][1200/2069]\tBatch Time 0.190 (0.210)\tData Time 0.000 (0.006)\tLoss 2.3807 (2.2794)\t\n",
      "Epoch: [193][1400/2069]\tBatch Time 0.277 (0.210)\tData Time 0.037 (0.005)\tLoss 2.3212 (2.2851)\t\n",
      "Epoch: [193][1600/2069]\tBatch Time 0.216 (0.210)\tData Time 0.001 (0.006)\tLoss 1.8969 (2.2863)\t\n",
      "Epoch: [193][1800/2069]\tBatch Time 0.192 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0779 (2.2929)\t\n",
      "Epoch: [193][2000/2069]\tBatch Time 0.193 (0.210)\tData Time 0.001 (0.006)\tLoss 2.1478 (2.2893)\t\n",
      "[0/619]\tBatch Time 0.381 (0.381)\tLoss 2.6424 (2.6424)\t\n",
      "[200/619]\tBatch Time 0.107 (0.098)\tLoss 1.7499 (2.7199)\t\n",
      "[400/619]\tBatch Time 0.140 (0.097)\tLoss 2.2504 (2.7252)\t\n",
      "[600/619]\tBatch Time 0.104 (0.098)\tLoss 4.2403 (2.7358)\t\n",
      "\n",
      " * LOSS - 2.739\n",
      "\n",
      "\n",
      "Epochs since last improvement: 34\n",
      "\n",
      "Epoch: [194][0/2069]\tBatch Time 0.818 (0.818)\tData Time 0.574 (0.574)\tLoss 2.3102 (2.3102)\t\n",
      "Epoch: [194][200/2069]\tBatch Time 0.216 (0.215)\tData Time 0.000 (0.008)\tLoss 1.5601 (2.2452)\t\n",
      "Epoch: [194][400/2069]\tBatch Time 0.234 (0.210)\tData Time 0.008 (0.005)\tLoss 1.2083 (2.2517)\t\n",
      "Epoch: [194][600/2069]\tBatch Time 0.258 (0.211)\tData Time 0.024 (0.006)\tLoss 1.8040 (2.2565)\t\n",
      "Epoch: [194][800/2069]\tBatch Time 0.223 (0.211)\tData Time 0.000 (0.006)\tLoss 2.0022 (2.2683)\t\n",
      "Epoch: [194][1000/2069]\tBatch Time 0.251 (0.212)\tData Time 0.024 (0.007)\tLoss 1.5698 (2.2765)\t\n",
      "Epoch: [194][1200/2069]\tBatch Time 0.189 (0.211)\tData Time 0.000 (0.006)\tLoss 2.7677 (2.2824)\t\n",
      "Epoch: [194][1400/2069]\tBatch Time 0.221 (0.210)\tData Time 0.000 (0.006)\tLoss 2.2957 (2.2843)\t\n",
      "Epoch: [194][1600/2069]\tBatch Time 0.196 (0.210)\tData Time 0.000 (0.006)\tLoss 1.8379 (2.2872)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [194][1800/2069]\tBatch Time 0.220 (0.210)\tData Time 0.000 (0.006)\tLoss 2.0956 (2.2904)\t\n",
      "Epoch: [194][2000/2069]\tBatch Time 0.233 (0.210)\tData Time 0.001 (0.006)\tLoss 2.2022 (2.2887)\t\n",
      "[0/619]\tBatch Time 0.319 (0.319)\tLoss 3.8624 (3.8624)\t\n",
      "[200/619]\tBatch Time 0.098 (0.097)\tLoss 2.8614 (2.6678)\t\n",
      "[400/619]\tBatch Time 0.096 (0.096)\tLoss 2.4539 (2.7065)\t\n",
      "[600/619]\tBatch Time 0.094 (0.096)\tLoss 2.4358 (2.7347)\t\n",
      "\n",
      " * LOSS - 2.738\n",
      "\n",
      "\n",
      "Epochs since last improvement: 35\n",
      "\n",
      "Epoch: [195][0/2069]\tBatch Time 0.861 (0.861)\tData Time 0.669 (0.669)\tLoss 1.7311 (1.7311)\t\n",
      "Epoch: [195][200/2069]\tBatch Time 0.211 (0.218)\tData Time 0.000 (0.015)\tLoss 1.4271 (2.2377)\t\n",
      "Epoch: [195][400/2069]\tBatch Time 0.215 (0.217)\tData Time 0.000 (0.011)\tLoss 2.2130 (2.2368)\t\n",
      "Epoch: [195][600/2069]\tBatch Time 0.196 (0.214)\tData Time 0.001 (0.009)\tLoss 2.5902 (2.2399)\t\n",
      "Epoch: [195][800/2069]\tBatch Time 0.228 (0.213)\tData Time 0.000 (0.008)\tLoss 2.4851 (2.2538)\t\n",
      "Epoch: [195][1000/2069]\tBatch Time 0.216 (0.213)\tData Time 0.001 (0.008)\tLoss 2.0525 (2.2654)\t\n",
      "Epoch: [195][1200/2069]\tBatch Time 0.215 (0.213)\tData Time 0.000 (0.008)\tLoss 2.9282 (2.2664)\t\n",
      "Epoch: [195][1400/2069]\tBatch Time 0.247 (0.212)\tData Time 0.000 (0.007)\tLoss 2.0112 (2.2787)\t\n",
      "Epoch: [195][1600/2069]\tBatch Time 0.186 (0.212)\tData Time 0.000 (0.007)\tLoss 2.4149 (2.2834)\t\n",
      "Epoch: [195][1800/2069]\tBatch Time 0.187 (0.212)\tData Time 0.001 (0.007)\tLoss 1.6905 (2.2918)\t\n",
      "Epoch: [195][2000/2069]\tBatch Time 0.226 (0.212)\tData Time 0.000 (0.007)\tLoss 2.7076 (2.2875)\t\n",
      "[0/619]\tBatch Time 0.364 (0.364)\tLoss 3.2602 (3.2602)\t\n",
      "[200/619]\tBatch Time 0.109 (0.097)\tLoss 2.0070 (2.7183)\t\n",
      "[400/619]\tBatch Time 0.093 (0.098)\tLoss 2.6948 (2.7169)\t\n",
      "[600/619]\tBatch Time 0.077 (0.098)\tLoss 2.6804 (2.7481)\t\n",
      "\n",
      " * LOSS - 2.746\n",
      "\n",
      "\n",
      "Epochs since last improvement: 36\n",
      "\n",
      "Epoch: [196][0/2069]\tBatch Time 0.887 (0.887)\tData Time 0.673 (0.673)\tLoss 2.0088 (2.0088)\t\n",
      "Epoch: [196][200/2069]\tBatch Time 0.207 (0.215)\tData Time 0.000 (0.008)\tLoss 2.9030 (2.2300)\t\n",
      "Epoch: [196][400/2069]\tBatch Time 0.276 (0.213)\tData Time 0.024 (0.007)\tLoss 2.3801 (2.2545)\t\n",
      "Epoch: [196][600/2069]\tBatch Time 0.184 (0.211)\tData Time 0.000 (0.005)\tLoss 1.3967 (2.2568)\t\n",
      "Epoch: [196][800/2069]\tBatch Time 0.323 (0.211)\tData Time 0.112 (0.006)\tLoss 1.5958 (2.2618)\t\n",
      "Epoch: [196][1000/2069]\tBatch Time 0.237 (0.210)\tData Time 0.008 (0.005)\tLoss 2.0192 (2.2575)\t\n",
      "Epoch: [196][1200/2069]\tBatch Time 0.189 (0.211)\tData Time 0.000 (0.005)\tLoss 2.3241 (2.2531)\t\n",
      "Epoch: [196][1400/2069]\tBatch Time 0.270 (0.210)\tData Time 0.060 (0.005)\tLoss 2.5633 (2.2613)\t\n",
      "Epoch: [196][1600/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.005)\tLoss 1.7164 (2.2617)\t\n",
      "Epoch: [196][1800/2069]\tBatch Time 0.188 (0.210)\tData Time 0.000 (0.005)\tLoss 2.2879 (2.2658)\t\n",
      "Epoch: [196][2000/2069]\tBatch Time 0.184 (0.210)\tData Time 0.000 (0.005)\tLoss 2.5925 (2.2687)\t\n",
      "[0/619]\tBatch Time 0.285 (0.285)\tLoss 2.5867 (2.5867)\t\n",
      "[200/619]\tBatch Time 0.081 (0.097)\tLoss 3.2188 (2.6882)\t\n",
      "[400/619]\tBatch Time 0.078 (0.097)\tLoss 2.9761 (2.7172)\t\n",
      "[600/619]\tBatch Time 0.119 (0.097)\tLoss 2.4742 (2.7153)\t\n",
      "\n",
      " * LOSS - 2.716\n",
      "\n",
      "\n",
      "Epochs since last improvement: 37\n",
      "\n",
      "Epoch: [197][0/2069]\tBatch Time 0.800 (0.800)\tData Time 0.537 (0.537)\tLoss 1.9805 (1.9805)\t\n",
      "Epoch: [197][200/2069]\tBatch Time 0.241 (0.214)\tData Time 0.001 (0.008)\tLoss 2.6996 (2.2752)\t\n",
      "Epoch: [197][400/2069]\tBatch Time 0.219 (0.211)\tData Time 0.000 (0.006)\tLoss 2.1614 (2.2879)\t\n",
      "Epoch: [197][600/2069]\tBatch Time 0.192 (0.211)\tData Time 0.000 (0.006)\tLoss 2.3827 (2.2867)\t\n",
      "Epoch: [197][800/2069]\tBatch Time 0.179 (0.211)\tData Time 0.000 (0.007)\tLoss 1.9679 (2.2752)\t\n",
      "Epoch: [197][1000/2069]\tBatch Time 0.214 (0.211)\tData Time 0.001 (0.006)\tLoss 1.9709 (2.2760)\t\n",
      "Epoch: [197][1200/2069]\tBatch Time 0.190 (0.211)\tData Time 0.000 (0.006)\tLoss 2.8348 (2.2851)\t\n",
      "Epoch: [197][1400/2069]\tBatch Time 0.214 (0.210)\tData Time 0.005 (0.005)\tLoss 2.4310 (2.2916)\t\n",
      "Epoch: [197][1600/2069]\tBatch Time 0.198 (0.211)\tData Time 0.011 (0.005)\tLoss 2.0327 (2.2804)\t\n",
      "Epoch: [197][1800/2069]\tBatch Time 0.180 (0.210)\tData Time 0.000 (0.005)\tLoss 2.2790 (2.2847)\t\n",
      "Epoch: [197][2000/2069]\tBatch Time 0.216 (0.210)\tData Time 0.000 (0.005)\tLoss 2.0228 (2.2853)\t\n",
      "[0/619]\tBatch Time 0.334 (0.334)\tLoss 2.5486 (2.5486)\t\n",
      "[200/619]\tBatch Time 0.107 (0.097)\tLoss 2.2463 (2.7328)\t\n",
      "[400/619]\tBatch Time 0.098 (0.095)\tLoss 2.3176 (2.7227)\t\n",
      "[600/619]\tBatch Time 0.081 (0.096)\tLoss 2.5385 (2.7211)\t\n",
      "\n",
      " * LOSS - 2.714\n",
      "\n",
      "\n",
      "Epochs since last improvement: 38\n",
      "\n",
      "Epoch: [198][0/2069]\tBatch Time 0.712 (0.712)\tData Time 0.457 (0.457)\tLoss 2.0110 (2.0110)\t\n",
      "Epoch: [198][200/2069]\tBatch Time 0.229 (0.212)\tData Time 0.008 (0.007)\tLoss 2.0457 (2.2118)\t\n",
      "Epoch: [198][400/2069]\tBatch Time 0.254 (0.212)\tData Time 0.026 (0.007)\tLoss 2.3289 (2.2353)\t\n",
      "Epoch: [198][600/2069]\tBatch Time 0.191 (0.212)\tData Time 0.000 (0.007)\tLoss 2.3919 (2.2601)\t\n",
      "Epoch: [198][800/2069]\tBatch Time 0.192 (0.212)\tData Time 0.000 (0.008)\tLoss 2.0072 (2.2714)\t\n",
      "Epoch: [198][1000/2069]\tBatch Time 0.214 (0.213)\tData Time 0.003 (0.008)\tLoss 2.5919 (2.2690)\t\n",
      "Epoch: [198][1200/2069]\tBatch Time 0.213 (0.213)\tData Time 0.000 (0.008)\tLoss 1.9423 (2.2713)\t\n",
      "Epoch: [198][1400/2069]\tBatch Time 0.362 (0.213)\tData Time 0.162 (0.008)\tLoss 1.6190 (2.2813)\t\n",
      "Epoch: [198][1600/2069]\tBatch Time 0.184 (0.213)\tData Time 0.000 (0.008)\tLoss 2.5751 (2.2859)\t\n",
      "Epoch: [198][1800/2069]\tBatch Time 0.188 (0.212)\tData Time 0.008 (0.007)\tLoss 2.3245 (2.2921)\t\n",
      "Epoch: [198][2000/2069]\tBatch Time 0.199 (0.212)\tData Time 0.000 (0.007)\tLoss 2.1518 (2.2939)\t\n",
      "[0/619]\tBatch Time 0.359 (0.359)\tLoss 2.6135 (2.6135)\t\n",
      "[200/619]\tBatch Time 0.125 (0.098)\tLoss 2.9121 (2.8141)\t\n",
      "[400/619]\tBatch Time 0.084 (0.098)\tLoss 2.8657 (2.8196)\t\n",
      "[600/619]\tBatch Time 0.115 (0.097)\tLoss 4.1239 (2.8329)\t\n",
      "\n",
      " * LOSS - 2.832\n",
      "\n",
      "\n",
      "Epochs since last improvement: 39\n",
      "\n",
      "Epoch: [199][0/2069]\tBatch Time 0.857 (0.857)\tData Time 0.609 (0.609)\tLoss 2.4190 (2.4190)\t\n",
      "Epoch: [199][200/2069]\tBatch Time 0.247 (0.213)\tData Time 0.000 (0.008)\tLoss 1.1862 (2.2433)\t\n",
      "Epoch: [199][400/2069]\tBatch Time 0.194 (0.213)\tData Time 0.000 (0.008)\tLoss 2.2076 (2.2424)\t\n",
      "Epoch: [199][600/2069]\tBatch Time 0.186 (0.213)\tData Time 0.000 (0.007)\tLoss 2.0887 (2.2600)\t\n",
      "Epoch: [199][800/2069]\tBatch Time 0.191 (0.212)\tData Time 0.000 (0.008)\tLoss 2.6680 (2.2713)\t\n",
      "Epoch: [199][1000/2069]\tBatch Time 0.339 (0.212)\tData Time 0.126 (0.007)\tLoss 2.1424 (2.2831)\t\n",
      "Epoch: [199][1200/2069]\tBatch Time 0.197 (0.211)\tData Time 0.008 (0.007)\tLoss 2.3375 (2.2860)\t\n",
      "Epoch: [199][1400/2069]\tBatch Time 0.182 (0.211)\tData Time 0.000 (0.007)\tLoss 2.8525 (2.2833)\t\n",
      "Epoch: [199][1600/2069]\tBatch Time 0.245 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1752 (2.2888)\t\n",
      "Epoch: [199][1800/2069]\tBatch Time 0.225 (0.211)\tData Time 0.000 (0.007)\tLoss 2.1143 (2.2903)\t\n",
      "Epoch: [199][2000/2069]\tBatch Time 0.187 (0.210)\tData Time 0.000 (0.006)\tLoss 1.9437 (2.2939)\t\n",
      "[0/619]\tBatch Time 0.265 (0.265)\tLoss 2.6048 (2.6048)\t\n",
      "[200/619]\tBatch Time 0.074 (0.099)\tLoss 2.6420 (2.7801)\t\n",
      "[400/619]\tBatch Time 0.087 (0.098)\tLoss 3.2593 (2.7154)\t\n",
      "[600/619]\tBatch Time 0.075 (0.098)\tLoss 2.8525 (2.7286)\t\n",
      "\n",
      " * LOSS - 2.729\n",
      "\n",
      "\n",
      "Epochs since last improvement: 40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global epochs_since_improvement, start_epoch, label_map, best_loss, epoch, checkpoint\n",
    "\n",
    "# initialize model or load checkpoint\n",
    "if checkpoint is None:\n",
    "    model = SSD300(n_classes)\n",
    "    # Initialize the optimizer, with twice the default learning rate for biases\n",
    "    biases = list()\n",
    "    not_biases = list()\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param_name.endswith('.bias'):\n",
    "                biases.append(param)\n",
    "            else:\n",
    "                not_biases.append(param)\n",
    "    optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    \n",
    "# move to  default device\n",
    "model = model.to(device)      # model to GPU\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device) # Loss function to GPU\n",
    "\n",
    "# Custom dataloaders\n",
    "train_dataset = PascalVOCDataset(data_folder,'train',keep_difficult)\n",
    "val_dataset   = PascalVOCDataset(data_folder,'test' ,keep_difficult)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                            collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True) # pass in our collate function here\n",
    "val_loader    = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                             collate_fn=val_dataset.collate_fn, num_workers=workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss = validate(val_loader=val_loader,\n",
    "                        model=model,\n",
    "                        criterion=criterion)\n",
    "\n",
    "    # Did validation loss improve?\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(val_loss, best_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, epochs_since_improvement, model, optimizer, val_loss, best_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PrettyPrinter()\n",
    "\n",
    "# Parameters\n",
    "data_folder = './'\n",
    "keep_difficult = True\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = './BEST_checkpoint_ssd300.pth.tar'\n",
    "\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=workers,\n",
    "                                          pin_memory=True)\n",
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate\n",
    "\n",
    "    test_loader: test data loader\n",
    "    model:  trained model\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes  = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels= list()\n",
    "    true_scores= list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult' when we calculate 'mAP' score\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # for each batch\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating model')):\n",
    "            images = images.to(device) # (N, 3, 300, 300)\n",
    "\n",
    "            # Forward prop\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                        min_score=0.01, max_overlap=0.45,\n",
    "                                                                                        top_k=200)\n",
    "            \n",
    "            # Store this batch's results for mAP calculation\n",
    "            # True boxes & labels\n",
    "            boxes  = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.tp(deivce) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Cakcukate mAP\n",
    "        # Calculate mAP\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "        # Print AP for each class\n",
    "        pp.pprint(APs)\n",
    "\n",
    "        print('\\nMean Average Precision (mAP): %.3f' % mAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/78 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0a3b9d898ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b6736630bb57>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(test_loader, model)\u001b[0m\n\u001b[1;32m     57\u001b[0m             det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n\u001b[1;32m     58\u001b[0m                                                                                         \u001b[0mmin_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                                                                         top_k=200)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Store this batch's results for mAP calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-641e68843cd8>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m# Keep only predicted boxes and scores where scores for this class are above minimum_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mclass_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (8732)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mscore_above_min_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_score\u001b[0m \u001b[0;31m# torch.uint8 (byte) tensor, for infexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0mn_above_min_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_above_min_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_above_min_score\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_score' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
