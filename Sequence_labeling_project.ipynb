{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_labeling_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Single-Shot-Detection-project/blob/master/Sequence_labeling_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEyZ1KroE9iI",
        "colab_type": "text"
      },
      "source": [
        "We will be implementing the [Empower Sequence Labeling with Task-Aware Neural Language Model](https://arxiv.org/pdf/1709.04109.pdf) paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tmFs0zFHDQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "decive = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QENba6KHo59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway network\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        size: size of Linear layer (should match input size)\n",
        "        num_layers: number of transform and gate layers\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList() # A list of transform layers\n",
        "        self.gate = nn.ModuleList()      # A list of gate layers\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward-prop.\n",
        "        Returns a tensor with the same dimensions as input tensor\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = F.relu(self.transform[0](x))  # transform with the first transform layer\n",
        "        g = F.sigmoid(self.gate[0](x))              # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = self.dropout(g*transformed + (1-g)*x)               # combine input and transformed input with ratio of g\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(self.num_layers):\n",
        "            transformed = F.relu(self.transform[i](out))\n",
        "            g = F.sigmoid(self.gate[i](out))\n",
        "            out = self.dropout(g*transformed+(1-g)*out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaftUlVLOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Confitional Random Field\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        hidden_dim: the size of word/BLSTM's output (which is the input size for CRF)\n",
        "        tagset_size: number of tags(depending on our dataset)\n",
        "        \"\"\"\n",
        "\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_() # initializa the transition matrix to be all zeros\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        feats:   output of word/BLSTM, a tensor of dimensions-(batch_size, timesteps, hidden_dim)\n",
        "        returns: CRF scores, a tensor of dimensions-(batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps  = feats.size(1)\n",
        "\n",
        "        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        # here we broadcast emission_scores in order to compute the total score later with transition score\n",
        "        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size, self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK0OBqVT61P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.data.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6o7q1jQxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF\n",
        "    \"\"\"\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        tagset_size:   number of tags\n",
        "        charset_size:  size of character vocabulary\n",
        "        char_emb_dim:  size of character embeddings\n",
        "        char_rnn_dim:  size of charactor RNNS/LSTMs\n",
        "        char_rnn_layers: number of layers in character RNN/LSTMs\n",
        "        vocab_size:    input vocabulary size\n",
        "        lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        word_emb_dim:  size of word embeddings\n",
        "        word_rnn_dim:  size of word RNN/BLSTM\n",
        "        word_rnn_layers: number of layers in word RNNs/LSTMs\n",
        "        dropout:       dropout\n",
        "        highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size  = tagset_size # this is the size of the outout vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size  = vocab_size     # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim  = word_emb_dim\n",
        "        self.word_rnn_dim  = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "        \n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # charactor embedding layer\n",
        "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim) \n",
        "\n",
        "        # forward char LSTM\n",
        "        self.forw_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim, \n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        # backward char LSTM\n",
        "        self.back_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim,\n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.word_embeds = nn.Embedding(num_embeddings=self.wordset_size,embedding_dim=self.word_emb_dim)\n",
        "        # Define word-level bidirection LSTM\n",
        "        # Take note on the hidden_size\n",
        "        self.word_blstm   = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, # input_size\n",
        "                                    self.word_rnn_dim // 2,                    # hidden_size\n",
        "                                    # This is because Bi-directional LSTM will concat forward and backward output\n",
        "                                    # therefore we specify word_rnn_dim//2 but will get output size of word_rnn_dim\n",
        "                                    num_layers=self.word_rnn_layers,\n",
        "                                    bidirectional=True,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "        \n",
        "        # Conditinoal Random Field layer\n",
        "        self.crf = CRF(hidden_dim=self.word_rnn_dim,tagset_size=self.tagset_size)\n",
        "\n",
        "        # 3 places that we implemented highway connections\n",
        "        self.forw_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.back_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, \n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        \n",
        "        # Linear layers for language models, They are used for \"muti-task training\" for language models (predicting next word)\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "\n",
        "    def init_word_embedding(self, embedding):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        embedding: pre-trained embeddings to be loaded\n",
        "        \"\"\"\n",
        "        self.word_embeds.weights = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (if using pre-trained embedding layer, consider no fine-tuning)\n",
        "\n",
        "        fine_tune: bool decides if fine_tune\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        cmaps_f: padded encoded forward  character sequences. (batch_size, char_pad_len)\n",
        "        cmaps_b: padded encoded backward character sequences. (batch_size, char_pad_len)\n",
        "        cmarkers_f: padded forward character markers.          (batch_size, word_pad_len)\n",
        "        cmarkers_b: padded backward character markers.         (batch_size, word_pad_len)\n",
        "        wmaps: padded encoded word sequences.                 (batch_size, word_pad_len)\n",
        "        tmaps: padded tag sequences.                          (batch_size, word_pad_len)\n",
        "        wmap_lengths: word sequence lengths.                  (batch_size)\n",
        "        cmap_lengths: character sequence lengths              (batch_size)\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size   = cmaps_f.size(0)\n",
        "        self.word_pad_len = wmaps.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length for grouping up for padding later\n",
        "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
        "        cmaps_f = cmaps_f[char_sort_ind]\n",
        "        cmaps_b = cmaps_b[char_sort_ind]\n",
        "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
        "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
        "        wmaps = wmaps[char_sort_ind]\n",
        "        tmaps = tmaps[char_sort_ind]\n",
        "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
        "\n",
        "        # Embedding look-up for characters, turning each character to its embedding of char_emb_dim size\n",
        "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.char_embeds(cmaps_b)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.dropout(cb)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        cf = pack_padded_sequence(cf, lengths=cmap_lengths.tolist(), batch_first=True) # packed sequence of char_emb_dim, with real sequence lengths\n",
        "        cb = pack_padded_sequence(cb, lengths=cmap_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # LSTM for forward and backword language model & feature extraction for Bi-directional LSTM\n",
        "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
        "        cb, _ = self.back_char_lstm(cb)   \n",
        "\n",
        "        # Unpack packed sequence\n",
        "        cf, _ = pad_packed_sequence(cf, batch_first=True) # (batch, max_char_len_in_batch, char_rnn_dim)\n",
        "        cb, _ = pad_packed_sequence(cb, batch_first=True) \n",
        "\n",
        "        # Sanity check\n",
        "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
        "\n",
        "        # Select RNN outpus only at marker points (spaces in the character sequence)\n",
        "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        # torch.gather return output same dim as index, with value taken from cf. In this case we can see that dim=1 of output might be different from input(cf)\n",
        "        cf_selected = torch.gather(cf, 1, index=cmarkers_f) # (batch_size, word_pad_len, char_rnn_dim)\n",
        "        cb_selected = torch.gather(cb, 1, index=cmarkers_b)\n",
        "\n",
        "        # Only for co-training language model(to boost performance), not useful for tagging after model is trained\n",
        "        if self.training:   # this toggle is true when we set model.train(), false if we set model.eval()\n",
        "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
        "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
        "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
        "\n",
        "        # Sort by decreasing true word sequence length\n",
        "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
        "        wmaps = wmaps[word_sort_ind]\n",
        "        tmaps = tmaps[word_sort_ind]\n",
        "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
        "        cb_selected = cb_selected[word_sort_ind]\n",
        "        if self.training:\n",
        "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
        "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
        "\n",
        "        # Embedding look-up for words\n",
        "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Sub-word information at each word\n",
        "        subword = self.subword_hw(self.dropout(torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
        "        subword = self.dropout(subword)\n",
        "\n",
        "        # Concatenate word embeddings and sub-word features\n",
        "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, 2*char_rnn_dim+word_emb_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        w = pack_padded_sequence(w, list(wmap_lengths), batch_first=True)   # packed sequence of word_emb_dim+2*char_rnn_dim\n",
        "\n",
        "        # Bi-directional LSTM\n",
        "        w, _ = self.word_blstm(w)   # packed sequence of word_rnn_dim, with real sequence lengths\n",
        "        \n",
        "        # Unpack packed sequence\n",
        "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        crf_scores = self.crf(w)     # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
        "\n",
        "        if self.training:\n",
        "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
        "        else:\n",
        "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRFQhNj_Bfr",
        "colab_type": "code",
        "outputId": "9ec83692-3da7-48a7-cca9-569466a0b3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixkzwHv_ye6",
        "colab_type": "code",
        "outputId": "cda6f92b-f12d-4ebc-c928-cb5fe1c21a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2018\n",
            " 2019\n",
            "'Attention in deep learning.md'\n",
            " BEST_checkpoint_lm_lstm_crf.pth.tar\n",
            " checkpoint_lm_lstm_crf.pth.tar\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Enoava\n",
            " FastAI-NLP.ipynb\n",
            "'LSTM in PyTorch.md'\n",
            "'LSTM in PyTorch.pdf'\n",
            "'Response to comments_MECHMT_2018_1107_-V4_17_Dec.docx'\n",
            " Snaps\n",
            " Stack-presentation-Dermotologist.png\n",
            "'Starwars Project Video V2.mp4'\n",
            " YOLO.md\n",
            " YOLO.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwrkKoX_zxO",
        "colab_type": "code",
        "outputId": "d4595d3c-98c6-458e-fd48-a623fde65eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/data/embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm_y4aE__pm",
        "colab_type": "code",
        "outputId": "8e7b1ce3-8813-4fed-91ee-de4f20f761c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATeBjYOFA5pg",
        "colab_type": "code",
        "outputId": "0dfa8142-bad8-4a13-80d4-1ccebc97cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmg97Kl9Z-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# Rewrite the __getitem__ and add __len__\n",
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHc2_4Nj-iTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGatVIt56gXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "from utils import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_mY6W265rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'ner'        # tagging task, choose between [ner, pos]\n",
        "train_file = '/gdrive/My Drive/data/CoNLL-2003/eng.train'\n",
        "val_file   = '/gdrive/My Drive/data/CoNLL-2003/eng.testa'\n",
        "test_file  = '/gdrive/My Drive/data/CoNLL-2003/eng.testb'\n",
        "emb_file   = '/gdrive/My Drive/data/embeddings/glove.6B.100d.txt'\n",
        "min_word_freq = 5 # threshold for word frequency to be recognized not as xxunk\n",
        "min_char_freq = 1 # same thing for char frequency\n",
        "caseless   = True # lowercase everything?\n",
        "expand_vocab = True # expand model's input vocabulary to the pre-trained embedding vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30 # character embedding size\n",
        "with open(emb_file, 'r') as f:\n",
        "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embdding size, \"-1\" is because in the txt file the first place is the word itself, followed by the actual embeddings\n",
        "word_rnn_dim = 300  # word BLSTM hidden size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1 # number of layers in character RNN\n",
        "word_rnn_layers = 1 # number of layers in word BLSTM\n",
        "highway_layers  = 1 # number of layers in highway network\n",
        "dropout = 0.55       # universal dropout rate\n",
        "fine_tune_word_embeddings = False\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0   # start at this epoch\n",
        "batch_size  = 10  # batch size\n",
        "lr = 0.015  \n",
        "lr_decay = 0.05\n",
        "momentum = 0.9\n",
        "workers  = 4\n",
        "epochs   = 200    # number of epochs without triggering early stoping\n",
        "grad_clip = 5.\n",
        "print_freq = 300  # print every ___ batches\n",
        "best_f1  = 0.\n",
        "checkpoint = 'BEST_checkpoint_lm_lstm_crf.pth.tar' # Model checkpoint to load. None if training from scratch\n",
        "\n",
        "tag_ind = 1 if task == 'pos' else 3 # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLn_7JoDGUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "# Read training and validation data\n",
        "train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    word_map  = checkpoint['word_map']\n",
        "    lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "    tag_map   = checkpoint['tag_map']\n",
        "    char_map  = checkpoint['char_map']\n",
        "    start_epoch = checkpoint['epoch'] +1\n",
        "    best_f1   = checkpoint['f1']\n",
        "else:\n",
        "    # create word, char, tag maps\n",
        "    # maps are essentially dictionaries that map a token to an integer\n",
        "    word_map, char_map, tag_map = create_maps(train_words+val_words,train_tags+val_tags, min_word_freq, min_char_freq)\n",
        "\n",
        "    # load pre-trained embeddings, if expand_vocab==True, word_map expand to embedding_word_map\n",
        "    # lm_vocab_size is the word_map size before expand to \"out-of-corpus vocab\"\n",
        "    embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map, expand_vocab)\n",
        "\n",
        "    model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                        charset_size=len(char_map),\n",
        "                        char_emb_dim=char_emb_dim,\n",
        "                        char_rnn_dim=char_rnn_dim,\n",
        "                        char_rnn_layers=char_rnn_layers,\n",
        "                        vocab_size=len(word_map),       # This is the length after expand\n",
        "                        lm_vocab_size=lm_vocab_size,    # len(word_map) before expand, not influenced by the embedding vocab\n",
        "                        word_emb_dim=word_emb_dim,\n",
        "                        word_rnn_dim=word_rnn_dim,\n",
        "                        word_rnn_layers=word_rnn_layers,\n",
        "                        dropout=dropout,\n",
        "                        highway_layers=highway_layers).to(device)\n",
        "    model.init_word_embedding(embeddings.to(device)) # initializa embedding layers with pre-trained embeddings.(Essentially we just make it nn.Parameter)\n",
        "    model.fine_tune_word_embeddings(fine_tune_word_embeddings)    # decide if these nn.Parameters has requires_grad = True (trainable)\n",
        "    optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re6HfRf1tGAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos): return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
        "\n",
        "# This monkey-path is here to enable plotting tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0BzLrdtmkC",
        "colab_type": "code",
        "outputId": "728d78e6-d06e-41e0-8590-eee86ce9db82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from torch import tensor\n",
        "def combine_scheds(pcts, scheds):\n",
        "    assert sum(pcts) == 1.\n",
        "    pcts = tensor([0] + list(pcts))\n",
        "    assert torch.all(pcts >= 0)\n",
        "    pcts = torch.cumsum(pcts, 0)\n",
        "    def _inner(pos):\n",
        "        idx = (pos >= pcts).nonzero().max()\n",
        "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
        "        return scheds[idx](actual_pos)\n",
        "    return _inner\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 1.1), sched_cos(1.1, 0.1)])\n",
        "a = torch.arange(0, 100)\n",
        "p = torch.linspace(0.01,1,100)\n",
        "plt.plot(a, [sched(o) for o in p])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f018032c828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ8UVncplAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7eiHeWJ7zSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining how one training step will be computed.\n",
        "# train() is used in each epoch later\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-5, 1e-3), sched_cos(1e-3, 1e-6)])\n",
        "learning_rates = list()\n",
        "\n",
        "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder, total_iters):\n",
        "    \"\"\"\n",
        "    train_loader: DataLoader for training data\n",
        "    model: LM_LSTM_CRF\n",
        "    lm_criterion:  nn.CrossEntropyLoss()\n",
        "    crf_criterion: ViterbiLoss()\n",
        "    optimizer: SGD/adam/adbound whatever your choice is\n",
        "    epoch: epoch number\n",
        "    vb_decoder: viterbi decoder(to decode and find F1 score)\n",
        "    \"\"\"\n",
        "    model.train()  # training mode, so dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
        "    data_time = AverageMeter()  # data loading time per batch\n",
        "    ce_losses = AverageMeter()  # cross entropy loss\n",
        "    vb_losses = AverageMeter()  # viterbi loss\n",
        "    f1s = AverageMeter()  # f1 score\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(train_loader):\n",
        "        data_time.update(time.time()-start)\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence. \n",
        "        # This saves some compute on nn.Linear layers (RNNs are not affected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                                             cmaps_b,\n",
        "                                                                                                             cmarkers_f,\n",
        "                                                                                                             cmarkers_b,\n",
        "                                                                                                             wmaps,\n",
        "                                                                                                             tmaps,\n",
        "                                                                                                             wmap_lengths,\n",
        "                                                                                                             cmap_lengths)\n",
        "        \n",
        "        # LM loss\n",
        "\n",
        "        # We don't predict the next word at the pads or <end> tokens\n",
        "        # Hence, only predict [word1, word2, word3, word4] among [word1, word2, word3, word4,<pad>,<pad>,<pad>,<pad>,<pad>,<end>]\n",
        "        # So prediction lengths are word sequence lengths -1\n",
        "        lm_lengths = wmap_lengths_sorted - 1 # (batch_size) the effective length of each row\n",
        "        lm_lengths = lm_lengths.tolist()\n",
        "\n",
        "        # Remove scores at timesteps we won't predict at\n",
        "        # pack_padded_sequence is a good trick to do this (145 PyTorch tricks(my other repo)---Trick #11)\n",
        "        lm_f_scores = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
        "        lm_b_scores = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
        "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
        "        lm_f_targets = wmaps_sorted[:, 1:]\n",
        "        lm_f_targets = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
        "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
        "        lm_b_targets = torch.cat([torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), \n",
        "                                  wmaps_sorted], dim=1)\n",
        "        lm_b_targets = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
        "        \n",
        "        # Calculate loss\n",
        "        ce_loss = lm_criterion(lm_f_scores.data, lm_f_targets.data) + lm_criterion(lm_b_scores.data, lm_b_targets.data)\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "        loss = ce_loss + vb_loss\n",
        "\n",
        "        # Learning rate annealing\n",
        "        current_iter = epoch*n_batches + i\n",
        "        new_lr = sched(current_iter/total_iters)\n",
        "        optimizer.param_groups[0]['lr'] = new_lr\n",
        "        learning_rates.append(new_lr)\n",
        "        # print(f'the learning rate in Epoch {epoch}, step {i} is {new_lr}')\n",
        "\n",
        "        # Back-prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Viterbi decode to find accuracy/F1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
        "\n",
        "        # Compute F1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "        f1s.update(f1, sum(lm_lengths))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print training status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                          batch_time=batch_time,\n",
        "                                                          data_time=data_time, ce_loss=ce_losses,\n",
        "                                                          vb_loss=vb_losses, f1=f1s))\n",
        "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
        "    \"\"\"\n",
        "    val_loader:    Dataloader for validation data\n",
        "    model:         Model\n",
        "    crf_criterion: Viterbi loss layer\n",
        "    return:        validation F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    vb_losses  = AverageMeter()\n",
        "    f1s        = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # validation loops\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(val_loader):\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                    cmaps_b,\n",
        "                                                                                    cmarkers_f,\n",
        "                                                                                    cmarkers_b,\n",
        "                                                                                    wmaps,\n",
        "                                                                                    tmaps,\n",
        "                                                                                    wmap_lengths,\n",
        "                                                                                    cmap_lengths)\n",
        "\n",
        "        # Viterbi / CRF layer loss\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "\n",
        "        # f1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                  vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
        "                                                                          f1=f1s))\n",
        "\n",
        "    return f1s.avg    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUtUGyqDAuu",
        "colab_type": "code",
        "outputId": "31776812-1336-46ae-d8bb-8d48ebbbc745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loss funcitons\n",
        "lm_criterion  = nn.CrossEntropyLoss().to(device)\n",
        "crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "# Since the language model's vocab is restricted on in-corpus indices, encode training/val with only these!\n",
        "# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "# they exist in the pre-trained embeddings.(these embeddings are added after the <unk> token)\n",
        "temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "\n",
        "# train_input = (padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, \n",
        "#                padded_tmaps, wmap_lengths,   cmap_lengths)\n",
        "train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)\n",
        "\n",
        "# Viterbi decoder (to find accuracy during validation)\n",
        "vb_decoder = ViterbiDecoder(tag_map)\n",
        "\n",
        "# Calculate total number of iterations for learning rate annealing\n",
        "n_batches   = len(train_loader)\n",
        "total_iters = epochs*n_batches\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader  = train_loader,\n",
        "          model         = model,\n",
        "          lm_criterion  = lm_criterion,\n",
        "          crf_criterion = crf_criterion,\n",
        "          optimizer     = optimizer,\n",
        "          epoch         = epoch,\n",
        "          vb_decoder    = vb_decoder,\n",
        "          total_iters   = total_iters\n",
        "          )\n",
        "    # One epoch's validation\n",
        "    val_f1 = validate(val_loader = val_loader,\n",
        "                      model      = model,\n",
        "                      crf_criterion = crf_criterion,\n",
        "                      vb_decoder    = vb_decoder)\n",
        "    is_best = val_f1 > best_f1\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
        "\n",
        "    # Decay learning rate every epoch\n",
        "    # adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [149][0/1405]\tBatch Time 0.385 (0.385)\tData Load Time 0.225 (0.225)\tCE Loss 10.2783 (10.2783)\tVB Loss 1.2254 (1.2254)\tF1 0.837 (0.837)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [149][300/1405]\tBatch Time 0.148 (0.123)\tData Load Time 0.004 (0.005)\tCE Loss 10.7173 (10.6801)\tVB Loss 2.4122 (1.3201)\tF1 0.830 (0.837)\n",
            "Epoch: [149][600/1405]\tBatch Time 0.113 (0.125)\tData Load Time 0.006 (0.005)\tCE Loss 10.6629 (10.6855)\tVB Loss 1.7885 (1.4000)\tF1 0.696 (0.832)\n",
            "Epoch: [149][900/1405]\tBatch Time 0.135 (0.125)\tData Load Time 0.005 (0.005)\tCE Loss 10.6079 (10.6842)\tVB Loss 1.2572 (1.4038)\tF1 0.824 (0.831)\n",
            "Epoch: [149][1200/1405]\tBatch Time 0.103 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.2561 (10.7011)\tVB Loss 0.5178 (1.4107)\tF1 0.971 (0.829)\n",
            "Validation: [0/325]\tBatch Time 0.279 (0.279)\tVB Loss 1.9834 (1.9834)\tF1 Score 0.731 (0.731)\t\n",
            "Validation: [100/325]\tBatch Time 0.079 (0.075)\tVB Loss 2.3356 (1.4371)\tF1 Score 0.946 (0.866)\t\n",
            "Validation: [200/325]\tBatch Time 0.052 (0.073)\tVB Loss 1.0747 (1.5438)\tF1 Score 0.816 (0.863)\t\n",
            "Validation: [300/325]\tBatch Time 0.062 (0.075)\tVB Loss 1.0114 (1.5541)\tF1 Score 0.916 (0.855)\t\n",
            "\n",
            " * LOSS - 1.546, F1 SCORE - 0.857\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LM_LSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Highway. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [150][0/1405]\tBatch Time 0.441 (0.441)\tData Load Time 0.248 (0.248)\tCE Loss 10.8401 (10.8401)\tVB Loss 2.9209 (2.9209)\tF1 0.748 (0.748)\n",
            "Epoch: [150][300/1405]\tBatch Time 0.142 (0.130)\tData Load Time 0.005 (0.005)\tCE Loss 10.4147 (10.7156)\tVB Loss 1.6731 (1.3902)\tF1 0.855 (0.835)\n",
            "Epoch: [150][600/1405]\tBatch Time 0.110 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.3935 (10.7308)\tVB Loss 0.5870 (1.3703)\tF1 0.957 (0.831)\n",
            "Epoch: [150][900/1405]\tBatch Time 0.131 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.9312 (10.7126)\tVB Loss 2.4288 (1.3853)\tF1 0.845 (0.828)\n",
            "Epoch: [150][1200/1405]\tBatch Time 0.114 (0.125)\tData Load Time 0.005 (0.004)\tCE Loss 10.4055 (10.7045)\tVB Loss 2.1579 (1.3855)\tF1 0.779 (0.831)\n",
            "Validation: [0/325]\tBatch Time 0.310 (0.310)\tVB Loss 1.7891 (1.7891)\tF1 Score 0.840 (0.840)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.078)\tVB Loss 1.0920 (1.5627)\tF1 Score 0.789 (0.847)\t\n",
            "Validation: [200/325]\tBatch Time 0.047 (0.077)\tVB Loss 0.8718 (1.5835)\tF1 Score 0.919 (0.850)\t\n",
            "Validation: [300/325]\tBatch Time 0.069 (0.076)\tVB Loss 1.4216 (1.5705)\tF1 Score 0.932 (0.853)\t\n",
            "\n",
            " * LOSS - 1.550, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [151][0/1405]\tBatch Time 0.371 (0.371)\tData Load Time 0.250 (0.250)\tCE Loss 9.6210 (9.6210)\tVB Loss 0.7016 (0.7016)\tF1 0.890 (0.890)\n",
            "Epoch: [151][300/1405]\tBatch Time 0.144 (0.131)\tData Load Time 0.004 (0.005)\tCE Loss 11.2078 (10.6910)\tVB Loss 3.8200 (1.4225)\tF1 0.590 (0.823)\n",
            "Epoch: [151][600/1405]\tBatch Time 0.100 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.8096 (10.6855)\tVB Loss 0.5372 (1.4227)\tF1 0.778 (0.825)\n",
            "Epoch: [151][900/1405]\tBatch Time 0.171 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.5934 (10.6967)\tVB Loss 1.3124 (1.4292)\tF1 0.901 (0.825)\n",
            "Epoch: [151][1200/1405]\tBatch Time 0.147 (0.126)\tData Load Time 0.004 (0.004)\tCE Loss 11.3540 (10.7040)\tVB Loss 1.6341 (1.4239)\tF1 0.935 (0.828)\n",
            "Validation: [0/325]\tBatch Time 0.328 (0.328)\tVB Loss 1.5217 (1.5217)\tF1 Score 0.670 (0.670)\t\n",
            "Validation: [100/325]\tBatch Time 0.076 (0.076)\tVB Loss 3.2876 (1.5642)\tF1 Score 0.800 (0.851)\t\n",
            "Validation: [200/325]\tBatch Time 0.131 (0.075)\tVB Loss 0.1099 (1.5663)\tF1 Score 1.000 (0.859)\t\n",
            "Validation: [300/325]\tBatch Time 0.079 (0.074)\tVB Loss 1.4286 (1.5599)\tF1 Score 0.847 (0.857)\t\n",
            "\n",
            " * LOSS - 1.557, F1 SCORE - 0.857\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n",
            "Epoch: [152][0/1405]\tBatch Time 0.432 (0.432)\tData Load Time 0.250 (0.250)\tCE Loss 10.6655 (10.6655)\tVB Loss 1.9269 (1.9269)\tF1 0.714 (0.714)\n",
            "Epoch: [152][300/1405]\tBatch Time 0.139 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.2292 (10.6958)\tVB Loss 2.0144 (1.4170)\tF1 0.881 (0.832)\n",
            "Epoch: [152][600/1405]\tBatch Time 0.144 (0.124)\tData Load Time 0.005 (0.005)\tCE Loss 10.4641 (10.6948)\tVB Loss 0.4754 (1.3852)\tF1 0.987 (0.830)\n",
            "Epoch: [152][900/1405]\tBatch Time 0.158 (0.124)\tData Load Time 0.005 (0.005)\tCE Loss 10.9742 (10.6866)\tVB Loss 0.8038 (1.3675)\tF1 0.565 (0.832)\n",
            "Epoch: [152][1200/1405]\tBatch Time 0.133 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.9797 (10.6814)\tVB Loss 3.4999 (1.3896)\tF1 0.721 (0.829)\n",
            "Validation: [0/325]\tBatch Time 0.303 (0.303)\tVB Loss 1.2717 (1.2717)\tF1 Score 0.885 (0.885)\t\n",
            "Validation: [100/325]\tBatch Time 0.052 (0.078)\tVB Loss 0.3165 (1.6012)\tF1 Score 0.912 (0.854)\t\n",
            "Validation: [200/325]\tBatch Time 0.050 (0.075)\tVB Loss 1.1175 (1.5287)\tF1 Score 0.881 (0.858)\t\n",
            "Validation: [300/325]\tBatch Time 0.085 (0.074)\tVB Loss 0.7930 (1.5864)\tF1 Score 0.909 (0.859)\t\n",
            "\n",
            " * LOSS - 1.569, F1 SCORE - 0.858\n",
            "\n",
            "Epoch: [153][0/1405]\tBatch Time 0.389 (0.389)\tData Load Time 0.269 (0.269)\tCE Loss 11.1081 (11.1081)\tVB Loss 1.3201 (1.3201)\tF1 0.446 (0.446)\n",
            "Epoch: [153][300/1405]\tBatch Time 0.152 (0.131)\tData Load Time 0.004 (0.006)\tCE Loss 10.2774 (10.6693)\tVB Loss 1.4356 (1.3626)\tF1 0.824 (0.830)\n",
            "Epoch: [153][600/1405]\tBatch Time 0.153 (0.129)\tData Load Time 0.006 (0.005)\tCE Loss 10.7063 (10.6833)\tVB Loss 2.2167 (1.3755)\tF1 0.869 (0.832)\n",
            "Epoch: [153][900/1405]\tBatch Time 0.119 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.9529 (10.6843)\tVB Loss 2.1811 (1.3900)\tF1 0.933 (0.830)\n",
            "Epoch: [153][1200/1405]\tBatch Time 0.128 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.8100 (10.7041)\tVB Loss 1.5706 (1.3808)\tF1 0.804 (0.834)\n",
            "Validation: [0/325]\tBatch Time 0.315 (0.315)\tVB Loss 0.4364 (0.4364)\tF1 Score 0.950 (0.950)\t\n",
            "Validation: [100/325]\tBatch Time 0.074 (0.075)\tVB Loss 0.5253 (1.5040)\tF1 Score 0.977 (0.862)\t\n",
            "Validation: [200/325]\tBatch Time 0.077 (0.075)\tVB Loss 0.6736 (1.4754)\tF1 Score 0.879 (0.861)\t\n",
            "Validation: [300/325]\tBatch Time 0.052 (0.074)\tVB Loss 0.9363 (1.4953)\tF1 Score 0.690 (0.858)\t\n",
            "\n",
            " * LOSS - 1.547, F1 SCORE - 0.857\n",
            "\n",
            "Epoch: [154][0/1405]\tBatch Time 0.386 (0.386)\tData Load Time 0.232 (0.232)\tCE Loss 11.7281 (11.7281)\tVB Loss 0.8982 (0.8982)\tF1 0.798 (0.798)\n",
            "Epoch: [154][300/1405]\tBatch Time 0.129 (0.132)\tData Load Time 0.004 (0.006)\tCE Loss 8.8882 (10.6903)\tVB Loss 1.4187 (1.3539)\tF1 0.738 (0.830)\n",
            "Epoch: [154][600/1405]\tBatch Time 0.147 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.3406 (10.6711)\tVB Loss 1.5239 (1.3639)\tF1 0.873 (0.829)\n",
            "Epoch: [154][900/1405]\tBatch Time 0.119 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.5030 (10.6988)\tVB Loss 1.1156 (1.3800)\tF1 0.977 (0.828)\n",
            "Epoch: [154][1200/1405]\tBatch Time 0.149 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.1612 (10.7080)\tVB Loss 2.2909 (1.3917)\tF1 0.788 (0.830)\n",
            "Validation: [0/325]\tBatch Time 0.306 (0.306)\tVB Loss 0.6035 (0.6035)\tF1 Score 0.773 (0.773)\t\n",
            "Validation: [100/325]\tBatch Time 0.099 (0.077)\tVB Loss 2.6408 (1.5017)\tF1 Score 0.691 (0.854)\t\n",
            "Validation: [200/325]\tBatch Time 0.071 (0.075)\tVB Loss 0.8874 (1.5039)\tF1 Score 0.767 (0.855)\t\n",
            "Validation: [300/325]\tBatch Time 0.075 (0.075)\tVB Loss 0.4028 (1.5501)\tF1 Score 0.990 (0.853)\t\n",
            "\n",
            " * LOSS - 1.561, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n",
            "Epoch: [155][0/1405]\tBatch Time 0.388 (0.388)\tData Load Time 0.257 (0.257)\tCE Loss 10.4173 (10.4173)\tVB Loss 1.6821 (1.6821)\tF1 0.891 (0.891)\n",
            "Epoch: [155][300/1405]\tBatch Time 0.106 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.5151 (10.6660)\tVB Loss 0.8128 (1.4186)\tF1 0.920 (0.830)\n",
            "Epoch: [155][600/1405]\tBatch Time 0.105 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.5413 (10.6649)\tVB Loss 1.7877 (1.3614)\tF1 0.619 (0.838)\n",
            "Epoch: [155][900/1405]\tBatch Time 0.125 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.3723 (10.6898)\tVB Loss 1.1655 (1.3868)\tF1 0.718 (0.835)\n",
            "Epoch: [155][1200/1405]\tBatch Time 0.111 (0.126)\tData Load Time 0.004 (0.004)\tCE Loss 9.7300 (10.6814)\tVB Loss 1.1541 (1.3890)\tF1 0.765 (0.835)\n",
            "Validation: [0/325]\tBatch Time 0.289 (0.289)\tVB Loss 2.6000 (2.6000)\tF1 Score 0.761 (0.761)\t\n",
            "Validation: [100/325]\tBatch Time 0.060 (0.074)\tVB Loss 0.9530 (1.4810)\tF1 Score 0.940 (0.862)\t\n",
            "Validation: [200/325]\tBatch Time 0.066 (0.074)\tVB Loss 0.9504 (1.5314)\tF1 Score 0.908 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.056 (0.073)\tVB Loss 0.9431 (1.5226)\tF1 Score 0.806 (0.854)\t\n",
            "\n",
            " * LOSS - 1.559, F1 SCORE - 0.852\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [156][0/1405]\tBatch Time 0.392 (0.392)\tData Load Time 0.242 (0.242)\tCE Loss 10.6103 (10.6103)\tVB Loss 1.0815 (1.0815)\tF1 0.857 (0.857)\n",
            "Epoch: [156][300/1405]\tBatch Time 0.131 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.1187 (10.6459)\tVB Loss 1.1581 (1.3073)\tF1 0.966 (0.841)\n",
            "Epoch: [156][600/1405]\tBatch Time 0.126 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.9057 (10.6788)\tVB Loss 1.6890 (1.3315)\tF1 0.914 (0.843)\n",
            "Epoch: [156][900/1405]\tBatch Time 0.132 (0.126)\tData Load Time 0.005 (0.005)\tCE Loss 10.6127 (10.6720)\tVB Loss 1.1804 (1.3619)\tF1 0.792 (0.838)\n",
            "Epoch: [156][1200/1405]\tBatch Time 0.137 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 11.3150 (10.6799)\tVB Loss 1.6118 (1.3758)\tF1 0.772 (0.838)\n",
            "Validation: [0/325]\tBatch Time 0.289 (0.289)\tVB Loss 2.3050 (2.3050)\tF1 Score 0.834 (0.834)\t\n",
            "Validation: [100/325]\tBatch Time 0.053 (0.074)\tVB Loss 1.0216 (1.5724)\tF1 Score 0.904 (0.855)\t\n",
            "Validation: [200/325]\tBatch Time 0.069 (0.073)\tVB Loss 1.8722 (1.5169)\tF1 Score 0.853 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.071 (0.073)\tVB Loss 1.8376 (1.5616)\tF1 Score 0.924 (0.853)\t\n",
            "\n",
            " * LOSS - 1.567, F1 SCORE - 0.852\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n",
            "Epoch: [157][0/1405]\tBatch Time 0.424 (0.424)\tData Load Time 0.237 (0.237)\tCE Loss 10.8562 (10.8562)\tVB Loss 2.5452 (2.5452)\tF1 0.843 (0.843)\n",
            "Epoch: [157][300/1405]\tBatch Time 0.138 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 10.5456 (10.6621)\tVB Loss 0.3031 (1.3707)\tF1 1.000 (0.837)\n",
            "Epoch: [157][600/1405]\tBatch Time 0.138 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 11.0979 (10.6521)\tVB Loss 1.7958 (1.3653)\tF1 0.894 (0.834)\n",
            "Epoch: [157][900/1405]\tBatch Time 0.120 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.0676 (10.6765)\tVB Loss 0.8890 (1.3655)\tF1 0.895 (0.835)\n",
            "Epoch: [157][1200/1405]\tBatch Time 0.116 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.6897 (10.6875)\tVB Loss 0.5416 (1.3700)\tF1 0.871 (0.836)\n",
            "Validation: [0/325]\tBatch Time 0.278 (0.278)\tVB Loss 0.8755 (0.8755)\tF1 Score 0.953 (0.953)\t\n",
            "Validation: [100/325]\tBatch Time 0.069 (0.076)\tVB Loss 1.3087 (1.3472)\tF1 Score 0.757 (0.860)\t\n",
            "Validation: [200/325]\tBatch Time 0.073 (0.075)\tVB Loss 1.3564 (1.5270)\tF1 Score 0.861 (0.849)\t\n",
            "Validation: [300/325]\tBatch Time 0.074 (0.075)\tVB Loss 1.2362 (1.5532)\tF1 Score 0.922 (0.851)\t\n",
            "\n",
            " * LOSS - 1.545, F1 SCORE - 0.850\n",
            "\n",
            "\n",
            "Epochs since improvement: 4\n",
            "\n",
            "Epoch: [158][0/1405]\tBatch Time 0.433 (0.433)\tData Load Time 0.266 (0.266)\tCE Loss 11.7612 (11.7612)\tVB Loss 0.5946 (0.5946)\tF1 0.970 (0.970)\n",
            "Epoch: [158][300/1405]\tBatch Time 0.108 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 9.7471 (10.6969)\tVB Loss 0.3194 (1.3204)\tF1 0.976 (0.852)\n",
            "Epoch: [158][600/1405]\tBatch Time 0.115 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.8338 (10.6985)\tVB Loss 2.2267 (1.3427)\tF1 0.864 (0.843)\n",
            "Epoch: [158][900/1405]\tBatch Time 0.139 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.5077 (10.6894)\tVB Loss 1.3484 (1.3485)\tF1 0.886 (0.837)\n",
            "Epoch: [158][1200/1405]\tBatch Time 0.112 (0.125)\tData Load Time 0.005 (0.004)\tCE Loss 10.1183 (10.6838)\tVB Loss 1.9478 (1.3671)\tF1 0.799 (0.835)\n",
            "Validation: [0/325]\tBatch Time 0.333 (0.333)\tVB Loss 5.0345 (5.0345)\tF1 Score 0.765 (0.765)\t\n",
            "Validation: [100/325]\tBatch Time 0.100 (0.076)\tVB Loss 1.6779 (1.5656)\tF1 Score 0.859 (0.845)\t\n",
            "Validation: [200/325]\tBatch Time 0.078 (0.077)\tVB Loss 2.6823 (1.6054)\tF1 Score 0.779 (0.850)\t\n",
            "Validation: [300/325]\tBatch Time 0.085 (0.076)\tVB Loss 0.3528 (1.5348)\tF1 Score 0.877 (0.854)\t\n",
            "\n",
            " * LOSS - 1.545, F1 SCORE - 0.853\n",
            "\n",
            "\n",
            "Epochs since improvement: 5\n",
            "\n",
            "Epoch: [159][0/1405]\tBatch Time 0.474 (0.474)\tData Load Time 0.245 (0.245)\tCE Loss 10.5481 (10.5481)\tVB Loss 1.2288 (1.2288)\tF1 0.673 (0.673)\n",
            "Epoch: [159][300/1405]\tBatch Time 0.143 (0.130)\tData Load Time 0.005 (0.005)\tCE Loss 11.1331 (10.6868)\tVB Loss 2.0830 (1.3559)\tF1 0.863 (0.836)\n",
            "Epoch: [159][600/1405]\tBatch Time 0.116 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.8449 (10.6977)\tVB Loss 1.2911 (1.3336)\tF1 0.903 (0.838)\n",
            "Epoch: [159][900/1405]\tBatch Time 0.153 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.5820 (10.7062)\tVB Loss 1.1184 (1.3700)\tF1 0.887 (0.834)\n",
            "Epoch: [159][1200/1405]\tBatch Time 0.103 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.0112 (10.6926)\tVB Loss 0.6064 (1.3664)\tF1 0.832 (0.836)\n",
            "Validation: [0/325]\tBatch Time 0.308 (0.308)\tVB Loss 1.5204 (1.5204)\tF1 Score 0.713 (0.713)\t\n",
            "Validation: [100/325]\tBatch Time 0.096 (0.074)\tVB Loss 0.9222 (1.4297)\tF1 Score 0.874 (0.861)\t\n",
            "Validation: [200/325]\tBatch Time 0.083 (0.075)\tVB Loss 1.2314 (1.5257)\tF1 Score 0.841 (0.860)\t\n",
            "Validation: [300/325]\tBatch Time 0.087 (0.074)\tVB Loss 2.9165 (1.5583)\tF1 Score 0.811 (0.860)\t\n",
            "\n",
            " * LOSS - 1.559, F1 SCORE - 0.856\n",
            "\n",
            "\n",
            "Epochs since improvement: 6\n",
            "\n",
            "Epoch: [160][0/1405]\tBatch Time 0.391 (0.391)\tData Load Time 0.244 (0.244)\tCE Loss 11.0326 (11.0326)\tVB Loss 0.6793 (0.6793)\tF1 0.976 (0.976)\n",
            "Epoch: [160][300/1405]\tBatch Time 0.133 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.4228 (10.7011)\tVB Loss 1.6366 (1.3633)\tF1 0.864 (0.835)\n",
            "Epoch: [160][600/1405]\tBatch Time 0.117 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 9.2147 (10.6900)\tVB Loss 1.7652 (1.3669)\tF1 0.835 (0.836)\n",
            "Epoch: [160][900/1405]\tBatch Time 0.087 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.0034 (10.6930)\tVB Loss 0.6911 (1.3679)\tF1 0.664 (0.834)\n",
            "Epoch: [160][1200/1405]\tBatch Time 0.144 (0.126)\tData Load Time 0.005 (0.005)\tCE Loss 11.2748 (10.6899)\tVB Loss 2.2577 (1.3737)\tF1 0.668 (0.835)\n",
            "Validation: [0/325]\tBatch Time 0.334 (0.334)\tVB Loss 1.7063 (1.7063)\tF1 Score 0.894 (0.894)\t\n",
            "Validation: [100/325]\tBatch Time 0.079 (0.077)\tVB Loss 4.3247 (1.5261)\tF1 Score 0.759 (0.845)\t\n",
            "Validation: [200/325]\tBatch Time 0.060 (0.076)\tVB Loss 0.6185 (1.5490)\tF1 Score 0.751 (0.845)\t\n",
            "Validation: [300/325]\tBatch Time 0.075 (0.074)\tVB Loss 2.0533 (1.5404)\tF1 Score 0.937 (0.849)\t\n",
            "\n",
            " * LOSS - 1.549, F1 SCORE - 0.849\n",
            "\n",
            "\n",
            "Epochs since improvement: 7\n",
            "\n",
            "Epoch: [161][0/1405]\tBatch Time 0.383 (0.383)\tData Load Time 0.241 (0.241)\tCE Loss 10.5327 (10.5327)\tVB Loss 1.2027 (1.2027)\tF1 0.902 (0.902)\n",
            "Epoch: [161][300/1405]\tBatch Time 0.157 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 11.3522 (10.6454)\tVB Loss 1.4916 (1.3471)\tF1 0.808 (0.831)\n",
            "Epoch: [161][600/1405]\tBatch Time 0.125 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 11.1519 (10.6533)\tVB Loss 0.6396 (1.3710)\tF1 0.685 (0.829)\n",
            "Epoch: [161][900/1405]\tBatch Time 0.153 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 11.1398 (10.6697)\tVB Loss 0.5100 (1.3781)\tF1 0.927 (0.831)\n",
            "Epoch: [161][1200/1405]\tBatch Time 0.140 (0.125)\tData Load Time 0.004 (0.004)\tCE Loss 10.8943 (10.6654)\tVB Loss 2.4096 (1.3699)\tF1 0.665 (0.831)\n",
            "Validation: [0/325]\tBatch Time 0.295 (0.295)\tVB Loss 0.7300 (0.7300)\tF1 Score 0.863 (0.863)\t\n",
            "Validation: [100/325]\tBatch Time 0.065 (0.077)\tVB Loss 0.7262 (1.4826)\tF1 Score 0.901 (0.861)\t\n",
            "Validation: [200/325]\tBatch Time 0.060 (0.075)\tVB Loss 2.0131 (1.5492)\tF1 Score 0.630 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.072 (0.074)\tVB Loss 0.5588 (1.5473)\tF1 Score 0.910 (0.857)\t\n",
            "\n",
            " * LOSS - 1.548, F1 SCORE - 0.857\n",
            "\n",
            "\n",
            "Epochs since improvement: 8\n",
            "\n",
            "Epoch: [162][0/1405]\tBatch Time 0.397 (0.397)\tData Load Time 0.259 (0.259)\tCE Loss 10.2980 (10.2980)\tVB Loss 0.3575 (0.3575)\tF1 1.000 (1.000)\n",
            "Epoch: [162][300/1405]\tBatch Time 0.143 (0.131)\tData Load Time 0.005 (0.005)\tCE Loss 10.9631 (10.6802)\tVB Loss 0.4218 (1.3596)\tF1 0.969 (0.844)\n",
            "Epoch: [162][600/1405]\tBatch Time 0.122 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.3845 (10.6621)\tVB Loss 0.9313 (1.3775)\tF1 0.886 (0.843)\n",
            "Epoch: [162][900/1405]\tBatch Time 0.118 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.9550 (10.6701)\tVB Loss 0.3643 (1.3588)\tF1 0.959 (0.845)\n",
            "Epoch: [162][1200/1405]\tBatch Time 0.129 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.3772 (10.6640)\tVB Loss 0.8774 (1.3752)\tF1 0.957 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.357 (0.357)\tVB Loss 1.4228 (1.4228)\tF1 Score 0.838 (0.838)\t\n",
            "Validation: [100/325]\tBatch Time 0.070 (0.075)\tVB Loss 2.2167 (1.4509)\tF1 Score 0.828 (0.854)\t\n",
            "Validation: [200/325]\tBatch Time 0.084 (0.074)\tVB Loss 0.1711 (1.4328)\tF1 Score 1.000 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.065 (0.073)\tVB Loss 0.4567 (1.4741)\tF1 Score 0.937 (0.851)\t\n",
            "\n",
            " * LOSS - 1.522, F1 SCORE - 0.851\n",
            "\n",
            "\n",
            "Epochs since improvement: 9\n",
            "\n",
            "Epoch: [163][0/1405]\tBatch Time 0.528 (0.528)\tData Load Time 0.359 (0.359)\tCE Loss 9.2621 (9.2621)\tVB Loss 1.4473 (1.4473)\tF1 0.574 (0.574)\n",
            "Epoch: [163][300/1405]\tBatch Time 0.113 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 11.0740 (10.6763)\tVB Loss 1.9550 (1.3095)\tF1 0.836 (0.843)\n",
            "Epoch: [163][600/1405]\tBatch Time 0.123 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.4459 (10.6878)\tVB Loss 3.4861 (1.3359)\tF1 0.846 (0.839)\n",
            "Epoch: [163][900/1405]\tBatch Time 0.106 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.9573 (10.6858)\tVB Loss 2.3265 (1.3393)\tF1 0.747 (0.839)\n",
            "Epoch: [163][1200/1405]\tBatch Time 0.096 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 11.2981 (10.6736)\tVB Loss 1.6015 (1.3355)\tF1 0.819 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.307 (0.307)\tVB Loss 1.8756 (1.8756)\tF1 Score 0.948 (0.948)\t\n",
            "Validation: [100/325]\tBatch Time 0.048 (0.081)\tVB Loss 0.8273 (1.6158)\tF1 Score 0.896 (0.861)\t\n",
            "Validation: [200/325]\tBatch Time 0.033 (0.076)\tVB Loss 1.3679 (1.6288)\tF1 Score 0.982 (0.857)\t\n",
            "Validation: [300/325]\tBatch Time 0.081 (0.075)\tVB Loss 1.7200 (1.5424)\tF1 Score 0.873 (0.857)\t\n",
            "\n",
            " * LOSS - 1.554, F1 SCORE - 0.856\n",
            "\n",
            "\n",
            "Epochs since improvement: 10\n",
            "\n",
            "Epoch: [164][0/1405]\tBatch Time 0.418 (0.418)\tData Load Time 0.259 (0.259)\tCE Loss 10.3693 (10.3693)\tVB Loss 0.2362 (0.2362)\tF1 1.000 (1.000)\n",
            "Epoch: [164][300/1405]\tBatch Time 0.134 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.4138 (10.6789)\tVB Loss 1.3861 (1.4092)\tF1 0.746 (0.834)\n",
            "Epoch: [164][600/1405]\tBatch Time 0.105 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 9.6701 (10.6982)\tVB Loss 1.8117 (1.3662)\tF1 0.591 (0.833)\n",
            "Epoch: [164][900/1405]\tBatch Time 0.118 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.1644 (10.6770)\tVB Loss 1.6604 (1.3745)\tF1 0.820 (0.834)\n",
            "Epoch: [164][1200/1405]\tBatch Time 0.147 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.6345 (10.6871)\tVB Loss 2.6597 (1.3697)\tF1 0.544 (0.834)\n",
            "Validation: [0/325]\tBatch Time 0.283 (0.283)\tVB Loss 2.8336 (2.8336)\tF1 Score 0.646 (0.646)\t\n",
            "Validation: [100/325]\tBatch Time 0.079 (0.076)\tVB Loss 1.3600 (1.6879)\tF1 Score 0.709 (0.868)\t\n",
            "Validation: [200/325]\tBatch Time 0.072 (0.075)\tVB Loss 1.0280 (1.5846)\tF1 Score 0.871 (0.863)\t\n",
            "Validation: [300/325]\tBatch Time 0.062 (0.073)\tVB Loss 1.5413 (1.5347)\tF1 Score 0.813 (0.861)\t\n",
            "\n",
            " * LOSS - 1.552, F1 SCORE - 0.861\n",
            "\n",
            "Epoch: [165][0/1405]\tBatch Time 0.381 (0.381)\tData Load Time 0.229 (0.229)\tCE Loss 8.6318 (8.6318)\tVB Loss 1.1038 (1.1038)\tF1 0.717 (0.717)\n",
            "Epoch: [165][300/1405]\tBatch Time 0.136 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 10.8291 (10.7026)\tVB Loss 2.8834 (1.2915)\tF1 0.770 (0.836)\n",
            "Epoch: [165][600/1405]\tBatch Time 0.151 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.8433 (10.6996)\tVB Loss 2.2703 (1.3539)\tF1 0.918 (0.832)\n",
            "Epoch: [165][900/1405]\tBatch Time 0.121 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.4683 (10.6781)\tVB Loss 1.7015 (1.3628)\tF1 0.833 (0.832)\n",
            "Epoch: [165][1200/1405]\tBatch Time 0.095 (0.127)\tData Load Time 0.004 (0.004)\tCE Loss 9.9100 (10.6687)\tVB Loss 0.3213 (1.3415)\tF1 0.854 (0.833)\n",
            "Validation: [0/325]\tBatch Time 0.285 (0.285)\tVB Loss 0.2980 (0.2980)\tF1 Score 0.987 (0.987)\t\n",
            "Validation: [100/325]\tBatch Time 0.064 (0.074)\tVB Loss 0.4693 (1.6123)\tF1 Score 0.989 (0.848)\t\n",
            "Validation: [200/325]\tBatch Time 0.079 (0.075)\tVB Loss 1.4285 (1.6057)\tF1 Score 0.909 (0.849)\t\n",
            "Validation: [300/325]\tBatch Time 0.062 (0.073)\tVB Loss 0.7217 (1.5469)\tF1 Score 0.929 (0.852)\t\n",
            "\n",
            " * LOSS - 1.553, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n",
            "Epoch: [166][0/1405]\tBatch Time 0.393 (0.393)\tData Load Time 0.233 (0.233)\tCE Loss 10.2339 (10.2339)\tVB Loss 1.1677 (1.1677)\tF1 0.762 (0.762)\n",
            "Epoch: [166][300/1405]\tBatch Time 0.119 (0.129)\tData Load Time 0.005 (0.005)\tCE Loss 11.1096 (10.7137)\tVB Loss 1.5578 (1.3569)\tF1 0.824 (0.833)\n",
            "Epoch: [166][600/1405]\tBatch Time 0.143 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.5985 (10.6772)\tVB Loss 1.3175 (1.3363)\tF1 0.894 (0.835)\n",
            "Epoch: [166][900/1405]\tBatch Time 0.110 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 9.9398 (10.6647)\tVB Loss 2.9081 (1.3319)\tF1 0.807 (0.834)\n",
            "Epoch: [166][1200/1405]\tBatch Time 0.132 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.8298 (10.6731)\tVB Loss 1.1780 (1.3417)\tF1 0.701 (0.835)\n",
            "Validation: [0/325]\tBatch Time 0.307 (0.307)\tVB Loss 4.0432 (4.0432)\tF1 Score 0.631 (0.631)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.074)\tVB Loss 1.7644 (1.3562)\tF1 Score 0.780 (0.857)\t\n",
            "Validation: [200/325]\tBatch Time 0.072 (0.075)\tVB Loss 1.3104 (1.5809)\tF1 Score 0.908 (0.846)\t\n",
            "Validation: [300/325]\tBatch Time 0.085 (0.074)\tVB Loss 0.6796 (1.5624)\tF1 Score 0.899 (0.847)\t\n",
            "\n",
            " * LOSS - 1.545, F1 SCORE - 0.848\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [167][0/1405]\tBatch Time 0.397 (0.397)\tData Load Time 0.231 (0.231)\tCE Loss 10.1881 (10.1881)\tVB Loss 1.5277 (1.5277)\tF1 0.695 (0.695)\n",
            "Epoch: [167][300/1405]\tBatch Time 0.124 (0.131)\tData Load Time 0.004 (0.005)\tCE Loss 11.5838 (10.6879)\tVB Loss 1.1150 (1.3343)\tF1 0.919 (0.840)\n",
            "Epoch: [167][600/1405]\tBatch Time 0.137 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.1270 (10.6719)\tVB Loss 0.8214 (1.3181)\tF1 0.876 (0.838)\n",
            "Epoch: [167][900/1405]\tBatch Time 0.130 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.4078 (10.6632)\tVB Loss 1.2988 (1.3286)\tF1 0.851 (0.836)\n",
            "Epoch: [167][1200/1405]\tBatch Time 0.076 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.8850 (10.6677)\tVB Loss 0.3682 (1.3337)\tF1 0.917 (0.837)\n",
            "Validation: [0/325]\tBatch Time 0.300 (0.300)\tVB Loss 1.6335 (1.6335)\tF1 Score 0.887 (0.887)\t\n",
            "Validation: [100/325]\tBatch Time 0.095 (0.078)\tVB Loss 0.5443 (1.5426)\tF1 Score 0.904 (0.846)\t\n",
            "Validation: [200/325]\tBatch Time 0.055 (0.076)\tVB Loss 1.0417 (1.5304)\tF1 Score 0.776 (0.852)\t\n",
            "Validation: [300/325]\tBatch Time 0.049 (0.074)\tVB Loss 3.2955 (1.5361)\tF1 Score 0.742 (0.856)\t\n",
            "\n",
            " * LOSS - 1.531, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n",
            "Epoch: [168][0/1405]\tBatch Time 0.371 (0.371)\tData Load Time 0.231 (0.231)\tCE Loss 10.4256 (10.4256)\tVB Loss 1.1895 (1.1895)\tF1 0.930 (0.930)\n",
            "Epoch: [168][300/1405]\tBatch Time 0.135 (0.131)\tData Load Time 0.004 (0.005)\tCE Loss 11.5412 (10.6897)\tVB Loss 0.8874 (1.3044)\tF1 0.970 (0.845)\n",
            "Epoch: [168][600/1405]\tBatch Time 0.149 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.8353 (10.6824)\tVB Loss 1.3052 (1.3347)\tF1 0.877 (0.837)\n",
            "Epoch: [168][900/1405]\tBatch Time 0.153 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.7705 (10.6713)\tVB Loss 1.5681 (1.3554)\tF1 0.807 (0.836)\n",
            "Epoch: [168][1200/1405]\tBatch Time 0.107 (0.126)\tData Load Time 0.005 (0.004)\tCE Loss 10.9289 (10.6556)\tVB Loss 0.9933 (1.3464)\tF1 0.808 (0.837)\n",
            "Validation: [0/325]\tBatch Time 0.346 (0.346)\tVB Loss 0.8075 (0.8075)\tF1 Score 0.944 (0.944)\t\n",
            "Validation: [100/325]\tBatch Time 0.051 (0.078)\tVB Loss 2.6553 (1.6977)\tF1 Score 0.855 (0.842)\t\n",
            "Validation: [200/325]\tBatch Time 0.068 (0.074)\tVB Loss 1.8641 (1.5461)\tF1 Score 0.786 (0.851)\t\n",
            "Validation: [300/325]\tBatch Time 0.069 (0.075)\tVB Loss 5.1036 (1.5507)\tF1 Score 0.722 (0.851)\t\n",
            "\n",
            " * LOSS - 1.546, F1 SCORE - 0.853\n",
            "\n",
            "\n",
            "Epochs since improvement: 4\n",
            "\n",
            "Epoch: [169][0/1405]\tBatch Time 0.355 (0.355)\tData Load Time 0.230 (0.230)\tCE Loss 9.6493 (9.6493)\tVB Loss 0.8329 (0.8329)\tF1 0.933 (0.933)\n",
            "Epoch: [169][300/1405]\tBatch Time 0.099 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 9.6752 (10.7136)\tVB Loss 0.3206 (1.3564)\tF1 0.977 (0.830)\n",
            "Epoch: [169][600/1405]\tBatch Time 0.126 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.5775 (10.6791)\tVB Loss 2.5346 (1.3272)\tF1 0.804 (0.834)\n",
            "Epoch: [169][900/1405]\tBatch Time 0.137 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.0977 (10.6741)\tVB Loss 2.9920 (1.3243)\tF1 0.814 (0.834)\n",
            "Epoch: [169][1200/1405]\tBatch Time 0.145 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.6037 (10.6638)\tVB Loss 1.8344 (1.3331)\tF1 0.565 (0.833)\n",
            "Validation: [0/325]\tBatch Time 0.272 (0.272)\tVB Loss 0.1642 (0.1642)\tF1 Score 0.915 (0.915)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.076)\tVB Loss 1.9318 (1.4362)\tF1 Score 0.789 (0.859)\t\n",
            "Validation: [200/325]\tBatch Time 0.076 (0.075)\tVB Loss 0.8835 (1.5436)\tF1 Score 0.758 (0.859)\t\n",
            "Validation: [300/325]\tBatch Time 0.082 (0.075)\tVB Loss 2.1151 (1.5570)\tF1 Score 0.868 (0.857)\t\n",
            "\n",
            " * LOSS - 1.533, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 5\n",
            "\n",
            "Epoch: [170][0/1405]\tBatch Time 0.642 (0.642)\tData Load Time 0.348 (0.348)\tCE Loss 10.5843 (10.5843)\tVB Loss 0.8254 (0.8254)\tF1 0.965 (0.965)\n",
            "Epoch: [170][300/1405]\tBatch Time 0.113 (0.131)\tData Load Time 0.004 (0.006)\tCE Loss 10.6302 (10.6792)\tVB Loss 0.9401 (1.3758)\tF1 0.863 (0.837)\n",
            "Epoch: [170][600/1405]\tBatch Time 0.158 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 11.6260 (10.6455)\tVB Loss 1.8551 (1.3406)\tF1 0.686 (0.838)\n",
            "Epoch: [170][900/1405]\tBatch Time 0.141 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 11.4579 (10.6652)\tVB Loss 1.0963 (1.3549)\tF1 0.751 (0.835)\n",
            "Epoch: [170][1200/1405]\tBatch Time 0.080 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.8710 (10.6569)\tVB Loss 0.1254 (1.3401)\tF1 1.000 (0.838)\n",
            "Validation: [0/325]\tBatch Time 0.356 (0.356)\tVB Loss 2.7169 (2.7169)\tF1 Score 0.802 (0.802)\t\n",
            "Validation: [100/325]\tBatch Time 0.067 (0.081)\tVB Loss 2.8599 (1.7768)\tF1 Score 0.865 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.063 (0.077)\tVB Loss 0.7115 (1.6615)\tF1 Score 0.896 (0.855)\t\n",
            "Validation: [300/325]\tBatch Time 0.062 (0.075)\tVB Loss 1.4328 (1.5582)\tF1 Score 0.910 (0.857)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.857\n",
            "\n",
            "\n",
            "Epochs since improvement: 6\n",
            "\n",
            "Epoch: [171][0/1405]\tBatch Time 0.414 (0.414)\tData Load Time 0.268 (0.268)\tCE Loss 9.5905 (9.5905)\tVB Loss 0.2231 (0.2231)\tF1 1.000 (1.000)\n",
            "Epoch: [171][300/1405]\tBatch Time 0.095 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.8186 (10.6932)\tVB Loss 1.1404 (1.3523)\tF1 0.688 (0.838)\n",
            "Epoch: [171][600/1405]\tBatch Time 0.126 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 9.8090 (10.6498)\tVB Loss 1.1843 (1.3495)\tF1 0.962 (0.839)\n",
            "Epoch: [171][900/1405]\tBatch Time 0.146 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.3060 (10.6662)\tVB Loss 1.9572 (1.3361)\tF1 0.858 (0.841)\n",
            "Epoch: [171][1200/1405]\tBatch Time 0.121 (0.126)\tData Load Time 0.004 (0.004)\tCE Loss 10.1921 (10.6713)\tVB Loss 0.6324 (1.3321)\tF1 0.697 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.336 (0.336)\tVB Loss 2.1454 (2.1454)\tF1 Score 0.830 (0.830)\t\n",
            "Validation: [100/325]\tBatch Time 0.071 (0.077)\tVB Loss 0.6395 (1.6347)\tF1 Score 0.970 (0.838)\t\n",
            "Validation: [200/325]\tBatch Time 0.063 (0.075)\tVB Loss 1.4549 (1.5419)\tF1 Score 0.827 (0.850)\t\n",
            "Validation: [300/325]\tBatch Time 0.059 (0.075)\tVB Loss 1.0123 (1.5337)\tF1 Score 0.836 (0.849)\t\n",
            "\n",
            " * LOSS - 1.542, F1 SCORE - 0.850\n",
            "\n",
            "\n",
            "Epochs since improvement: 7\n",
            "\n",
            "Epoch: [172][0/1405]\tBatch Time 0.367 (0.367)\tData Load Time 0.230 (0.230)\tCE Loss 10.1542 (10.1542)\tVB Loss 0.5324 (0.5324)\tF1 0.925 (0.925)\n",
            "Epoch: [172][300/1405]\tBatch Time 0.111 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.5047 (10.6910)\tVB Loss 0.5723 (1.3543)\tF1 0.823 (0.842)\n",
            "Epoch: [172][600/1405]\tBatch Time 0.128 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.6633 (10.6679)\tVB Loss 0.5843 (1.3548)\tF1 0.925 (0.842)\n",
            "Epoch: [172][900/1405]\tBatch Time 0.126 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 9.5060 (10.6743)\tVB Loss 1.0591 (1.3373)\tF1 0.802 (0.840)\n",
            "Epoch: [172][1200/1405]\tBatch Time 0.139 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 10.0791 (10.6631)\tVB Loss 1.1357 (1.3152)\tF1 0.835 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.306 (0.306)\tVB Loss 1.0247 (1.0247)\tF1 Score 0.966 (0.966)\t\n",
            "Validation: [100/325]\tBatch Time 0.062 (0.078)\tVB Loss 0.7351 (1.5834)\tF1 Score 0.870 (0.842)\t\n",
            "Validation: [200/325]\tBatch Time 0.062 (0.075)\tVB Loss 2.9366 (1.5157)\tF1 Score 0.775 (0.856)\t\n",
            "Validation: [300/325]\tBatch Time 0.076 (0.075)\tVB Loss 1.2899 (1.5173)\tF1 Score 0.919 (0.854)\t\n",
            "\n",
            " * LOSS - 1.541, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 8\n",
            "\n",
            "Epoch: [173][0/1405]\tBatch Time 0.399 (0.399)\tData Load Time 0.230 (0.230)\tCE Loss 11.0849 (11.0849)\tVB Loss 1.4163 (1.4163)\tF1 0.832 (0.832)\n",
            "Epoch: [173][300/1405]\tBatch Time 0.151 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.8377 (10.7010)\tVB Loss 0.7743 (1.3609)\tF1 0.770 (0.831)\n",
            "Epoch: [173][600/1405]\tBatch Time 0.148 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.6589 (10.6809)\tVB Loss 2.4328 (1.3325)\tF1 0.876 (0.839)\n",
            "Epoch: [173][900/1405]\tBatch Time 0.155 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.9785 (10.6863)\tVB Loss 1.7275 (1.3276)\tF1 0.871 (0.840)\n",
            "Epoch: [173][1200/1405]\tBatch Time 0.143 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.0392 (10.6786)\tVB Loss 0.7566 (1.3319)\tF1 0.923 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.308 (0.308)\tVB Loss 2.2828 (2.2828)\tF1 Score 0.811 (0.811)\t\n",
            "Validation: [100/325]\tBatch Time 0.069 (0.077)\tVB Loss 1.2823 (1.5480)\tF1 Score 0.764 (0.859)\t\n",
            "Validation: [200/325]\tBatch Time 0.077 (0.076)\tVB Loss 0.6720 (1.4972)\tF1 Score 0.940 (0.861)\t\n",
            "Validation: [300/325]\tBatch Time 0.049 (0.075)\tVB Loss 1.6308 (1.5249)\tF1 Score 0.777 (0.855)\t\n",
            "\n",
            " * LOSS - 1.536, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 9\n",
            "\n",
            "Epoch: [174][0/1405]\tBatch Time 0.439 (0.439)\tData Load Time 0.260 (0.260)\tCE Loss 11.0241 (11.0241)\tVB Loss 2.3611 (2.3611)\tF1 0.681 (0.681)\n",
            "Epoch: [174][300/1405]\tBatch Time 0.131 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.3291 (10.6541)\tVB Loss 1.4398 (1.2809)\tF1 0.856 (0.848)\n",
            "Epoch: [174][600/1405]\tBatch Time 0.103 (0.125)\tData Load Time 0.005 (0.005)\tCE Loss 9.8272 (10.6684)\tVB Loss 2.2726 (1.3247)\tF1 0.832 (0.842)\n",
            "Epoch: [174][900/1405]\tBatch Time 0.128 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.9103 (10.6636)\tVB Loss 1.7439 (1.3327)\tF1 0.917 (0.840)\n",
            "Epoch: [174][1200/1405]\tBatch Time 0.114 (0.126)\tData Load Time 0.005 (0.005)\tCE Loss 10.5148 (10.6642)\tVB Loss 1.4808 (1.3418)\tF1 0.555 (0.838)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 1.7292 (1.7292)\tF1 Score 0.859 (0.859)\t\n",
            "Validation: [100/325]\tBatch Time 0.091 (0.076)\tVB Loss 1.1073 (1.4999)\tF1 Score 0.892 (0.861)\t\n",
            "Validation: [200/325]\tBatch Time 0.081 (0.075)\tVB Loss 1.2005 (1.5626)\tF1 Score 0.826 (0.860)\t\n",
            "Validation: [300/325]\tBatch Time 0.068 (0.075)\tVB Loss 0.7213 (1.5418)\tF1 Score 0.832 (0.860)\t\n",
            "\n",
            " * LOSS - 1.551, F1 SCORE - 0.859\n",
            "\n",
            "Epoch: [175][0/1405]\tBatch Time 0.421 (0.421)\tData Load Time 0.233 (0.233)\tCE Loss 11.4879 (11.4879)\tVB Loss 1.3145 (1.3145)\tF1 0.872 (0.872)\n",
            "Epoch: [175][300/1405]\tBatch Time 0.100 (0.136)\tData Load Time 0.005 (0.006)\tCE Loss 9.9834 (10.6808)\tVB Loss 0.8097 (1.3732)\tF1 0.963 (0.827)\n",
            "Epoch: [175][600/1405]\tBatch Time 0.172 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 11.6733 (10.6710)\tVB Loss 0.9397 (1.3250)\tF1 0.916 (0.834)\n",
            "Epoch: [175][900/1405]\tBatch Time 0.113 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 9.3968 (10.6780)\tVB Loss 0.7458 (1.3241)\tF1 0.965 (0.835)\n",
            "Epoch: [175][1200/1405]\tBatch Time 0.105 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.7546 (10.6659)\tVB Loss 1.3845 (1.3389)\tF1 0.704 (0.835)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 2.3804 (2.3804)\tF1 Score 0.713 (0.713)\t\n",
            "Validation: [100/325]\tBatch Time 0.058 (0.076)\tVB Loss 2.7996 (1.5462)\tF1 Score 0.591 (0.849)\t\n",
            "Validation: [200/325]\tBatch Time 0.093 (0.075)\tVB Loss 0.6818 (1.5532)\tF1 Score 0.943 (0.852)\t\n",
            "Validation: [300/325]\tBatch Time 0.056 (0.075)\tVB Loss 1.1146 (1.5167)\tF1 Score 0.709 (0.854)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n",
            "Epoch: [176][0/1405]\tBatch Time 0.425 (0.425)\tData Load Time 0.245 (0.245)\tCE Loss 10.8237 (10.8237)\tVB Loss 1.7204 (1.7204)\tF1 0.850 (0.850)\n",
            "Epoch: [176][300/1405]\tBatch Time 0.163 (0.131)\tData Load Time 0.004 (0.006)\tCE Loss 9.2597 (10.6999)\tVB Loss 1.0098 (1.3829)\tF1 0.782 (0.834)\n",
            "Epoch: [176][600/1405]\tBatch Time 0.137 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 11.2657 (10.6847)\tVB Loss 1.0057 (1.3521)\tF1 0.952 (0.838)\n",
            "Epoch: [176][900/1405]\tBatch Time 0.122 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.5905 (10.6654)\tVB Loss 2.1675 (1.3380)\tF1 0.880 (0.841)\n",
            "Epoch: [176][1200/1405]\tBatch Time 0.127 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 11.1717 (10.6622)\tVB Loss 1.8923 (1.3314)\tF1 0.735 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.315 (0.315)\tVB Loss 2.7714 (2.7714)\tF1 Score 0.680 (0.680)\t\n",
            "Validation: [100/325]\tBatch Time 0.046 (0.076)\tVB Loss 3.2147 (1.4833)\tF1 Score 0.838 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.079 (0.074)\tVB Loss 1.9270 (1.5146)\tF1 Score 0.599 (0.860)\t\n",
            "Validation: [300/325]\tBatch Time 0.060 (0.074)\tVB Loss 1.7552 (1.5252)\tF1 Score 0.830 (0.856)\t\n",
            "\n",
            " * LOSS - 1.541, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [177][0/1405]\tBatch Time 0.421 (0.421)\tData Load Time 0.247 (0.247)\tCE Loss 11.0230 (11.0230)\tVB Loss 1.6620 (1.6620)\tF1 0.656 (0.656)\n",
            "Epoch: [177][300/1405]\tBatch Time 0.132 (0.130)\tData Load Time 0.005 (0.005)\tCE Loss 10.5585 (10.7042)\tVB Loss 0.8087 (1.3350)\tF1 0.891 (0.836)\n",
            "Epoch: [177][600/1405]\tBatch Time 0.156 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 11.0481 (10.6881)\tVB Loss 1.9613 (1.3448)\tF1 0.821 (0.838)\n",
            "Epoch: [177][900/1405]\tBatch Time 0.141 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.3434 (10.6735)\tVB Loss 2.4233 (1.3465)\tF1 0.919 (0.837)\n",
            "Epoch: [177][1200/1405]\tBatch Time 0.140 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.6831 (10.6683)\tVB Loss 1.7497 (1.3323)\tF1 0.879 (0.837)\n",
            "Validation: [0/325]\tBatch Time 0.303 (0.303)\tVB Loss 0.9922 (0.9922)\tF1 Score 0.847 (0.847)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.078)\tVB Loss 0.9176 (1.5315)\tF1 Score 0.698 (0.855)\t\n",
            "Validation: [200/325]\tBatch Time 0.072 (0.075)\tVB Loss 1.5968 (1.5183)\tF1 Score 0.762 (0.855)\t\n",
            "Validation: [300/325]\tBatch Time 0.084 (0.076)\tVB Loss 1.7835 (1.5406)\tF1 Score 0.869 (0.851)\t\n",
            "\n",
            " * LOSS - 1.544, F1 SCORE - 0.850\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n",
            "Epoch: [178][0/1405]\tBatch Time 0.441 (0.441)\tData Load Time 0.228 (0.228)\tCE Loss 10.8790 (10.8790)\tVB Loss 1.7096 (1.7096)\tF1 0.857 (0.857)\n",
            "Epoch: [178][300/1405]\tBatch Time 0.147 (0.133)\tData Load Time 0.005 (0.005)\tCE Loss 11.5764 (10.6997)\tVB Loss 1.1043 (1.3128)\tF1 0.927 (0.844)\n",
            "Epoch: [178][600/1405]\tBatch Time 0.117 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 10.5891 (10.6518)\tVB Loss 1.3865 (1.3140)\tF1 0.936 (0.843)\n",
            "Epoch: [178][900/1405]\tBatch Time 0.150 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 9.7252 (10.6610)\tVB Loss 0.7628 (1.3186)\tF1 0.903 (0.841)\n",
            "Epoch: [178][1200/1405]\tBatch Time 0.123 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.8151 (10.6594)\tVB Loss 1.3113 (1.3178)\tF1 0.919 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.272 (0.272)\tVB Loss 1.6915 (1.6915)\tF1 Score 0.867 (0.867)\t\n",
            "Validation: [100/325]\tBatch Time 0.081 (0.077)\tVB Loss 0.7118 (1.4820)\tF1 Score 0.982 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.095 (0.076)\tVB Loss 2.3160 (1.5128)\tF1 Score 0.817 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.069 (0.075)\tVB Loss 2.3672 (1.5188)\tF1 Score 0.828 (0.855)\t\n",
            "\n",
            " * LOSS - 1.539, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 4\n",
            "\n",
            "Epoch: [179][0/1405]\tBatch Time 0.380 (0.380)\tData Load Time 0.244 (0.244)\tCE Loss 11.3283 (11.3283)\tVB Loss 0.9296 (0.9296)\tF1 0.825 (0.825)\n",
            "Epoch: [179][300/1405]\tBatch Time 0.093 (0.131)\tData Load Time 0.005 (0.005)\tCE Loss 11.4936 (10.6715)\tVB Loss 0.6004 (1.3305)\tF1 0.956 (0.842)\n",
            "Epoch: [179][600/1405]\tBatch Time 0.127 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.4644 (10.6501)\tVB Loss 0.9903 (1.3238)\tF1 0.910 (0.845)\n",
            "Epoch: [179][900/1405]\tBatch Time 0.142 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.2072 (10.6704)\tVB Loss 1.9576 (1.3221)\tF1 0.941 (0.845)\n",
            "Epoch: [179][1200/1405]\tBatch Time 0.125 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.4613 (10.6650)\tVB Loss 0.9619 (1.3373)\tF1 0.948 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.355 (0.355)\tVB Loss 2.8954 (2.8954)\tF1 Score 0.771 (0.771)\t\n",
            "Validation: [100/325]\tBatch Time 0.071 (0.079)\tVB Loss 0.5183 (1.7733)\tF1 Score 0.964 (0.838)\t\n",
            "Validation: [200/325]\tBatch Time 0.067 (0.077)\tVB Loss 0.6140 (1.6408)\tF1 Score 0.986 (0.845)\t\n",
            "Validation: [300/325]\tBatch Time 0.069 (0.075)\tVB Loss 0.9634 (1.5489)\tF1 Score 0.732 (0.853)\t\n",
            "\n",
            " * LOSS - 1.539, F1 SCORE - 0.851\n",
            "\n",
            "\n",
            "Epochs since improvement: 5\n",
            "\n",
            "Epoch: [180][0/1405]\tBatch Time 0.391 (0.391)\tData Load Time 0.221 (0.221)\tCE Loss 11.3292 (11.3292)\tVB Loss 2.1082 (2.1082)\tF1 0.654 (0.654)\n",
            "Epoch: [180][300/1405]\tBatch Time 0.134 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.9626 (10.6426)\tVB Loss 0.7822 (1.2648)\tF1 0.945 (0.846)\n",
            "Epoch: [180][600/1405]\tBatch Time 0.123 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.1330 (10.6609)\tVB Loss 1.7212 (1.2962)\tF1 0.700 (0.844)\n",
            "Epoch: [180][900/1405]\tBatch Time 0.121 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 11.8547 (10.6596)\tVB Loss 2.4984 (1.3182)\tF1 0.850 (0.842)\n",
            "Epoch: [180][1200/1405]\tBatch Time 0.116 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.4575 (10.6561)\tVB Loss 0.3116 (1.3038)\tF1 0.968 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.304 (0.304)\tVB Loss 1.3178 (1.3178)\tF1 Score 0.810 (0.810)\t\n",
            "Validation: [100/325]\tBatch Time 0.050 (0.075)\tVB Loss 0.9549 (1.5990)\tF1 Score 0.779 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.068 (0.075)\tVB Loss 1.1555 (1.5803)\tF1 Score 0.850 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.072 (0.074)\tVB Loss 3.6736 (1.5526)\tF1 Score 0.654 (0.856)\t\n",
            "\n",
            " * LOSS - 1.547, F1 SCORE - 0.856\n",
            "\n",
            "\n",
            "Epochs since improvement: 6\n",
            "\n",
            "Epoch: [181][0/1405]\tBatch Time 0.381 (0.381)\tData Load Time 0.221 (0.221)\tCE Loss 12.1032 (12.1032)\tVB Loss 0.8953 (0.8953)\tF1 0.937 (0.937)\n",
            "Epoch: [181][300/1405]\tBatch Time 0.066 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 9.3662 (10.6474)\tVB Loss 0.0770 (1.2479)\tF1 1.000 (0.839)\n",
            "Epoch: [181][600/1405]\tBatch Time 0.141 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.0681 (10.6405)\tVB Loss 1.0715 (1.2764)\tF1 0.871 (0.843)\n",
            "Epoch: [181][900/1405]\tBatch Time 0.098 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 9.5672 (10.6308)\tVB Loss 1.6271 (1.2743)\tF1 0.668 (0.842)\n",
            "Epoch: [181][1200/1405]\tBatch Time 0.142 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.5601 (10.6521)\tVB Loss 1.2645 (1.3117)\tF1 0.906 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.297 (0.297)\tVB Loss 1.6285 (1.6285)\tF1 Score 0.881 (0.881)\t\n",
            "Validation: [100/325]\tBatch Time 0.065 (0.076)\tVB Loss 1.0424 (1.4428)\tF1 Score 0.892 (0.862)\t\n",
            "Validation: [200/325]\tBatch Time 0.074 (0.076)\tVB Loss 3.7193 (1.5728)\tF1 Score 0.777 (0.855)\t\n",
            "Validation: [300/325]\tBatch Time 0.064 (0.075)\tVB Loss 0.9698 (1.5557)\tF1 Score 0.934 (0.852)\t\n",
            "\n",
            " * LOSS - 1.546, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 7\n",
            "\n",
            "Epoch: [182][0/1405]\tBatch Time 0.411 (0.411)\tData Load Time 0.247 (0.247)\tCE Loss 10.5800 (10.5800)\tVB Loss 1.0028 (1.0028)\tF1 0.930 (0.930)\n",
            "Epoch: [182][300/1405]\tBatch Time 0.127 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.0513 (10.6430)\tVB Loss 1.3185 (1.3923)\tF1 0.940 (0.838)\n",
            "Epoch: [182][600/1405]\tBatch Time 0.111 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.6673 (10.6537)\tVB Loss 0.9142 (1.3641)\tF1 0.950 (0.841)\n",
            "Epoch: [182][900/1405]\tBatch Time 0.115 (0.126)\tData Load Time 0.005 (0.005)\tCE Loss 10.5375 (10.6392)\tVB Loss 1.9615 (1.3241)\tF1 0.692 (0.842)\n",
            "Epoch: [182][1200/1405]\tBatch Time 0.122 (0.125)\tData Load Time 0.005 (0.005)\tCE Loss 10.5338 (10.6477)\tVB Loss 0.5207 (1.3276)\tF1 0.885 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.308 (0.308)\tVB Loss 2.2441 (2.2441)\tF1 Score 0.896 (0.896)\t\n",
            "Validation: [100/325]\tBatch Time 0.078 (0.077)\tVB Loss 2.3265 (1.5004)\tF1 Score 0.811 (0.848)\t\n",
            "Validation: [200/325]\tBatch Time 0.065 (0.076)\tVB Loss 2.4056 (1.5465)\tF1 Score 0.589 (0.856)\t\n",
            "Validation: [300/325]\tBatch Time 0.071 (0.074)\tVB Loss 1.3066 (1.5149)\tF1 Score 0.932 (0.857)\t\n",
            "\n",
            " * LOSS - 1.551, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 8\n",
            "\n",
            "Epoch: [183][0/1405]\tBatch Time 0.332 (0.332)\tData Load Time 0.233 (0.233)\tCE Loss 10.0115 (10.0115)\tVB Loss 0.5848 (0.5848)\tF1 0.765 (0.765)\n",
            "Epoch: [183][300/1405]\tBatch Time 0.112 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.4026 (10.6680)\tVB Loss 1.2988 (1.2734)\tF1 0.811 (0.836)\n",
            "Epoch: [183][600/1405]\tBatch Time 0.098 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.4883 (10.6529)\tVB Loss 1.3008 (1.3070)\tF1 0.458 (0.841)\n",
            "Epoch: [183][900/1405]\tBatch Time 0.112 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.1655 (10.6506)\tVB Loss 1.4510 (1.3217)\tF1 0.662 (0.839)\n",
            "Epoch: [183][1200/1405]\tBatch Time 0.108 (0.127)\tData Load Time 0.004 (0.004)\tCE Loss 10.7378 (10.6579)\tVB Loss 1.1462 (1.3126)\tF1 0.852 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.285 (0.285)\tVB Loss 3.8780 (3.8780)\tF1 Score 0.705 (0.705)\t\n",
            "Validation: [100/325]\tBatch Time 0.056 (0.075)\tVB Loss 0.4957 (1.6192)\tF1 Score 0.964 (0.861)\t\n",
            "Validation: [200/325]\tBatch Time 0.077 (0.073)\tVB Loss 2.2938 (1.6105)\tF1 Score 0.740 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.062 (0.073)\tVB Loss 1.3333 (1.5453)\tF1 Score 0.786 (0.855)\t\n",
            "\n",
            " * LOSS - 1.549, F1 SCORE - 0.854\n",
            "\n",
            "\n",
            "Epochs since improvement: 9\n",
            "\n",
            "Epoch: [184][0/1405]\tBatch Time 0.411 (0.411)\tData Load Time 0.241 (0.241)\tCE Loss 11.5329 (11.5329)\tVB Loss 1.4741 (1.4741)\tF1 0.976 (0.976)\n",
            "Epoch: [184][300/1405]\tBatch Time 0.120 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.0701 (10.6955)\tVB Loss 1.9423 (1.2593)\tF1 0.747 (0.854)\n",
            "Epoch: [184][600/1405]\tBatch Time 0.116 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.8310 (10.6705)\tVB Loss 1.7839 (1.2960)\tF1 0.704 (0.850)\n",
            "Epoch: [184][900/1405]\tBatch Time 0.119 (0.126)\tData Load Time 0.004 (0.004)\tCE Loss 11.6474 (10.6600)\tVB Loss 1.1785 (1.2931)\tF1 0.854 (0.847)\n",
            "Epoch: [184][1200/1405]\tBatch Time 0.126 (0.125)\tData Load Time 0.004 (0.004)\tCE Loss 10.9807 (10.6589)\tVB Loss 0.6459 (1.3170)\tF1 0.874 (0.845)\n",
            "Validation: [0/325]\tBatch Time 0.322 (0.322)\tVB Loss 0.4310 (0.4310)\tF1 Score 0.980 (0.980)\t\n",
            "Validation: [100/325]\tBatch Time 0.078 (0.077)\tVB Loss 1.8409 (1.5823)\tF1 Score 0.864 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.072 (0.076)\tVB Loss 0.7962 (1.5769)\tF1 Score 0.945 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.071 (0.074)\tVB Loss 1.7062 (1.5319)\tF1 Score 0.909 (0.860)\t\n",
            "\n",
            " * LOSS - 1.540, F1 SCORE - 0.857\n",
            "\n",
            "Epoch: [185][0/1405]\tBatch Time 0.412 (0.412)\tData Load Time 0.249 (0.249)\tCE Loss 10.5233 (10.5233)\tVB Loss 1.0438 (1.0438)\tF1 0.889 (0.889)\n",
            "Epoch: [185][300/1405]\tBatch Time 0.153 (0.135)\tData Load Time 0.005 (0.006)\tCE Loss 10.3955 (10.6951)\tVB Loss 0.8701 (1.2611)\tF1 0.899 (0.853)\n",
            "Epoch: [185][600/1405]\tBatch Time 0.132 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 10.7698 (10.6553)\tVB Loss 1.2841 (1.2973)\tF1 0.931 (0.840)\n",
            "Epoch: [185][900/1405]\tBatch Time 0.168 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 10.3345 (10.6539)\tVB Loss 1.3630 (1.3040)\tF1 0.824 (0.842)\n",
            "Epoch: [185][1200/1405]\tBatch Time 0.139 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 11.6196 (10.6452)\tVB Loss 1.2774 (1.3010)\tF1 0.899 (0.843)\n",
            "Validation: [0/325]\tBatch Time 0.298 (0.298)\tVB Loss 2.2223 (2.2223)\tF1 Score 0.843 (0.843)\t\n",
            "Validation: [100/325]\tBatch Time 0.060 (0.075)\tVB Loss 0.5489 (1.6525)\tF1 Score 0.922 (0.845)\t\n",
            "Validation: [200/325]\tBatch Time 0.070 (0.073)\tVB Loss 1.3149 (1.5472)\tF1 Score 0.908 (0.850)\t\n",
            "Validation: [300/325]\tBatch Time 0.081 (0.073)\tVB Loss 0.6160 (1.5515)\tF1 Score 0.818 (0.852)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n",
            "Epoch: [186][0/1405]\tBatch Time 0.347 (0.347)\tData Load Time 0.236 (0.236)\tCE Loss 9.9993 (9.9993)\tVB Loss 0.3158 (0.3158)\tF1 0.899 (0.899)\n",
            "Epoch: [186][300/1405]\tBatch Time 0.134 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 11.2404 (10.6358)\tVB Loss 2.7572 (1.3810)\tF1 0.860 (0.835)\n",
            "Epoch: [186][600/1405]\tBatch Time 0.223 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.2632 (10.6315)\tVB Loss 2.0108 (1.3347)\tF1 0.733 (0.837)\n",
            "Epoch: [186][900/1405]\tBatch Time 0.118 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.6944 (10.6398)\tVB Loss 1.0746 (1.3285)\tF1 0.792 (0.837)\n",
            "Epoch: [186][1200/1405]\tBatch Time 0.125 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.2067 (10.6552)\tVB Loss 1.4665 (1.3241)\tF1 0.834 (0.839)\n",
            "Validation: [0/325]\tBatch Time 0.290 (0.290)\tVB Loss 1.5108 (1.5108)\tF1 Score 0.866 (0.866)\t\n",
            "Validation: [100/325]\tBatch Time 0.060 (0.078)\tVB Loss 0.2172 (1.4442)\tF1 Score 0.990 (0.864)\t\n",
            "Validation: [200/325]\tBatch Time 0.079 (0.076)\tVB Loss 2.7614 (1.5139)\tF1 Score 0.804 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.089 (0.076)\tVB Loss 1.1818 (1.5342)\tF1 Score 0.943 (0.856)\t\n",
            "\n",
            " * LOSS - 1.543, F1 SCORE - 0.855\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [187][0/1405]\tBatch Time 0.356 (0.356)\tData Load Time 0.257 (0.257)\tCE Loss 9.2863 (9.2863)\tVB Loss 0.8676 (0.8676)\tF1 0.828 (0.828)\n",
            "Epoch: [187][300/1405]\tBatch Time 0.181 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 10.7975 (10.6591)\tVB Loss 1.2207 (1.3248)\tF1 0.862 (0.843)\n",
            "Epoch: [187][600/1405]\tBatch Time 0.140 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.5996 (10.6715)\tVB Loss 0.2430 (1.3764)\tF1 0.977 (0.835)\n",
            "Epoch: [187][900/1405]\tBatch Time 0.168 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.3929 (10.6608)\tVB Loss 1.6408 (1.3693)\tF1 0.700 (0.834)\n",
            "Epoch: [187][1200/1405]\tBatch Time 0.169 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.3474 (10.6530)\tVB Loss 1.9604 (1.3412)\tF1 0.830 (0.838)\n",
            "Validation: [0/325]\tBatch Time 0.287 (0.287)\tVB Loss 0.1687 (0.1687)\tF1 Score 1.000 (1.000)\t\n",
            "Validation: [100/325]\tBatch Time 0.065 (0.077)\tVB Loss 0.8979 (1.5615)\tF1 Score 0.811 (0.853)\t\n",
            "Validation: [200/325]\tBatch Time 0.059 (0.076)\tVB Loss 2.6752 (1.5315)\tF1 Score 0.873 (0.854)\t\n",
            "Validation: [300/325]\tBatch Time 0.084 (0.075)\tVB Loss 2.1493 (1.5356)\tF1 Score 0.874 (0.853)\t\n",
            "\n",
            " * LOSS - 1.537, F1 SCORE - 0.853\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n",
            "Epoch: [188][0/1405]\tBatch Time 0.394 (0.394)\tData Load Time 0.248 (0.248)\tCE Loss 10.7536 (10.7536)\tVB Loss 0.9704 (0.9704)\tF1 0.907 (0.907)\n",
            "Epoch: [188][300/1405]\tBatch Time 0.166 (0.132)\tData Load Time 0.004 (0.006)\tCE Loss 10.3154 (10.6814)\tVB Loss 3.4739 (1.3529)\tF1 0.779 (0.841)\n",
            "Epoch: [188][600/1405]\tBatch Time 0.131 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 11.4748 (10.6866)\tVB Loss 1.7609 (1.3365)\tF1 0.731 (0.840)\n",
            "Epoch: [188][900/1405]\tBatch Time 0.114 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.8156 (10.6603)\tVB Loss 0.6655 (1.3547)\tF1 0.664 (0.838)\n",
            "Epoch: [188][1200/1405]\tBatch Time 0.128 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.4601 (10.6474)\tVB Loss 1.9978 (1.3283)\tF1 0.888 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.313 (0.313)\tVB Loss 0.7900 (0.7900)\tF1 Score 0.959 (0.959)\t\n",
            "Validation: [100/325]\tBatch Time 0.071 (0.078)\tVB Loss 1.1376 (1.6495)\tF1 Score 0.745 (0.846)\t\n",
            "Validation: [200/325]\tBatch Time 0.086 (0.077)\tVB Loss 0.3604 (1.6892)\tF1 Score 0.778 (0.850)\t\n",
            "Validation: [300/325]\tBatch Time 0.102 (0.076)\tVB Loss 0.6220 (1.5356)\tF1 Score 0.957 (0.852)\t\n",
            "\n",
            " * LOSS - 1.536, F1 SCORE - 0.852\n",
            "\n",
            "\n",
            "Epochs since improvement: 4\n",
            "\n",
            "Epoch: [189][0/1405]\tBatch Time 0.440 (0.440)\tData Load Time 0.273 (0.273)\tCE Loss 11.3664 (11.3664)\tVB Loss 2.0688 (2.0688)\tF1 0.742 (0.742)\n",
            "Epoch: [189][300/1405]\tBatch Time 0.148 (0.131)\tData Load Time 0.004 (0.005)\tCE Loss 10.4844 (10.6378)\tVB Loss 1.3791 (1.3033)\tF1 0.883 (0.842)\n",
            "Epoch: [189][600/1405]\tBatch Time 0.127 (0.129)\tData Load Time 0.005 (0.005)\tCE Loss 11.1175 (10.6455)\tVB Loss 1.6396 (1.3193)\tF1 0.862 (0.839)\n",
            "Epoch: [189][900/1405]\tBatch Time 0.139 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 11.5423 (10.6626)\tVB Loss 2.8089 (1.3182)\tF1 0.763 (0.837)\n",
            "Epoch: [189][1200/1405]\tBatch Time 0.098 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.4952 (10.6594)\tVB Loss 1.4393 (1.3119)\tF1 0.902 (0.837)\n",
            "Validation: [0/325]\tBatch Time 0.282 (0.282)\tVB Loss 1.3397 (1.3397)\tF1 Score 0.910 (0.910)\t\n",
            "Validation: [100/325]\tBatch Time 0.074 (0.077)\tVB Loss 0.5997 (1.4173)\tF1 Score 0.892 (0.856)\t\n",
            "Validation: [200/325]\tBatch Time 0.097 (0.076)\tVB Loss 3.7267 (1.5740)\tF1 Score 0.727 (0.851)\t\n",
            "Validation: [300/325]\tBatch Time 0.048 (0.076)\tVB Loss 0.7735 (1.5283)\tF1 Score 0.792 (0.851)\t\n",
            "\n",
            " * LOSS - 1.536, F1 SCORE - 0.849\n",
            "\n",
            "\n",
            "Epochs since improvement: 5\n",
            "\n",
            "Epoch: [190][0/1405]\tBatch Time 0.348 (0.348)\tData Load Time 0.214 (0.214)\tCE Loss 9.7138 (9.7138)\tVB Loss 1.0969 (1.0969)\tF1 0.887 (0.887)\n",
            "Epoch: [190][300/1405]\tBatch Time 0.135 (0.131)\tData Load Time 0.005 (0.005)\tCE Loss 10.9465 (10.6847)\tVB Loss 1.5336 (1.2720)\tF1 0.862 (0.843)\n",
            "Epoch: [190][600/1405]\tBatch Time 0.073 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 9.1069 (10.6362)\tVB Loss 0.4013 (1.3179)\tF1 0.799 (0.842)\n",
            "Epoch: [190][900/1405]\tBatch Time 0.101 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.6331 (10.6538)\tVB Loss 2.0453 (1.2968)\tF1 0.717 (0.843)\n",
            "Epoch: [190][1200/1405]\tBatch Time 0.163 (0.127)\tData Load Time 0.005 (0.005)\tCE Loss 10.8498 (10.6506)\tVB Loss 1.7081 (1.2957)\tF1 0.827 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.343 (0.343)\tVB Loss 1.2416 (1.2416)\tF1 Score 0.697 (0.697)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.078)\tVB Loss 0.9509 (1.4153)\tF1 Score 0.878 (0.868)\t\n",
            "Validation: [200/325]\tBatch Time 0.062 (0.077)\tVB Loss 0.4775 (1.5611)\tF1 Score 0.895 (0.858)\t\n",
            "Validation: [300/325]\tBatch Time 0.060 (0.076)\tVB Loss 1.9410 (1.5549)\tF1 Score 0.608 (0.855)\t\n",
            "\n",
            " * LOSS - 1.536, F1 SCORE - 0.856\n",
            "\n",
            "\n",
            "Epochs since improvement: 6\n",
            "\n",
            "Epoch: [191][0/1405]\tBatch Time 0.391 (0.391)\tData Load Time 0.223 (0.223)\tCE Loss 10.5813 (10.5813)\tVB Loss 1.5945 (1.5945)\tF1 0.782 (0.782)\n",
            "Epoch: [191][300/1405]\tBatch Time 0.113 (0.132)\tData Load Time 0.006 (0.005)\tCE Loss 10.4558 (10.6638)\tVB Loss 0.8011 (1.3547)\tF1 0.899 (0.833)\n",
            "Epoch: [191][600/1405]\tBatch Time 0.119 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 11.4959 (10.6608)\tVB Loss 0.7945 (1.3445)\tF1 0.953 (0.830)\n",
            "Epoch: [191][900/1405]\tBatch Time 0.113 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 11.9189 (10.6612)\tVB Loss 2.1591 (1.3387)\tF1 0.580 (0.832)\n",
            "Epoch: [191][1200/1405]\tBatch Time 0.179 (0.128)\tData Load Time 0.006 (0.005)\tCE Loss 10.9273 (10.6527)\tVB Loss 1.8418 (1.3329)\tF1 0.848 (0.836)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 4.7361 (4.7361)\tF1 Score 0.781 (0.781)\t\n",
            "Validation: [100/325]\tBatch Time 0.081 (0.077)\tVB Loss 1.2728 (1.5139)\tF1 Score 0.986 (0.853)\t\n",
            "Validation: [200/325]\tBatch Time 0.073 (0.076)\tVB Loss 0.5615 (1.5674)\tF1 Score 0.919 (0.851)\t\n",
            "Validation: [300/325]\tBatch Time 0.071 (0.074)\tVB Loss 1.7894 (1.5611)\tF1 Score 0.771 (0.853)\t\n",
            "\n",
            " * LOSS - 1.539, F1 SCORE - 0.852\n",
            "\n",
            "\n",
            "Epochs since improvement: 7\n",
            "\n",
            "Epoch: [192][0/1405]\tBatch Time 0.395 (0.395)\tData Load Time 0.230 (0.230)\tCE Loss 10.2157 (10.2157)\tVB Loss 0.8043 (0.8043)\tF1 0.857 (0.857)\n",
            "Epoch: [192][300/1405]\tBatch Time 0.110 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 9.4245 (10.6503)\tVB Loss 1.7322 (1.2822)\tF1 0.694 (0.842)\n",
            "Epoch: [192][600/1405]\tBatch Time 0.137 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 11.2407 (10.6438)\tVB Loss 4.0699 (1.2856)\tF1 0.820 (0.842)\n",
            "Epoch: [192][900/1405]\tBatch Time 0.103 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 10.4000 (10.6448)\tVB Loss 0.6121 (1.3038)\tF1 0.882 (0.844)\n",
            "Epoch: [192][1200/1405]\tBatch Time 0.121 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 11.5585 (10.6525)\tVB Loss 1.8837 (1.3086)\tF1 0.874 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.314 (0.314)\tVB Loss 0.9528 (0.9528)\tF1 Score 0.950 (0.950)\t\n",
            "Validation: [100/325]\tBatch Time 0.079 (0.078)\tVB Loss 0.6282 (1.4704)\tF1 Score 0.922 (0.871)\t\n",
            "Validation: [200/325]\tBatch Time 0.061 (0.077)\tVB Loss 1.5193 (1.5708)\tF1 Score 0.741 (0.860)\t\n",
            "Validation: [300/325]\tBatch Time 0.046 (0.075)\tVB Loss 0.9063 (1.5444)\tF1 Score 0.712 (0.857)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.859\n",
            "\n",
            "Epoch: [193][0/1405]\tBatch Time 0.378 (0.378)\tData Load Time 0.236 (0.236)\tCE Loss 10.1031 (10.1031)\tVB Loss 0.4443 (0.4443)\tF1 0.985 (0.985)\n",
            "Epoch: [193][300/1405]\tBatch Time 0.125 (0.132)\tData Load Time 0.004 (0.005)\tCE Loss 10.6561 (10.6590)\tVB Loss 2.7693 (1.3108)\tF1 0.845 (0.843)\n",
            "Epoch: [193][600/1405]\tBatch Time 0.119 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.3546 (10.6631)\tVB Loss 1.0261 (1.2982)\tF1 0.894 (0.841)\n",
            "Epoch: [193][900/1405]\tBatch Time 0.143 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.9783 (10.6606)\tVB Loss 3.9574 (1.3138)\tF1 0.791 (0.840)\n",
            "Epoch: [193][1200/1405]\tBatch Time 0.163 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.6048 (10.6582)\tVB Loss 2.5089 (1.3217)\tF1 0.679 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.299 (0.299)\tVB Loss 1.0923 (1.0923)\tF1 Score 0.848 (0.848)\t\n",
            "Validation: [100/325]\tBatch Time 0.086 (0.078)\tVB Loss 1.3554 (1.6079)\tF1 Score 0.775 (0.870)\t\n",
            "Validation: [200/325]\tBatch Time 0.056 (0.076)\tVB Loss 0.8415 (1.5270)\tF1 Score 0.927 (0.866)\t\n",
            "Validation: [300/325]\tBatch Time 0.065 (0.076)\tVB Loss 2.0184 (1.5556)\tF1 Score 0.854 (0.859)\t\n",
            "\n",
            " * LOSS - 1.533, F1 SCORE - 0.859\n",
            "\n",
            "Epoch: [194][0/1405]\tBatch Time 0.418 (0.418)\tData Load Time 0.245 (0.245)\tCE Loss 10.2268 (10.2268)\tVB Loss 1.4435 (1.4435)\tF1 0.834 (0.834)\n",
            "Epoch: [194][300/1405]\tBatch Time 0.125 (0.133)\tData Load Time 0.004 (0.005)\tCE Loss 11.2309 (10.6782)\tVB Loss 2.2273 (1.3272)\tF1 0.821 (0.835)\n",
            "Epoch: [194][600/1405]\tBatch Time 0.140 (0.130)\tData Load Time 0.005 (0.005)\tCE Loss 10.4140 (10.6595)\tVB Loss 0.6210 (1.3259)\tF1 0.943 (0.835)\n",
            "Epoch: [194][900/1405]\tBatch Time 0.152 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 10.5222 (10.6514)\tVB Loss 2.3722 (1.3068)\tF1 0.859 (0.839)\n",
            "Epoch: [194][1200/1405]\tBatch Time 0.126 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 10.3804 (10.6490)\tVB Loss 1.6411 (1.3180)\tF1 0.839 (0.838)\n",
            "Validation: [0/325]\tBatch Time 0.300 (0.300)\tVB Loss 1.6189 (1.6189)\tF1 Score 0.822 (0.822)\t\n",
            "Validation: [100/325]\tBatch Time 0.062 (0.074)\tVB Loss 0.3551 (1.5542)\tF1 Score 0.901 (0.853)\t\n",
            "Validation: [200/325]\tBatch Time 0.079 (0.075)\tVB Loss 3.1484 (1.6412)\tF1 Score 0.820 (0.851)\t\n",
            "Validation: [300/325]\tBatch Time 0.081 (0.074)\tVB Loss 0.6725 (1.5446)\tF1 Score 0.799 (0.858)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.860\n",
            "\n",
            "Epoch: [195][0/1405]\tBatch Time 0.390 (0.390)\tData Load Time 0.234 (0.234)\tCE Loss 10.7586 (10.7586)\tVB Loss 1.8915 (1.8915)\tF1 0.684 (0.684)\n",
            "Epoch: [195][300/1405]\tBatch Time 0.096 (0.133)\tData Load Time 0.004 (0.005)\tCE Loss 10.0271 (10.6903)\tVB Loss 0.4280 (1.2959)\tF1 0.982 (0.840)\n",
            "Epoch: [195][600/1405]\tBatch Time 0.087 (0.130)\tData Load Time 0.004 (0.005)\tCE Loss 10.3440 (10.6485)\tVB Loss 1.1813 (1.2811)\tF1 0.692 (0.836)\n",
            "Epoch: [195][900/1405]\tBatch Time 0.119 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 10.2843 (10.6486)\tVB Loss 1.8422 (1.3042)\tF1 0.830 (0.838)\n",
            "Epoch: [195][1200/1405]\tBatch Time 0.121 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.7905 (10.6429)\tVB Loss 1.3850 (1.3084)\tF1 0.765 (0.840)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 1.2008 (1.2008)\tF1 Score 0.926 (0.926)\t\n",
            "Validation: [100/325]\tBatch Time 0.070 (0.078)\tVB Loss 0.7687 (1.3683)\tF1 Score 0.938 (0.875)\t\n",
            "Validation: [200/325]\tBatch Time 0.080 (0.075)\tVB Loss 1.7147 (1.4999)\tF1 Score 0.904 (0.863)\t\n",
            "Validation: [300/325]\tBatch Time 0.052 (0.075)\tVB Loss 1.2638 (1.5465)\tF1 Score 0.957 (0.859)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.861\n",
            "\n",
            "Epoch: [196][0/1405]\tBatch Time 0.405 (0.405)\tData Load Time 0.233 (0.233)\tCE Loss 10.6236 (10.6236)\tVB Loss 2.5568 (2.5568)\tF1 0.672 (0.672)\n",
            "Epoch: [196][300/1405]\tBatch Time 0.118 (0.132)\tData Load Time 0.004 (0.005)\tCE Loss 10.7476 (10.6950)\tVB Loss 0.6551 (1.3123)\tF1 0.886 (0.839)\n",
            "Epoch: [196][600/1405]\tBatch Time 0.101 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 8.7903 (10.6679)\tVB Loss 0.9363 (1.3334)\tF1 0.777 (0.841)\n",
            "Epoch: [196][900/1405]\tBatch Time 0.124 (0.128)\tData Load Time 0.004 (0.005)\tCE Loss 10.5719 (10.6655)\tVB Loss 1.7569 (1.3476)\tF1 0.897 (0.840)\n",
            "Epoch: [196][1200/1405]\tBatch Time 0.104 (0.127)\tData Load Time 0.006 (0.005)\tCE Loss 10.5289 (10.6491)\tVB Loss 0.1953 (1.3215)\tF1 1.000 (0.841)\n",
            "Validation: [0/325]\tBatch Time 0.322 (0.322)\tVB Loss 1.3353 (1.3353)\tF1 Score 0.717 (0.717)\t\n",
            "Validation: [100/325]\tBatch Time 0.055 (0.075)\tVB Loss 2.4173 (1.6021)\tF1 Score 0.769 (0.846)\t\n",
            "Validation: [200/325]\tBatch Time 0.065 (0.075)\tVB Loss 2.7784 (1.6068)\tF1 Score 0.611 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.070 (0.074)\tVB Loss 2.7808 (1.5540)\tF1 Score 0.844 (0.860)\t\n",
            "\n",
            " * LOSS - 1.533, F1 SCORE - 0.861\n",
            "\n",
            "Epoch: [197][0/1405]\tBatch Time 0.344 (0.344)\tData Load Time 0.233 (0.233)\tCE Loss 10.3575 (10.3575)\tVB Loss 2.2358 (2.2358)\tF1 0.804 (0.804)\n",
            "Epoch: [197][300/1405]\tBatch Time 0.134 (0.132)\tData Load Time 0.004 (0.005)\tCE Loss 11.5416 (10.6827)\tVB Loss 0.5871 (1.2960)\tF1 0.888 (0.857)\n",
            "Epoch: [197][600/1405]\tBatch Time 0.135 (0.128)\tData Load Time 0.005 (0.005)\tCE Loss 10.7063 (10.6625)\tVB Loss 0.7850 (1.2923)\tF1 0.972 (0.845)\n",
            "Epoch: [197][900/1405]\tBatch Time 0.144 (0.126)\tData Load Time 0.009 (0.005)\tCE Loss 10.3177 (10.6576)\tVB Loss 0.3109 (1.2992)\tF1 0.952 (0.844)\n",
            "Epoch: [197][1200/1405]\tBatch Time 0.111 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 10.2738 (10.6490)\tVB Loss 1.7907 (1.3053)\tF1 0.800 (0.842)\n",
            "Validation: [0/325]\tBatch Time 0.308 (0.308)\tVB Loss 0.4486 (0.4486)\tF1 Score 0.971 (0.971)\t\n",
            "Validation: [100/325]\tBatch Time 0.060 (0.076)\tVB Loss 1.1901 (1.5993)\tF1 Score 0.866 (0.854)\t\n",
            "Validation: [200/325]\tBatch Time 0.068 (0.074)\tVB Loss 4.2595 (1.5223)\tF1 Score 0.621 (0.861)\t\n",
            "Validation: [300/325]\tBatch Time 0.091 (0.075)\tVB Loss 1.9581 (1.5206)\tF1 Score 0.800 (0.857)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.856\n",
            "\n",
            "\n",
            "Epochs since improvement: 1\n",
            "\n",
            "Epoch: [198][0/1405]\tBatch Time 0.352 (0.352)\tData Load Time 0.213 (0.213)\tCE Loss 10.8504 (10.8504)\tVB Loss 1.2504 (1.2504)\tF1 0.807 (0.807)\n",
            "Epoch: [198][300/1405]\tBatch Time 0.184 (0.129)\tData Load Time 0.004 (0.005)\tCE Loss 9.7090 (10.6433)\tVB Loss 1.1233 (1.3071)\tF1 0.876 (0.836)\n",
            "Epoch: [198][600/1405]\tBatch Time 0.074 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 9.6235 (10.6588)\tVB Loss 1.0575 (1.2885)\tF1 0.865 (0.840)\n",
            "Epoch: [198][900/1405]\tBatch Time 0.073 (0.127)\tData Load Time 0.004 (0.005)\tCE Loss 8.7696 (10.6448)\tVB Loss 0.0669 (1.2907)\tF1 1.000 (0.841)\n",
            "Epoch: [198][1200/1405]\tBatch Time 0.119 (0.126)\tData Load Time 0.004 (0.004)\tCE Loss 10.8206 (10.6583)\tVB Loss 2.1668 (1.2877)\tF1 0.803 (0.843)\n",
            "Validation: [0/325]\tBatch Time 0.304 (0.304)\tVB Loss 6.8378 (6.8378)\tF1 Score 0.644 (0.644)\t\n",
            "Validation: [100/325]\tBatch Time 0.062 (0.075)\tVB Loss 0.0750 (1.5741)\tF1 Score 1.000 (0.839)\t\n",
            "Validation: [200/325]\tBatch Time 0.080 (0.075)\tVB Loss 0.8307 (1.5010)\tF1 Score 0.922 (0.851)\t\n",
            "Validation: [300/325]\tBatch Time 0.063 (0.075)\tVB Loss 1.1190 (1.5534)\tF1 Score 0.861 (0.851)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.851\n",
            "\n",
            "\n",
            "Epochs since improvement: 2\n",
            "\n",
            "Epoch: [199][0/1405]\tBatch Time 0.448 (0.448)\tData Load Time 0.266 (0.266)\tCE Loss 10.3809 (10.3809)\tVB Loss 0.9977 (0.9977)\tF1 0.785 (0.785)\n",
            "Epoch: [199][300/1405]\tBatch Time 0.148 (0.129)\tData Load Time 0.005 (0.005)\tCE Loss 10.7732 (10.6871)\tVB Loss 1.9156 (1.3217)\tF1 0.833 (0.843)\n",
            "Epoch: [199][600/1405]\tBatch Time 0.118 (0.126)\tData Load Time 0.005 (0.005)\tCE Loss 9.9301 (10.6319)\tVB Loss 0.5557 (1.3102)\tF1 0.979 (0.840)\n",
            "Epoch: [199][900/1405]\tBatch Time 0.116 (0.126)\tData Load Time 0.004 (0.005)\tCE Loss 11.4698 (10.6337)\tVB Loss 1.7411 (1.3070)\tF1 0.805 (0.843)\n",
            "Epoch: [199][1200/1405]\tBatch Time 0.121 (0.125)\tData Load Time 0.004 (0.005)\tCE Loss 9.7233 (10.6401)\tVB Loss 1.5097 (1.2993)\tF1 0.673 (0.843)\n",
            "Validation: [0/325]\tBatch Time 0.313 (0.313)\tVB Loss 1.8929 (1.8929)\tF1 Score 0.848 (0.848)\t\n",
            "Validation: [100/325]\tBatch Time 0.081 (0.079)\tVB Loss 1.1581 (1.4896)\tF1 Score 0.896 (0.851)\t\n",
            "Validation: [200/325]\tBatch Time 0.084 (0.077)\tVB Loss 3.8507 (1.5590)\tF1 Score 0.655 (0.853)\t\n",
            "Validation: [300/325]\tBatch Time 0.072 (0.076)\tVB Loss 1.2155 (1.5108)\tF1 Score 0.929 (0.859)\t\n",
            "\n",
            " * LOSS - 1.534, F1 SCORE - 0.857\n",
            "\n",
            "\n",
            "Epochs since improvement: 3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OipsVUg24hin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}